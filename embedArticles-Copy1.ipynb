{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ekvall/anaconda3/envs/terran3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/ekvall/anaconda3/envs/terran3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/ekvall/anaconda3/envs/terran3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/ekvall/anaconda3/envs/terran3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/ekvall/anaconda3/envs/terran3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/ekvall/anaconda3/envs/terran3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from bert_serving.client import BertClient\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from summarizer import Summarizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gensim\n",
    "import multiprocessing\n",
    "import numpy as np\n",
    "from bert_serving.client import BertClient\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "from bokeh.plotting import figure, show, output_notebook, output_file, save\n",
    "from bokeh.models import HoverTool, ColumnDataSource, value\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.sparse.linalg import svds\n",
    "import pickle\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from bokeh.transform import factor_cmap, factor_mark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GaussianMixture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pickle.load(open('/hdd/dataPhdProject/dataDict', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "didList = list()\n",
    "abstractList = list()\n",
    "titleList = list()\n",
    "for k,v in X.items():\n",
    "    didList.append(k)\n",
    "    abstractList.append(v[\"abstract\"])\n",
    "    titleList.append(v[\"title\"])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "bc = BertClient()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "projectDescription1 = \"Optimization is the algorithmic workhorse of a lot of modern AI and machine learning. Increasingly often, learning and decision tasks are formulated, analyzed and solved as large-scale optimization problems. As these problems grow in size (for example, due to increasing amounts of data or a desire to coordinate ever more decisions) existing algorithms struggle to find a sufficiently good solution within a short time. This research project aims at developing fundamental theory and algorithms for which are able to solve large-scale optimization problem at an unprecedented speed, eventually supporting decision-making in real-time.\"\n",
    "projectDescription2 = \"This thesis aims at developing efficient algorithms for solving some fundamental engineering problems in data science and machine learning. We investigate a variety of acceleration techniques for improving the convergence times of optimization algorithms.  First, we investigate how problem structure can be exploited to accelerate the solution of highly structured problems such as generalized eigenvalue and elastic net regression. We then consider Anderson acceleration, a generic and parameter-free extrapolation scheme, and show how it can be adapted to accelerate practical convergence of proximal gradient methods for a broad class of non-smooth problems. For all the methods developed in this thesis, we design novel algorithms, perform mathematical analysis of convergence rates, and conduct practical experiments on real-world data sets.\"\n",
    "projectDescription3 = \"Stochastic gradient methods with momentum are widely used in applications and at the core of optimization subroutines in many popular machine learning libraries. However, their sample complexities have never been obtained for problems that are non-convex and non-smooth. This paper establishes the convergence rate of a stochastic subgradient method with a momentum term of Polyak type for a broad class of non-smooth, non-convex, and constrained optimization problems. Our key innovation is the construction of a special Lyapunov function for which the proven complexity can be achieved without any tunning of the momentum parameter. For smooth problems, we extend the known complexity bound to the constrained case and demonstrate how the unconstrained case can be analyzed under weaker assumptions than the state-of-the-art. Numerical results confirm our theoretical developments.\"\n",
    "projectDescription4 = \"Anderson acceleration is a well-established and simple technique for speeding up fixed-point computations with countless applications. Previous studies of Anderson acceleration in optimization have only been able to provide convergence guarantees for unconstrained and smooth problems. This work introduces novel methods for adapting Anderson acceleration to (non-smooth and constrained) proximal gradient algorithms. Under some technical conditions, we extend the existing local convergence results of Anderson acceleration for smooth fixed-point mappings to the proposed scheme. We also prove analytically that it is not, in general, possible to guarantee global convergence of native Anderson acceleration. We therefore propose a simple scheme for stabilization that combines the global worst-case guarantees of proximal gradient methods with the local adaptation and practical speed-up of Anderson acceleration.\"\n",
    "projectDescription5 =  \"Existing deep learning frameworks optimize the computation graph of a DNN model by performing greedy rule-based graph transformations, which generally only consider transformations that strictly improve runtime performance. We propose relaxed graph substitutions that enable the exploration of complex graph optimizations by relaxing the strict performance improvement constraint, which greatly increases the space of semantically equivalent computation graphs that can be discovered by repeated application of a suitable set of graph transformations. We introduce a backtracking search algorithm over a set of relaxed graph substitutions to find optimized networks and use a flow-based graph split algorithm to recursively split a computation graph into smaller subgraphs to allow efficient search. We implement relaxed graph substitutions in a system called MetaFlow and show that MetaFlow improves the inference and training performance by 1.1-1.6× and 1.1-1.2× respectively over existing deep learning frameworks.\"\n",
    "projectDescription6 = \"In ultra-dense heterogeneous networks, caching popular contents at small base stations is considered as an effective way to reduce latency and redundant data transmission. Optimization of caching placement/replacement and content delivering can be computationally heavy, especially for large-scale networks. The provision of both time-efficient and high-quality solutions is challenging. Conventional iterative optimization methods, either optimal or heuristic, typically require a large number of iterations to achieve satisfactory performance, and result in considerable computational delay. This may limit their applications in practical network operations where online decisions have to be made. In this paper, we provide a viable alternative to the conventional methods for caching optimization, from a deep learning perspective. The idea is to train the optimization algorithms through a deep neural network (DNN) in advance, instead of directly applying them in real-time caching or scheduling. This allows significant complexity reduction in the delay-sensitive operation phase since the computational burden is shifted to the DDN training phase. Numerical results demonstrate that the DNN is of high computational efficiency. By training the designed DNN over a massive number of instances, the solution quality of the energy-efficient content delivering can be progressively approximated to around 90% of the optimum.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "projectDescriptionList = [projectDescription1, projectDescription2, projectDescription3, projectDescription4, projectDescription5, projectDescription6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "allAbs = abstractList + projectDescriptionList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = bc.encode(allAbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "projVec = emb[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import pairwise_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = pairwise_distances(emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "selectedArticles = list()\n",
    "selectedIds = list()\n",
    "for i in range(1,7):\n",
    "    selectedArticles += list(np.array(allAbs)[np.argsort(D[:,-i])][:30])\n",
    "    selectedIds += list(np.argsort(D[:,-i])[:30])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = [False if sa in projectDescriptionList else True for sa in selectedArticles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "155"
      ]
     },
     "execution_count": 339,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(selectedIds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "selectedArticles = np.array(selectedArticles)[mask]\n",
    "selectedIds = np.array(selectedIds)[mask]\n",
    "selectedDid = np.array(didList)[selectedIds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Summarizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------ 0 2019_66 -------------\n",
      "Designing effective architectures is one of the key factors behind the success of deep neural networks. Existing deep architectures are either manually designed or automatically searched by some Neural Architecture Search (NAS) methods. However, even a well-searched architecture may still contain many non-significant or redundant modules or operations (e.g., convolution or pooling), which may not only incur substantial memory consumption and computation cost but also deteriorate the performance. Thus, it is necessary to optimize the operations inside an architecture to improve the performance without introducing extra computation cost. Unfortunately, such a constrained optimization problem is NP-hard. To make the problem feasible, we cast the optimization problem into a Markov decision process (MDP) and seek to learn a Neural Architecture Transformer (NAT) to replace the redundant operations with the more computationally efficient ones (e.g., skip connection or directly removing the connection). Based on MDP, we learn NAT by exploiting reinforcement learning to obtain the optimization policies w.r.t. different architectures. To verify the effectiveness of the proposed strategies, we apply NAT on both hand-crafted architectures and NAS based architectures. Extensive experiments on two benchmark datasets, i.e., CIFAR-10 and ImageNet, demonstrate that the transformed architecture by NAT significantly outperforms both its original form and those architectures optimized by existing methods.\n",
      "------------ 1 2019_10 -------------\n",
      "A core capability of intelligent systems is the ability to quickly learn new tasks by drawing on prior experience. Gradient (or optimization) based meta-learning has recently emerged as an effective approach for few-shot learning. In this formulation, meta-parameters are learned in the outer loop, while task-specific models are learned in the inner-loop, by using only a small amount of data from the current task. A key challenge in scaling these approaches is the need to differentiate through the inner loop learning process, which can impose considerable computational and memory burdens. By drawing upon implicit differentiation, we develop the implicit MAML algorithm, which depends only on the solution to the inner level optimization and not the path taken by the inner loop optimizer. This effectively decouples the meta-gradient computation from the choice of inner loop optimizer. As a result, our approach is agnostic to the choice of inner loop optimizer and can gracefully handle many gradient steps without vanishing gradients or memory constraints. Theoretically, we prove that implicit MAML can compute accurate meta-gradients with a memory footprint that is, up to small constant factors, no more than that which is required to compute a single inner loop gradient and at no overall increase in the total computational cost. Experimentally, we show that these benefits of implicit MAML translate into empirical gains on few-shot image recognition benchmarks.\n",
      "------------ 2 2019_538 -------------\n",
      "Deep neural networks have yielded superior performance in many contemporary applications. However, the gradient computation in a deep model with millions of instances leads to a lengthy training process even with modern GPU/TPU hardware acceleration. In this paper, we propose AutoAssist, a simple framework to accelerate training of a deep neural network. Typically, as the training procedure evolves, the amount of improvement by a stochastic gradient update varies dynamically with the choice of instances in the mini-batch. In AutoAssist, we utilize this fact and design an instance shrinking operation that is used to filter out instances with relatively low marginal improvement to the current model; thus the computationally intensive gradient computations are performed on informative instances as much as possible. Specifically, we train a very lightweight Assistant model jointly with the original deep network, which we refer to as Boss. The Assistant model is designed to gauge the importance of a given instance with respect to the current Boss such that the shrinking operation can be applied in the batch generator. With careful design, we train the Boss and Assistant in a nonblocking and asynchronous fashion such that overhead is minimal. To demonstrate the effectiveness of AutoAssist, we conduct experiments on two contemporary applications: image classification using ResNets with varied number of layers, and neural machine translation using LSTMs, ConvS2S and Transformer models. For each application, we verify that AutoAssist leads to significant reduction in training time; in particular, 30% to 40% of the total operation count can be reduced which leads to faster convergence and a corresponding decrease in training time.\n",
      "------------ 3 2016_469 -------------\n",
      "Gaussian Process bandit optimization has emerged as a powerful tool for optimizing noisy black box functions. One example in machine learning is hyper-parameter optimization where each evaluation of the target function may require training a model which may involve days or even weeks of computation. Most methods for this so-called “Bayesian optimization” only allow sequential exploration of the parameter space. However, it is often desirable to propose batches or sets of parameter values to explore simultaneously, especially when there are large parallel processing facilities at our disposal. Batch methods require modeling the interaction between the different evaluations in the batch, which can be expensive in complex scenarios. In this paper, we propose a new approach for parallelizing Bayesian optimization by modeling the diversity of a batch via Determinantal point processes (DPPs) whose kernels are learned automatically. This allows us to generalize a previous result as well as prove better regret bounds based on DPP sampling. Our experiments on a variety of synthetic and real-world robotics and hyper-parameter optimization tasks indicate that our DPP-based methods, especially those based on DPP sampling, outperform state-of-the-art methods.\n",
      "------------ 4 2018_410 -------------\n",
      "Huge scale machine learning problems are nowadays tackled by distributed optimization algorithms, i.e. algorithms that leverage the compute power of many devices for training. The communication overhead is a key bottleneck that hinders perfect scalability. Various recent works proposed to use quantization or sparsification techniques to reduce the amount of data that needs to be communicated, for instance by only sending the most significant entries of the stochastic gradient (top-k sparsification). Whilst such schemes showed very promising performance in practice, they have eluded theoretical analysis so far. In this work we analyze Stochastic Gradient Descent (SGD) with k-sparsification or compression (for instance top-k or random-k) and show that this scheme converges at the same rate as vanilla SGD when equipped with error compensation (keeping track of accumulated errors in memory). That is, communication can be reduced by a factor of the dimension of the problem (sometimes even more) whilst still converging at the same rate. We present numerical experiments to illustrate the theoretical findings and the good scalability for distributed applications.\n",
      "------------ 5 2019_572 -------------\n",
      "Deep Neural Network (DNN) is powerful but computationally expensive and memory intensive, thus impeding its practical usage on resource-constrained front-end devices. DNN pruning is an approach for deep model compression, which aims at eliminating some parameters with tolerable performance degradation. In this paper, we propose a novel momentum-SGD-based optimization method to reduce the network complexity by on-the-fly pruning. Concretely, given a global compression ratio, we categorize all the parameters into two parts at each training iteration which are updated using different rules. In this way, we gradually zero out the redundant parameters, as we update them using only the ordinary weight decay but no gradients derived from the objective function. As a departure from prior methods that require heavy human works to tune the layer-wise sparsity ratios, prune by solving complicated non-differentiable problems or finetune the model after pruning, our method is characterized by 1) global compression that automatically finds the appropriate per-layer sparsity ratios; 2) end-to-end training; 3) no need for a time-consuming re-training process after pruning; and 4) superior capability to find better winning tickets which have won the initialization lottery.\n",
      "------------ 6 2019_925 -------------\n",
      "To improve the resilience of distributed training to worst-case, or Byzantine node failures, several recent methods have replaced gradient averaging with robust aggregation methods. Such techniques can have high computational costs, often quadratic in the number of compute nodes, and only have limited robustness guarantees. Other methods have instead used redundancy to guarantee robustness, but can only tolerate limited numbers of Byzantine failures. In this work, we present DETOX, a Byzantine-resilient distributed training framework that combines algorithmic redundancy with robust aggregation. DETOX operates in two steps, a filtering step that uses limited redundancy to significantly reduce the effect of Byzantine nodes, and a hierarchical aggregation step that can be used in tandem with any state-of-the-art robust aggregation method. We show theoretically that this leads to a substantial increase in robustness, and has a per iteration runtime that can be nearly linear in the number of compute nodes. We provide extensive experiments over real distributed setups across a variety of large-scale machine learning tasks, showing that DETOX leads to orders of magnitude accuracy and speedup improvements over many state-of-the-art Byzantine-resilient approaches.\n",
      "------------ 7 2015_256 -------------\n",
      "Stochastic Gradient Descent (SGD) is a workhorse in machine learning, yet it is also known to be slow relative to steepest descent. Recently, variance reduction techniques such as SVRG and SAGA have been proposed to overcome this weakness. With asymptotically vanishing variance, a constant step size can be maintained, resulting in geometric convergence rates. However, these methods are either based on occasional computations of full gradients at pivot points (SVRG), or on keeping per data point corrections in memory (SAGA). This has the disadvantage that one cannot employ these methods in a streaming setting and that speed-ups relative to SGD may need a certain number of epochs in order to materialize. This paper investigates a new class of algorithms that can exploit neighborhood structure in the training data to share and re-use information about past stochastic gradients across data points. While not meant to be offering advantages in an asymptotic setting, there are significant benefits in the transient optimization phase, in particular in a streaming or single-epoch setting. We investigate this family of algorithms in a thorough analysis and show supporting experimental results. As a side-product we provide a simple and unified proof technique for a broad class of variance reduction algorithms.\n",
      "------------ 8 2018_915 -------------\n",
      "Training deep neural networks requires an exorbitant amount of computation resources, including a heterogeneous mix of GPU and CPU devices. It is critical to place operations in a neural network on these devices in an optimal way, so that the training process can complete within the shortest amount of time. The state-of-the-art uses reinforcement learning to learn placement skills by repeatedly performing Monte-Carlo experiments. However, due to its equal treatment of placement samples, we argue that there remains ample room for significant improvements. In this paper, we propose a new joint learning algorithm, called Post, that integrates cross-entropy minimization and proximal policy optimization to achieve theoretically guaranteed optimal efficiency. In order to incorporate the cross-entropy method as a sampling technique, we propose to represent placements using discrete probability distributions, which allows us to estimate an optimal probability mass by maximal likelihood estimation, a powerful tool with the best possible efficiency. We have implemented Post in the Google Cloud platform, and our extensive experiments with several popular neural network training benchmarks have demonstrated clear evidence of superior performance: with the same amount of learning time, it leads to placements that have training times up to 63.7% shorter over the state-of-the-art.\n",
      "------------ 9 2018_174 -------------\n",
      "Stochastic convex optimization algorithms are the most popular way to train machine learning models on large-scale data. Scaling up the training process of these models is crucial, but the most popular algorithm, Stochastic Gradient Descent (SGD), is a serial method that is surprisingly hard to parallelize. In this paper, we propose an efficient distributed stochastic optimization method by combining adaptivity with variance reduction techniques. Our analysis yields a linear speedup in the number of machines, constant memory footprint, and only a logarithmic number of communication rounds. Critically, our approach is a black-box reduction that parallelizes any serial online learning algorithm, streamlining prior analysis and allowing us to leverage the significant progress that has been made in designing adaptive algorithms. In particular, we achieve optimal convergence rates without any prior knowledge of smoothness parameters, yielding a more robust algorithm that reduces the need for hyperparameter tuning. We implement our algorithm in the Spark distributed framework and exhibit dramatic performance gains on large-scale logistic regression problems.\n",
      "------------ 10 2019_879 -------------\n",
      "Expected improvement and other acquisition functions widely used in Bayesian optimization use a \"one-step\" assumption: they value objective function evaluations assuming no future evaluations will be performed. Because we usually evaluate over multiple steps, this assumption may leave substantial room for improvement. Existing theory gives acquisition functions looking multiple steps in the future but calculating them requires solving a high-dimensional continuous-state continuous-action Markov decision process (MDP). Fast exact solutions of this MDP remain out of reach of today's methods. As a result, previous two- and multi-step lookahead Bayesian optimization algorithms are either too expensive to implement in most practical settings or resort to heuristics that may fail to fully realize the promise of two-step lookahead. This paper proposes a computationally efficient algorithm that provides an accurate solution to the two-step lookahead Bayesian optimization problem in seconds to at most several minutes of computation per batch of evaluations. The resulting acquisition function provides increased query efficiency and robustness compared with previous two- and multi-step lookahead methods in both single-threaded and batch experiments. This unlocks the value of two-step lookahead in practice. We demonstrate the value of our algorithm with extensive experiments on synthetic test functions and real-world problems.\n",
      "------------ 11 2018_855 -------------\n",
      "Distributed implementations of mini-batch stochastic gradient descent (SGD) suffer from communication overheads, attributed to the high frequency of gradient updates inherent in small-batch training. Training with large batches can reduce these overheads; however it besets the convergence of the algorithm and the generalization performance. In this work, we take a first step towards analyzing how the structure (width and depth) of a neural network affects the performance of large-batch training. We present new theoretical results which suggest that--for a fixed number of parameters--wider networks are more amenable to fast large-batch training compared to deeper ones. We provide extensive experiments on residual and fully-connected neural networks which suggest that wider networks can be trained using larger batches without incurring a convergence slow-down, unlike their deeper variants.\n",
      "------------ 12 2015_232 -------------\n",
      "The alternating direction method of multipliers (ADMM) is an important tool for solving complex optimization problems, but it involves minimization sub-steps that are often difficult to solve efficiently. The Primal-Dual Hybrid Gradient (PDHG) method is a powerful alternative that often has simpler substeps than ADMM, thus producing lower complexity solvers. Despite the flexibility of this method, PDHG is often impractical because it requires the careful choice of multiple stepsize parameters. There is often no intuitive way to choose these parameters to maximize efficiency, or even achieve convergence. We propose self-adaptive stepsize rules that automatically tune PDHG parameters for optimal convergence. We rigorously analyze our methods, and identify convergence rates. Numerical experiments show that adaptive PDHG has strong advantages over non-adaptive methods in terms of both efficiency and simplicity for the user.\n",
      "------------ 13 2016_511 -------------\n",
      "One crucial goal in kernel online learning is to bound the model size. Common approaches employ budget maintenance procedures to restrict the model sizes using removal, projection, or merging strategies. Although projection and merging, in the literature, are known to be the most effective strategies, they demand extensive computation whilst removal strategy fails to retain information of the removed vectors. An alternative way to address the model size problem is to apply random features to approximate the kernel function. This allows the model to be maintained directly in the random feature space, hence effectively resolve the curse of kernelization. However, this approach still suffers from a serious shortcoming as it needs to use a high dimensional random feature space to achieve a sufficiently accurate kernel approximation. Consequently, it leads to a significant increase in the computational cost. To address all of these aforementioned challenges, we present in this paper the Dual Space Gradient Descent (DualSGD), a novel framework that utilizes random features as an auxiliary space to maintain information from data points removed during budget maintenance. Consequently, our approach permits the budget to be maintained in a simple, direct and elegant way while simultaneously mitigating the impact of the dimensionality issue on learning performance. We further provide convergence analysis and extensively conduct experiments on five real-world datasets to demonstrate the predictive performance and scalability of our proposed method in comparison with the state-of-the-art baselines.\n",
      "------------ 14 2019_217 -------------\n",
      "Training convolutional neural network models is memory intensive since back-propagation requires storing activations of all intermediate layers. This presents a practical concern when seeking to deploy very deep architectures in production, especially when models need to be frequently re-trained on updated datasets. In this paper, we propose a new implementation for back-propagation that significantly reduces memory usage, by enabling the use of approximations with negligible computational cost and minimal effect on training performance. The algorithm reuses common buffers to temporarily store full activations and compute the forward pass exactly. It also stores approximate per-layer copies of activations, at significant memory savings, that are used in the backward pass. Compared to simply approximating activations within standard back-propagation, our method limits accumulation of errors across layers. This allows the use of much lower-precision approximations without affecting training accuracy. Experiments on CIFAR-10, CIFAR-100, and ImageNet show that our method yields performance close to exact training, while storing activations compactly with as low as 4-bit precision.\n",
      "------------ 15 2018_73 -------------\n",
      "To convert the input into binary code, hashing algorithm has been widely used for approximate nearest neighbor search on large-scale image sets due to its computation and storage efficiency. Deep hashing further improves the retrieval quality by combining the hash coding with deep neural network. However, a major difficulty in deep hashing lies in the discrete constraints imposed on the network output, which generally makes the optimization NP hard. In this work, we adopt the greedy principle to tackle this NP hard problem by iteratively updating the network toward the probable optimal discrete solution in each iteration. A hash coding layer is designed to implement our approach which strictly uses the sign function in forward propagation to maintain the discrete constraints, while in back propagation the gradients are transmitted intactly to the front layer to avoid the vanishing gradients. In addition to the theoretical derivation, we provide a new perspective to visualize and understand the effectiveness and efficiency of our algorithm. Experiments on benchmark datasets show that our scheme outperforms state-of-the-art hashing methods in both supervised and unsupervised tasks.\n",
      "------------ 16 2016_460 -------------\n",
      "We propose a novel approach to reduce memory consumption of the backpropagation through time (BPTT) algorithm when training recurrent neural networks (RNNs). Our approach uses dynamic programming to balance a trade-off between caching of intermediate results and recomputation. The algorithm is capable of tightly fitting within almost any user-set memory budget while finding an optimal execution policy minimizing the computational cost. Computational devices have limited memory capacity and maximizing a computational performance given a fixed memory budget is a practical use-case. We provide asymptotic computational upper bounds for various regimes. The algorithm is particularly effective for long sequences. For sequences of length 1000, our algorithm saves 95\\% of memory usage while using only one third more time per iteration than the standard BPTT.\n",
      "------------ 17 2018_372 -------------\n",
      "Deep neural networks, and in particular recurrent networks, are promising candidates to control autonomous agents that interact in real-time with the physical world. However, this requires a seamless integration of temporal features into the network’s architecture. For the training of and inference with recurrent neural networks, they are usually rolled out over time, and different rollouts exist. Conventionally during inference, the layers of a network are computed in a sequential manner resulting in sparse temporal integration of information and long response times. In this study, we present a theoretical framework to describe rollouts, the level of model-parallelization they induce, and demonstrate differences in solving specific tasks. We prove that certain rollouts, also for networks with only skip and no recurrent connections, enable earlier and more frequent responses, and show empirically that these early responses have better performance. The streaming rollout maximizes these properties and enables a fully parallel execution of the network reducing runtime on massively parallel devices. Finally, we provide an open-source toolbox to design, train, evaluate, and interact with streaming rollouts.\n",
      "------------ 18 2018_548 -------------\n",
      "Convolutional neural networks have achieved great success in various vision tasks; however, they incur heavy resource costs. By using deeper and wider networks, network accuracy can be improved rapidly. However, in an environment with limited resources (e.g., mobile applications), heavy networks may not be usable. This study shows that naive convolution can be deconstructed into a shift operation and pointwise convolution. To cope with various convolutions, we propose a new shift operation called active shift layer (ASL) that formulates the amount of shift as a learnable function with shift parameters. This new layer can be optimized end-to-end through backpropagation and it can provide optimal shift values. Finally, we apply this layer to a light and fast network that surpasses existing state-of-the-art networks.\n",
      "------------ 19 2019_672 -------------\n",
      "Efficient exploration is crucial to achieving good performance in reinforcement learning. Existing systematic exploration strategies (R-MAX, MBIE, UCRL, etc.), despite being promising theoretically, are essentially greedy strategies that follow some predefined heuristics. When the heuristics do not match the dynamics of Markov decision processes (MDPs) well, an excessive amount of time can be wasted in travelling through already-explored states, lowering the overall efficiency. We argue that explicit planning for exploration can help alleviate such a problem, and propose a Value Iteration for Exploration Cost (VIEC) algorithm which computes the optimal exploration scheme by solving an augmented MDP. We then present a detailed analysis of the exploration behaviour of some popular strategies, showing how these strategies can fail and spend O(n^2 md) or O(n^2 m + nmd) steps to collect sufficient data in some tower-shaped MDPs, while the optimal exploration scheme, which can be obtained by VIEC, only needs O(nmd), where n, m are the numbers of states and actions and d is the data demand. The analysis not only points out the weakness of existing heuristic-based strategies, but also suggests a remarkable potential in explicit planning for exploration.\n",
      "------------ 20 2016_234 -------------\n",
      "Faced with saturation of Moore's law and increasing size and dimension of data, system designers have increasingly resorted to parallel and distributed computing to reduce computation time of machine-learning algorithms. However, distributed computing is often bottle necked by a small fraction of slow processors called \"stragglers\" that reduce the speed of computation because the fusion node has to wait for all processors to complete their processing. To combat the effect of stragglers, recent literature proposes introducing redundancy in computations across processors, e.g., using repetition-based strategies or erasure codes. The fusion node can exploit this redundancy by completing the computation using outputs from only a subset of the processors, ignoring the stragglers. In this paper, we propose a novel technique - that we call \"Short-Dot\" - to introduce redundant computations in a coding theory inspired fashion, for computing linear transforms of long vectors. Instead of computing long dot products as required in the original linear transform, we construct a larger number of redundant and short dot products that can be computed more efficiently at individual processors. Further, only a subset of these short dot products are required at the fusion node to finish the computation successfully. We demonstrate through probabilistic analysis as well as experiments on computing clusters that Short-Dot offers significant speed-up compared to existing techniques. We also derive trade-offs between the length of the dot-products and the resilience to stragglers (number of processors required to finish), for any such strategy and compare it to that achieved by our strategy.\n",
      "------------ 21 2018_474 -------------\n",
      "Quantized Neural Networks (QNNs) are often used to improve network efficiency during the inference phase, i.e. after the network has been trained. Extensive research in the field suggests many different quantization schemes. Still, the number of bits required, as well as the best quantization scheme, are yet unknown. Our theoretical analysis suggests that most of the training process is robust to substantial precision reduction, and points to only a few specific operations that require higher precision. Armed with this knowledge, we quantize the model parameters, activations and layer gradients to 8-bit, leaving at higher precision only the final step in the computation of the weight gradients. Additionally, as QNNs require batch-normalization to be trained at high precision, we introduce Range Batch-Normalization (BN) which has significantly higher tolerance to quantization noise and improved computational complexity. Our simulations show that Range BN is equivalent to the traditional batch norm if a precise scale adjustment, which can be approximated analytically, is applied. To the best of the authors' knowledge, this work is the first to quantize the weights, activations, as well as a substantial volume of the gradients stream, in all layers (including batch normalization) to 8-bit while showing state-of-the-art results over the ImageNet-1K dataset.\n",
      "------------ 22 2019_414 -------------\n",
      "The performance of policy gradient methods is sensitive to hyperparameter settings that must be tuned for any new application. Widely used grid search methods for tuning hyperparameters are sample inefficient and computationally expensive. More advanced methods like Population Based Training that learn optimal schedules for hyperparameters instead of fixed settings can yield better results, but are also sample inefficient and computationally expensive. In this paper, we propose Hyperparameter Optimisation on the Fly (HOOF), a gradient-free algorithm that requires no more than one training run to automatically adapt the hyperparameter that affect the policy update directly through the gradient. The main idea is to use existing trajectories sampled by the policy gradient method to optimise a one-step improvement objective, yielding a sample and computationally efficient algorithm that is easy to implement. Our experimental results across multiple domains and algorithms show that using HOOF to learn these hyperparameter schedules leads to faster learning with improved performance.\n",
      "------------ 23 2016_455 -------------\n",
      "The use of Bayesian methods in large-scale data settings is attractive because of the rich hierarchical models, uncertainty quantification, and prior specification they provide. Standard Bayesian inference algorithms are computationally expensive, however, making their direct application to large datasets difficult or infeasible. Recent work on scaling Bayesian inference has focused on modifying the underlying algorithms to, for example, use only a random data subsample at each iteration. We leverage the insight that data is often redundant to instead obtain a weighted subset of the data (called a coreset) that is much smaller than the original dataset. We can then use this small coreset in any number of existing posterior inference algorithms without modification. In this paper, we develop an efficient coreset construction algorithm for Bayesian logistic regression models. We provide theoretical guarantees on the size and approximation quality of the coreset -- both for fixed, known datasets, and in expectation for a wide class of data generative models. Crucially, the proposed approach also permits efficient construction of the coreset in both streaming and parallel settings, with minimal additional effort. We demonstrate the efficacy of our approach on a number of synthetic and real-world datasets, and find that, in practice, the size of the coreset is independent of the original dataset size. Furthermore, constructing the coreset takes a negligible amount of time compared to that required to run MCMC on it.\n",
      "------------ 24 2018_304 -------------\n",
      "Hierarchical reinforcement learning (HRL) is a promising approach to extend traditional reinforcement learning (RL) methods to solve more complex tasks. Yet, the majority of current HRL methods require careful task-specific design and on-policy training, making them difficult to apply in real-world scenarios. In this paper, we study how we can develop HRL algorithms that are general, in that they do not make onerous additional assumptions beyond standard RL algorithms, and efficient, in the sense that they can be used with modest numbers of interaction samples, making them suitable for real-world problems such as robotic control. For generality, we develop a scheme where lower-level controllers are supervised with goals that are learned and proposed automatically by the higher-level controllers. To address efficiency, we propose to use off-policy experience for both higher- and lower-level training. This poses a considerable challenge, since changes to the lower-level behaviors change the action space for the higher-level policy, and we introduce an off-policy correction to remedy this challenge. This allows us to take advantage of recent advances in off-policy model-free RL to learn both higher and lower-level policies using substantially fewer environment interactions than on-policy algorithms. We find that our resulting HRL agent is generally applicable and highly sample-efficient. Our experiments show that our method can be used to learn highly complex behaviors for simulated robots, such as pushing objects and utilizing them to reach target locations, learning from only a few million samples, equivalent to a few days of real-time interaction. In comparisons with a number of prior HRL methods, we find that our approach substantially outperforms previous state-of-the-art techniques.\n",
      "------------ 25 2018_693 -------------\n",
      "Knowledge distillation is effective to train the small and generalisable network models for meeting the low-memory and fast running requirements. Existing offline distillation methods rely on a strong pre-trained teacher, which enables favourable knowledge discovery and transfer but requires a complex two-phase training procedure. Online counterparts address this limitation at the price of lacking a high-capacity teacher. In this work, we present an On-the-fly Native Ensemble (ONE) learning strategy for one-stage online distillation. Specifically, ONE only trains a single multi-branch network while simultaneously establishing a strong teacher on-the-fly to enhance the learning of target network. Extensive evaluations show that ONE improves the generalisation performance of a variety of deep neural networks more significantly than alternative methods on four image classification dataset: CIFAR10, CIFAR100, SVHN, and ImageNet, whilst having the computational efficiency advantages.\n",
      "------------ 26 2017_321 -------------\n",
      "Storing data in synthetic DNA offers the possibility of improving information density and durability by several orders of magnitude compared to current storage technologies. However, DNA data storage requires a computationally intensive process to retrieve the data. In particular, a crucial step in the data retrieval pipeline involves clustering billions of strings with respect to edit distance. Datasets in this domain have many notable properties, such as containing a very large number of small clusters that are well-separated in the edit distance metric space. In this regime, existing algorithms are unsuitable because of either their long running time or low accuracy. To address this issue, we present a novel distributed algorithm for approximately computing the underlying clusters. Our algorithm converges efficiently on any dataset that satisfies certain separability properties, such as those coming from DNA data storage systems. We also prove that, under these assumptions, our algorithm is robust to outliers and high levels of noise. We provide empirical justification of the accuracy, scalability, and convergence of our algorithm on real and synthetic data. Compared to the state-of-the-art algorithm for clustering DNA sequences, our algorithm simultaneously achieves higher accuracy and a 1000x speedup on three real datasets.\n",
      "------------ 27 2017_81 -------------\n",
      "In recent years, great progress has been made in a variety of application domains thanks to the development of increasingly deeper neural networks. Unfortunately, the huge number of units of these networks makes them expensive both computationally and memory-wise. To overcome this, exploiting the fact that deep networks are over-parametrized, several compression strategies have been proposed. These methods, however, typically start from a network that has been trained in a standard manner, without considering such a future compression. In this paper, we propose to explicitly account for compression in the training process. To this end, we introduce a regularizer that encourages the parameter matrix of each layer to have low rank during training. We show that accounting for compression during training allows us to learn much more compact, yet at least as effective, models than state-of-the-art compression techniques.\n",
      "------------ 28 2019_1144 -------------\n",
      "A key challenge in crowdsourcing is inferring the ground truth from noisy and unreliable data. To do so, existing approaches rely on collecting redundant information from the crowd, and aggregating it with some probabilistic method. However, oftentimes such methods are computationally inefficient, are restricted to some specific settings, or lack theoretical guarantees. In this paper, we revisit the problem of binary classification from crowdsourced data. Specifically we propose Streaming Bayesian Inference for Crowdsourcing (SBIC), a new algorithm that does not suffer from any of these limitations. First, SBIC has low complexity and can be used in a real-time online setting. Second, SBIC has the same accuracy as the best state-of-the-art algorithms in all settings. Third, SBIC has provable asymptotic guarantees both in the online and offline settings.\n",
      "------------ 29 2019_488 -------------\n",
      "Gradient Boosting (\\GB) is a popular and very successful ensemble method for binary trees. While various types of regularization of the base predictors are used with this algorithm, the theory that connects such regularizations with generalization guarantees is poorly understood. We fill this gap by deriving data-dependent learning guarantees for \\GB\\ used with \\emph{regularization}, expressed in terms of the Rademacher complexities of the constrained families of base predictors. We introduce a new algorithm, called \\rgb\\, that directly benefits from these generalization bounds and that, at every boosting round, applies the \\emph{Structural Risk Minimization} principle to search for a base predictor with the best empirical fit versus complexity trade-off. Inspired by \\emph{Randomized Coordinate Descent} we provide a scalable implementation of our algorithm, able to search over large families of base predictors. Finally, we provide experimental results, demonstrating that our algorithm achieves significantly better out-of-sample performance on multiple datasets than the standard \\GB\\ algorithm used with its regularization.\n",
      "------------ 30 2019_2 -------------\n",
      "In deep neural nets, lower level embedding layers account for a large portion of the total number of parameters. Tikhonov regularization, graph-based regularization, and hard parameter sharing are approaches that introduce explicit biases into training in a hope to reduce statistical complexity. Alternatively, we propose stochastically shared embeddings (SSE), a data-driven approach to regularizing embedding layers, which stochastically transitions between embeddings during stochastic gradient descent (SGD). Because SSE integrates seamlessly with existing SGD algorithms, it can be used with only minor modifications when training large scale neural networks. We develop two versions of SSE: SSE-Graph using knowledge graphs of embeddings; SSE-SE using no prior information. We provide theoretical guarantees for our method and show its empirical effectiveness on 6 distinct tasks, from simple neural networks with one hidden layer in recommender systems, to the transformer and BERT in natural languages. We find that when used along with widely-used regularization methods such as weight decay and dropout, our proposed SSE can further reduce overfitting, which often leads to more favorable generalization results.\n",
      "------------ 31 2019_1276 -------------\n",
      "We consider constraint-based methods for causal structure learning, such as the PC algorithm or any PC-derived algorithms whose ﬁrst step consists in pruning a complete graph to obtain an undirected graph skeleton, which is subsequently oriented. All constraint-based methods perform this ﬁrst step of removing dispensable edges, iteratively, whenever a separating set and corresponding conditional independence can be found. Yet, constraint-based methods lack robustness over sampling noise and are prone to uncover spurious conditional independences in ﬁnite datasets. In particular, there is no guarantee that the separating sets identiﬁed during the iterative pruning step remain consistent with the ﬁnal graph. In this paper, we propose a simple modiﬁcation of PC and PC-derived algorithms so as to ensure that all separating sets identiﬁed to remove dispensable edges are consistent with the ﬁnal graph,thus enhancing the explainability of constraint-basedmethods. It is achieved by repeating the constraint-based causal structure learning scheme, iteratively, while searching for separating sets that are consistent with the graph obtained at the previous iteration. Ensuring the consistency of separating sets can be done at a limited complexity cost, through the use of block-cut tree decomposition of graph skeletons, and is found to increase their validity in terms of actual d-separation. It also signiﬁcantly improves the sensitivity of constraint-based methods while retaining good overall structure learning performance. Finally and foremost, ensuring sepset consistency improves the interpretability of constraint-based models for real-life applications.\n",
      "------------ 32 2018_281 -------------\n",
      "We introduce a principled approach for unsupervised structure learning of deep neural networks. We propose a new interpretation for depth and inter-layer connectivity where conditional independencies in the input distribution are encoded hierarchically in the network structure. Thus, the depth of the network is determined inherently. The proposed method casts the problem of neural network structure learning as a problem of Bayesian network structure learning. Then, instead of directly learning the discriminative structure, it learns a generative graph, constructs its stochastic inverse, and then constructs a discriminative graph. We prove that conditional-dependency relations among the latent variables in the generative graph are preserved in the class-conditional discriminative graph. We demonstrate on image classification benchmarks that the deepest layers (convolutional and dense) of common networks can be replaced by significantly smaller learned structures, while maintaining classification accuracy---state-of-the-art on tested benchmarks. Our structure learning algorithm requires a small computational cost and runs efficiently on a standard desktop CPU.\n",
      "------------ 33 2015_105 -------------\n",
      "Recently, there has been growing interest in systematic search-based and importance sampling-based lifted inference algorithms for statistical relational models (SRMs). These lifted algorithms achieve significant complexity reductions over their propositional counterparts by using lifting rules that leverage symmetries in the relational representation. One drawback of these algorithms is that they use an inference-blind representation of the search space, which makes it difficult to efficiently pre-compute tight upper bounds on the exact cost of inference without running the algorithm to completion. In this paper, we present a principled approach to address this problem. We introduce a lifted analogue of the propositional And/Or search space framework, which we call a lifted And/Or schematic. Given a schematic-based representation of an SRM, we show how to efficiently compute a tight upper bound on the time and space cost of exact inference from a current assignment and the remaining schematic. We show how our bounding method can be used within a lifted importance sampling algorithm, in order to perform effective Rao-Blackwellisation, and demonstrate experimentally that the Rao-Blackwellised version of the algorithm yields more accurate estimates on several real-world datasets.\n",
      "------------ 34 2019_857 -------------\n",
      "Recent work has shown how to embed differentiable optimization problems (that is, problems whose solutions can be backpropagated through) as layers within deep learning architectures. This method provides a useful inductive bias for certain problems, but existing software for differentiable optimization layers is rigid and difficult to apply to new settings. In this paper, we propose an approach to differentiating through disciplined convex programs, a subclass of convex optimization problems used by domain-specific languages (DSLs) for convex optimization. We introduce disciplined parametrized programming, a subset of disciplined convex programming, and we show that every disciplined parametrized program can be represented as the composition of an affine map from parameters to problem data, a solver, and an affine map from the solver’s solution to a solution of the original problem (a new form we refer to as affine-solver-affine form). We then demonstrate how to efficiently differentiate through each of these components, allowing for end-to-end analytical differentiation through the entire convex program. We implement our methodology in version 1.1 of CVXPY, a popular Python-embedded DSL for convex optimization, and additionally implement differentiable layers for disciplined convex programs in PyTorch and TensorFlow 2.0. Our implementation significantly lowers the barrier to using convex optimization problems in differentiable programs. We present applications in linear machine learning models and in stochastic control, and we show that our layer is competitive (in execution time) compared to specialized differentiable solvers from past work.\n",
      "------------ 35 2018_183 -------------\n",
      "We present SplineNets, a practical and novel approach for using conditioning in convolutional neural networks (CNNs). SplineNets are continuous generalizations of neural decision graphs, and they can dramatically reduce runtime complexity and computation costs of CNNs, while maintaining or even increasing accuracy. Functions of SplineNets are both dynamic (i.e., conditioned on the input) and hierarchical (i.e.,conditioned on the computational path). SplineNets employ a unified loss function with a desired level of smoothness over both the network and decision parameters, while allowing for sparse activation of a subset of nodes for individual samples. In particular, we embed infinitely many function weights (e.g. filters) on smooth, low dimensional manifolds parameterized by compact B-splines, which are indexed by a position parameter. Instead of sampling from a categorical distribution to pick a branch, samples choose a continuous position to pick a function weight. We further show that by maximizing the mutual information between spline positions and class labels, the network can be optimally utilized and specialized for classification tasks. Experiments show that our approach can significantly increase the accuracy of ResNets with negligible cost in speed, matching the precision of a 110 level ResNet with a 32 level SplineNet.\n",
      "------------ 36 2018_965 -------------\n",
      "We address the problem of Bayesian structure learning for domains with hundreds of variables by employing non-parametric bootstrap, recursively. We propose a method that covers both model averaging and model selection in the same framework. The proposed method deals with the main weakness of constraint-based learning---sensitivity to errors in the independence tests---by a novel way of combining bootstrap with constraint-based learning. Essentially, we provide an algorithm for learning a tree, in which each node represents a scored CPDAG for a subset of variables and the level of the node corresponds to the maximal order of conditional independencies that are encoded in the graph. As higher order independencies are tested in deeper recursive calls, they benefit from more bootstrap samples, and therefore are more resistant to the curse-of-dimensionality. Moreover, the re-use of stable low order independencies allows greater computational efficiency. We also provide an algorithm for sampling CPDAGs efficiently from their posterior given the learned tree. That is, not from the full posterior, but from a reduced space of CPDAGs encoded in the learned tree. We empirically demonstrate that the proposed algorithm scales well to hundreds of variables, and learns better MAP models and more reliable causal relationships between variables, than other state-of-the-art-methods.\n",
      "------------ 37 2015_6 -------------\n",
      "We study the problem of multiclass classification with an extremely large number of classes (k), with the goal of obtaining train and test time complexity logarithmic in the number of classes. We develop top-down tree construction approaches for constructing logarithmic depth trees. On the theoretical front, we formulate a new objective function, which is optimized at each node of the tree and creates dynamic partitions of the data which are both pure (in terms of class labels) and balanced. We demonstrate that under favorable conditions, we can construct logarithmic depth trees that have leaves with low label entropy. However, the objective function at the nodes is challenging to optimize computationally. We address the empirical problem with a new online decision tree construction procedure. Experiments demonstrate that this online algorithm quickly achieves improvement in test error compared to more common logarithmic training time approaches, which makes it a plausible method in computationally constrained large-k applications.\n",
      "------------ 38 2019_1196 -------------\n",
      "Graph convolution is the core of most Graph Neural Networks (GNNs) and usually approximated by message passing between direct (one-hop) neighbors. In this work, we remove the restriction of using only the direct neighbors by introducing a powerful, yet spatially localized graph convolution: Graph diffusion convolution (GDC). GDC leverages generalized graph diffusion, examples of which are the heat kernel and personalized PageRank. It alleviates the problem of noisy and often arbitrarily defined edges in real graphs. We show that GDC is closely related to spectral-based models and thus combines the strengths of both spatial (message passing) and spectral methods. We demonstrate that replacing message passing with graph diffusion convolution consistently leads to significant performance improvements across a wide range of models on both supervised and unsupervised tasks and a variety of datasets. Furthermore, GDC is not limited to GNNs but can trivially be combined with any graph-based model or algorithm (e.g. spectral clustering) without requiring any changes to the latter or affecting its computational complexity. Our implementation is available online.\n",
      "------------ 39 2017_658 -------------\n",
      "Bayesian online algorithms for Sum-Product Networks (SPNs) need to update their posterior distribution after seeing one single additional instance. To do so, they must compute moments of the model parameters under this distribution. The best existing method for computing such moments scales quadratically in the size of the SPN, although it scales linearly for trees. This unfortunate scaling makes Bayesian online algorithms prohibitively expensive, except for small or tree-structured SPNs. We propose an optimal linear-time algorithm that works even when the SPN is a general directed acyclic graph (DAG), which significantly broadens the applicability of Bayesian online algorithms for SPNs. There are three key ingredients in the design and analysis of our algorithm: 1). For each edge in the graph, we construct a linear time reduction from the moment computation problem to a joint inference problem in SPNs. 2). Using the property that each SPN computes a multilinear polynomial, we give an efficient procedure for polynomial evaluation by differentiation without expanding the network that may contain exponentially many monomials. 3). We propose a dynamic programming method to further reduce the computation of the moments of all the edges in the graph from quadratic to linear. We demonstrate the usefulness of our linear time algorithm by applying it to develop a linear time assume density filter (ADF) for SPNs.\n",
      "------------ 40 2017_79 -------------\n",
      "Label distribution learning (LDL) is a general learning framework, which assigns to an instance a distribution over a set of labels rather than a single label or multiple labels. Current LDL methods have either restricted assumptions on the expression form of the label distribution or limitations in representation learning, e.g., to learn deep features in an end-to-end manner. This paper presents label distribution learning forests (LDLFs) - a novel label distribution learning algorithm based on differentiable decision trees, which have several advantages: 1) Decision trees have the potential to model any general form of label distributions by a mixture of leaf node predictions. 2) The learning of differentiable decision trees can be combined with representation learning. We define a distribution-based loss function for a forest, enabling all the trees to be learned jointly, and show that an update function for leaf node predictions, which guarantees a strict decrease of the loss function, can be derived by variational bounding. The effectiveness of the proposed LDLFs is verified on several LDL tasks and a computer vision application, showing significant improvements to the state-of-the-art LDL methods.\n",
      "------------ 41 2019_78 -------------\n",
      "Neural architecture search (NAS) is inherently subject to the gap of architectures during searching and validating. To bridge this gap, we develop Differentiable ArchiTecture Approximation (DATA) with an Ensemble Gumbel-Softmax (EGS) estimator to automatically approximate architectures during searching and validating in a differentiable manner. Technically, the EGS estimator consists of a group of Gumbel-Softmax estimators, which is capable of converting probability vectors to binary codes and passing gradients from binary codes to probability vectors. Benefiting from such modeling, in searching, architecture parameters and network weights in the NAS model can be jointly optimized with the standard back-propagation, yielding an end-to-end learning mechanism for searching deep models in a large enough search space. Conclusively, during validating, a high-performance architecture that approaches to the learned one during searching is readily built. Extensive experiments on a variety of popular datasets strongly evidence that our method is capable of discovering high-performance architectures for image classification, language modeling and semantic segmentation, while guaranteeing the requisite efficiency during searching.\n",
      "------------ 42 2016_281 -------------\n",
      "We present a general theoretical analysis of structured prediction with a series of new results. We give new data-dependent margin guarantees for structured prediction for a very wide family of loss functions and a general family of hypotheses, with an arbitrary factor graph decomposition. These are the tightest margin bounds known for both standard multi-class and general structured prediction problems. Our guarantees are expressed in terms of a data-dependent complexity measure, \\emph{factor graph complexity}, which we show can be estimated from data and bounded in terms of familiar quantities for several commonly used hypothesis sets, and a sparsity measure for features and graphs. Our proof techniques include generalizations of Talagrand's contraction lemma that can be of independent interest. We further extend our theory by leveraging the principle of Voted Risk Minimization (VRM) and show that learning is possible even with complex factor graphs. We present new learning bounds for this advanced setting, which we use to devise two new algorithms, \\emph{Voted Conditional Random Field} (VCRF) and \\emph{Voted Structured Boosting} (StructBoost). These algorithms can make use of complex features and factor graphs and yet benefit from favorable learning guarantees. We also report the results of experiments with VCRF on several datasets to validate our theory.\n",
      "------------ 43 2017_362 -------------\n",
      "We introduce deep neural networks for end-to-end differentiable theorem proving that operate on dense vector representations of symbols. These neural networks are recursively constructed by following the backward chaining algorithm as used in Prolog. Specifically, we replace symbolic unification with a differentiable computation on vector representations of symbols using a radial basis function kernel, thereby combining symbolic reasoning with learning subsymbolic vector representations. The resulting neural network can be trained to infer facts from a given incomplete knowledge base using gradient descent. By doing so, it learns to (i) place representations of similar symbols in close proximity in a vector space, (ii) make use of such similarities to prove facts, (iii) induce logical rules, and (iv) it can use provided and induced logical rules for complex multi-hop reasoning. On four benchmark knowledge bases we demonstrate that this architecture outperforms ComplEx, a state-of-the-art neural link prediction model, while at the same time inducing interpretable function-free first-order logic rules.\n",
      "------------ 44 2019_382 -------------\n",
      "We propose a new family of efficient and expressive deep generative models of graphs, called Graph Recurrent Attention Networks (GRANs). Our model generates graphs one block of nodes and associated edges at a time. The block size and sampling stride allow us to trade off sample quality for efficiency. Compared to previous RNN-based graph generative models, our framework better captures the auto-regressive conditioning between the already-generated and to-be-generated parts of the graph using Graph Neural Networks (GNNs) with attention. This not only reduces the dependency on node ordering but also bypasses the long-term bottleneck caused by the sequential nature of RNNs. Moreover, we parameterize the output distribution per block using a mixture of Bernoulli, which captures the correlations among generated edges within the block. Finally, we propose to handle node orderings in generation by marginalizing over a family of canonical orderings. On standard benchmarks, we achieve state-of-the-art time efficiency and sample quality compared to previous models. Additionally, we show our model is capable of generating large graphs of up to 5K nodes with good quality. Our code is released at: \\url{https://github.com/lrjconan/GRAN}.\n",
      "------------ 45 2019_563 -------------\n",
      "Search-based methods for hard combinatorial optimization are often guided by heuristics. Tuning heuristics in various conditions and situations is often time-consuming. In this paper, we propose NeuRewriter that learns a policy to pick heuristics and rewrite the local components of the current solution to iteratively improve it until convergence. The policy factorizes into a region-picking and a rule-picking component, each parameterized by a neural network trained with actor-critic methods in reinforcement learning. NeuRewriter captures the general structure of combinatorial problems and shows strong performance in three versatile tasks: expression simplification, online job scheduling and vehicle routing problems. NeuRewriter outperforms the expression simplification component in Z3; outperforms DeepRM and Google OR-tools in online job scheduling; and outperforms recent neural baselines and Google OR-tools in vehicle routing problems.\n",
      "------------ 46 2019_630 -------------\n",
      "We develop BatchBALD, a tractable approximation to the mutual information between a batch of points and model parameters, which we use as an acquisition function to select multiple informative points jointly for the task of deep Bayesian active learning. BatchBALD is a greedy linear-time 1 - \\nicefrac{1}{e}\n",
      "-approximate algorithm amenable to dynamic programming and efficient caching. We compare BatchBALD to the commonly used approach for batch data acquisition and find that the current approach acquires similar and redundant points, sometimes performing worse than randomly acquiring data. We finish by showing that, using BatchBALD to consider dependencies within an acquisition batch, we achieve new state of the art performance on standard benchmarks, providing substantial data efficiency improvements in batch acquisition.\n",
      "------------ 47 2019_1060 -------------\n",
      "Graph neural networks (GNNs) are effective models for many dynamical systems consisting of entities and relations. Although most GNN applications assume a single type of entity and relation, many situations involve multiple types of interactions. Relational inference is the problem of inferring these interactions and learning the dynamics from observational data. We frame relational inference as a modular meta-learning problem, where neural modules are trained to be composed in different ways to solve many tasks. This meta-learning framework allows us to implicitly encode time invariance and infer relations in context of one another rather than independently, which increases inference capacity. Framing inference as the inner-loop optimization of meta-learning leads to a model-based approach that is more data-efficient and capable of estimating the state of entities that we do not observe directly, but whose existence can be inferred from their effect on observed entities. To address the large search space of graph neural network compositions, we meta-learn a proposal function that speeds up the inner-loop simulated annealing search within the modular meta-learning algorithm, providing two orders of magnitude increase in the size of problems that can be addressed.\n",
      "------------ 48 2017_576 -------------\n",
      "We introduce an efficient algorithmic framework for model selection in online learning, also known as parameter-free online learning. Departing from previous work, which has focused on highly structured function classes such as nested balls in Hilbert space, we propose a generic meta-algorithm framework that achieves online model selection oracle inequalities under minimal structural assumptions. We give the first computationally efficient parameter-free algorithms that work in arbitrary Banach spaces under mild smoothness assumptions; previous results applied only to Hilbert spaces. We further derive new oracle inequalities for matrix classes, non-nested convex sets, and \\mathbb{R}^{d}\n",
      "with generic regularizers. Finally, we generalize these results by providing oracle inequalities for arbitrary non-linear classes in the online supervised learning model. These results are all derived through a unified meta-algorithm scheme using a novel \"multi-scale\" algorithm for prediction with expert advice based on random playout, which may be of independent interest.\n",
      "------------ 49 2019_539 -------------\n",
      "We propose a novel spectral convolutional neural network (CNN) model on graph structured data, namely Distributed Feedback-Looped Networks (DFNets). This model is incorporated with a robust class of spectral graph filters, called feedback-looped filters, to provide better localization on vertices, while still attaining fast convergence and linear memory requirements. Theoretically, feedback-looped filters can guarantee convergence w.r.t. a specified error bound, and be applied universally to any graph without knowing its structure. Furthermore, the propagation rule of this model can diversify features from the preceding layers to produce strong gradient flows. We have evaluated our model using two benchmark tasks: semi-supervised document classification on citation networks and semi-supervised entity classification on a knowledge graph. The experimental results show that our model considerably outperforms the state-of-the-art methods in both benchmark tasks over all datasets.\n",
      "------------ 50 2015_230 -------------\n",
      "We introduce Natural Neural Networks, a novel family of algorithms that speed up convergence by adapting their internal representation during training to improve conditioning of the Fisher matrix. In particular, we show a specific example that employs a simple and efficient reparametrization of the neural network weights by implicitly whitening the representation obtained at each layer, while preserving the feed-forward computation of the network. Such networks can be trained efficiently via the proposed Projected Natural Gradient Descent algorithm (PRONG), which amortizes the cost of these reparametrizations over many parameter updates and is closely related to the Mirror Descent online learning algorithm. We highlight the benefits of our method on both unsupervised and supervised learning tasks, and showcase its scalability by training on the large-scale ImageNet Challenge dataset.\n",
      "------------ 51 2017_353 -------------\n",
      "Matrix completion models are among the most common formulations of recommender systems. Recent works have showed a boost of performance of these techniques when introducing the pairwise relationships between users/items in the form of graphs, and imposing smoothness priors on these graphs. However, such techniques do not fully exploit the local stationary structures on user/item graphs, and the number of parameters to learn is linear w.r.t. the number of users and items. We propose a novel approach to overcome these limitations by using geometric deep learning on graphs. Our matrix completion architecture combines a novel multi-graph convolutional neural network that can learn meaningful statistical graph-structured patterns from users and items, and a recurrent neural network that applies a learnable diffusion on the score matrix. Our neural network system is computationally attractive as it requires a constant number of parameters independent of the matrix size. We apply our method on several standard datasets, showing that it outperforms state-of-the-art matrix completion techniques.\n",
      "------------ 52 2018_627 -------------\n",
      "Many structured prediction problems admit a natural loss function for evaluation such as the edit-distance or n\n",
      "-gram loss. However, existing learning algorithms are typically designed to optimize alternative objectives such as the cross-entropy. This is because a na\\\"{i}ve implementation of the natural loss functions often results in intractable gradient computations. In this paper, we design efficient gradient computation algorithms for two broad families of structured prediction loss functions: rational and tropical losses. These families include as special cases the n\n",
      "-gram loss, the edit-distance loss, and many other loss functions commonly used in natural language processing and computational biology tasks that are based on sequence similarity measures. Our algorithms make use of weighted automata and graph operations over appropriate semirings to design efficient solutions. They facilitate efficient gradient computation and hence enable one to train learning models such as neural networks with complex structured losses.\n",
      "------------ 53 2018_714 -------------\n",
      "A fundamental problem in program verification concerns inferring loop invariants. The problem is undecidable and even practical instances are challenging. Inspired by how human experts construct loop invariants, we propose a reasoning framework Code2Inv that constructs the solution by multi-step decision making and querying an external program graph memory block. By training with reinforcement learning, Code2Inv captures rich program features and avoids the need for ground truth solutions as supervision. Compared to previous learning tasks in domains with graph-structured data, it addresses unique challenges, such as a binary objective function and an extremely sparse reward that is given by an automated theorem prover only after the complete loop invariant is proposed. We evaluate Code2Inv on a suite of 133 benchmark problems and compare it to three state-of-the-art systems. It solves 106 problems compared to 73 by a stochastic search-based system, 77 by a heuristic search-based system, and 100 by a decision tree learning-based system. Moreover, the strategy learned can be generalized to new programs: compared to solving new instances from scratch, the pre-trained agent is more sample efficient in finding solutions.\n",
      "------------ 54 2019_351 -------------\n",
      "Tremendous amount of parameters make deep neural networks impractical to be deployed for edge-device-based real-world applications due to the limit of computational power and storage space. Existing studies have made progress on learning quantized deep models to reduce model size and energy consumption, i.e. converting full-precision weights (r\n",
      "'s) into discrete values (q\n",
      "'s) in a supervised training manner. However, the training process for quantization is non-differentiable, which leads to either infinite or zero gradients (g_r\n",
      ") w.r.t. r\n",
      ". To address this problem, most training-based quantization methods use the gradient w.r.t. q\n",
      "(g_q\n",
      ") with clipping to approximate g_r\n",
      "by Straight-Through-Estimator (STE) or manually design their computation. However, these methods only heuristically make training-based quantization applicable, without further analysis on how the approximated gradients can assist training of a quantized network. In this paper, we propose to learn g_r\n",
      "by a neural network. Specifically, a meta network is trained using g_q\n",
      "and r\n",
      "as inputs, and outputs g_r\n",
      "for subsequent weight updates. The meta network is updated together with the original quantized network. Our proposed method alleviates the problem of non-differentiability, and can be trained in an end-to-end manner. Extensive experiments are conducted with CIFAR10/100 and ImageNet on various deep networks to demonstrate the advantage of our proposed method in terms of a faster convergence rate and better performance. Codes are released at: \\texttt{https://github.com/csyhhu/MetaQuant}\n",
      "------------ 55 2019_955 -------------\n",
      "We develop a progressive training approach for neural networks which adaptively grows the network structure by splitting existing neurons to multiple off-springs. By leveraging a functional steepest descent idea, we derive a simple criterion for deciding the best subset of neurons to split and a \\emph{splitting gradient} for optimally updating the off-springs. Theoretically, our splitting strategy is a second order functional steepest descent for escaping saddle points in an \\Linfty\n",
      "-Wasserstein metric space, on which the standard parametric gradient descent is a first-order steepest descent. Our method provides a new computationally efficient approach for optimizing neural network structures, especially for learning lightweight neural architectures in resource-constrained settings.\n",
      "------------ 56 2018_963 -------------\n",
      "The broad set of deep generative models (DGMs) has achieved remarkable advances. However, it is often difficult to incorporate rich structured domain knowledge with the end-to-end DGMs. Posterior regularization (PR) offers a principled framework to impose structured constraints on probabilistic models, but has limited applicability to the diverse DGMs that can lack a Bayesian formulation or even explicit density evaluation. PR also requires constraints to be fully specified {\\it a priori}, which is impractical or suboptimal for complex knowledge with learnable uncertain parts. In this paper, we establish mathematical correspondence between PR and reinforcement learning (RL), and, based on the connection, expand PR to learn constraints as the extrinsic reward in RL. The resulting algorithm is model-agnostic to apply to any DGMs, and is flexible to adapt arbitrary constraints with the model jointly. Experiments on human image generation and templated sentence generation show models with learned knowledge constraints by our algorithm greatly improve over base generative models.\n",
      "------------ 57 2019_781 -------------\n",
      "Graph-based algorithms are among the most successful paradigms for solving semi-supervised learning tasks. Recent work on graph convolutional networks and neural graph learning methods has successfully combined the expressiveness of neural networks with graph structures. We propose a technique that, when applied to these methods, achieves state-of-the-art results on semi-supervised learning datasets. Traditional graph-based algorithms, such as label propagation, were designed with the underlying assumption that the label of a node can be imputed from that of the neighboring nodes. However, real-world graphs are either noisy or have edges that do not correspond to label agreement. To address this, we propose Graph Agreement Models (GAM), which introduces an auxiliary model that predicts the probability of two nodes sharing the same label as a learned function of their features. The agreement model is used when training a node classification model by encouraging agreement only for the pairs of nodes it deems likely to have the same label, thus guiding its parameters to better local optima. The classification and agreement models are trained jointly in a co-training fashion. Moreover, GAM can also be applied to any semi-supervised classification problem, by inducing a graph whenever one is not provided. We demonstrate that our method achieves a relative improvement of up to 72% for various node classification models, and obtains state-of-the-art results on multiple established datasets.\n",
      "------------ 58 2019_255 -------------\n",
      "Decentralized optimization is a powerful paradigm that finds applications in engineering and learning design. This work studies decentralized composite optimization problems with non-smooth regularization terms. Most existing gradient-based proximal decentralized methods are known to converge to the optimal solution with sublinear rates, and it remains unclear whether this family of methods can achieve global linear convergence. To tackle this problem, this work assumes the non-smooth regularization term is common across all networked agents, which is the case for many machine learning problems. Under this condition, we design a proximal gradient decentralized algorithm whose fixed point coincides with the desired minimizer. We then provide a concise proof that establishes its linear convergence. In the absence of the non-smooth term, our analysis technique covers the well known EXTRA algorithm and provides useful bounds on the convergence rate and step-size.\n",
      "------------ 59 2015_62 -------------\n",
      "We study the estimation of low rank matrices via nonconvex optimization. Compared with convex relaxation, nonconvex optimization exhibits superior empirical performance for large scale instances of low rank matrix estimation. However, the understanding of its theoretical guarantees are limited. In this paper, we define the notion of projected oracle divergence based on which we establish sufficient conditions for the success of nonconvex optimization. We illustrate the consequences of this general framework for matrix sensing and completion. In particular, we prove that a broad class of nonconvex optimization algorithms, including alternating minimization and gradient-type methods, geometrically converge to the global optimum and exactly recover the true low rank matrices under standard conditions.\n",
      "------------ 60 2016_397 -------------\n",
      "The Hard Thresholding Pursuit (HTP) is a class of truncated gradient descent methods for finding sparse solutions of \\ell_0\n",
      "-constrained loss minimization problems. The HTP-style methods have been shown to have strong approximation guarantee and impressive numerical performance in high dimensional statistical learning applications. However, the current theoretical treatment of these methods has traditionally been restricted to the analysis of parameter estimation consistency. It remains an open problem to analyze the support recovery performance (a.k.a., sparsistency) of this type of methods for recovering the global minimizer of the original NP-hard problem. In this paper, we bridge this gap by showing, for the first time, that exact recovery of the global sparse minimizer is possible for HTP-style methods under restricted strong condition number bounding conditions. We further show that HTP-style methods are able to recover the support of certain relaxed sparse solutions without assuming bounded restricted strong condition number. Numerical results on simulated data confirms our theoretical predictions.\n",
      "------------ 61 2018_547 -------------\n",
      "Stochastic gradient descent (SGD) remains the method of choice for deep learning, despite the limitations arising for ill-behaved objective functions. In cases where it could be estimated, the natural gradient has proven very effective at mitigating the catastrophic effects of pathological curvature in the objective function, but little is known theoretically about its convergence properties, and it has yet to find a practical implementation that would scale to very deep and large networks. Here, we derive an exact expression for the natural gradient in deep linear networks, which exhibit pathological curvature similar to the nonlinear case. We provide for the first time an analytical solution for its convergence rate, showing that the loss decreases exponentially to the global minimum in parameter space. Our expression for the natural gradient is surprisingly simple, computationally tractable, and explains why some approximations proposed previously work well in practice. This opens new avenues for approximating the natural gradient in the nonlinear case, and we show in preliminary experiments that our online natural gradient descent outperforms SGD on MNIST autoencoding while sharing its computational simplicity.\n",
      "------------ 62 2018_653 -------------\n",
      "Generative Adversarial Networks (GANs) are one of the most practical methods for learning data distributions. A popular GAN formulation is based on the use of Wasserstein distance as a metric between probability distributions. Unfortunately, minimizing the Wasserstein distance between the data distribution and the generative model distribution is a computationally challenging problem as its objective is non-convex, non-smooth, and even hard to compute. In this work, we show that obtaining gradient information of the smoothed Wasserstein GAN formulation, which is based on regularized Optimal Transport (OT), is computationally effortless and hence one can apply first order optimization methods to minimize this objective. Consequently, we establish theoretical convergence guarantee to stationarity for a proposed class of GAN optimization algorithms. Unlike the original non-smooth formulation, our algorithm only requires solving the discriminator to approximate optimality. We apply our method to learning MNIST digits as well as CIFAR-10 images. Our experiments show that our method is computationally efficient and generates images comparable to the state of the art algorithms given the same architecture and computational power.\n",
      "------------ 63 2017_418 -------------\n",
      "Importance sampling has become an indispensable strategy to speed up optimization algorithms for large-scale applications. Improved adaptive variants -- using importance values defined by the complete gradient information which changes during optimization -- enjoy favorable theoretical properties, but are typically computationally infeasible. In this paper we propose an efficient approximation of gradient-based sampling, which is based on safe bounds on the gradient. The proposed sampling distribution is (i) provably the \\emph{best sampling} with respect to the given bounds, (ii) always better than uniform sampling and fixed importance sampling and (iii) can efficiently be computed -- in many applications at negligible extra cost. The proposed sampling scheme is generic and can easily be integrated into existing algorithms. In particular, we show that coordinate-descent (CD) and stochastic gradient descent (SGD) can enjoy significant a speed-up under the novel scheme. The proven efficiency of the proposed sampling is verified by extensive numerical testing.\n",
      "------------ 64 2015_33 -------------\n",
      "Maximum a-posteriori (MAP) inference is an important task for many applications. Although the standard formulation gives rise to a hard combinatorial optimization problem, several effective approximations have been proposed and studied in recent years. We focus on linear programming (LP) relaxations, which have achieved state-of-the-art performance in many applications. However, optimization of the resulting program is in general challenging due to non-smoothness and complex non-separable constraints.Therefore, in this work we study the benefits of augmenting the objective function of the relaxation with strong convexity. Specifically, we introduce strong convexity by adding a quadratic term to the LP relaxation objective. We provide theoretical guarantees for the resulting programs, bounding the difference between their optimal value and the original optimum. Further, we propose suitable optimization algorithms and analyze their convergence.\n",
      "------------ 65 2017_535 -------------\n",
      "Despite the growing prominence of generative adversarial networks (GANs), optimization in GANs is still a poorly understood topic. In this paper, we analyze the\n",
      "gradient descent'' form of GAN optimization, i.e., the natural setting where we simultaneously take small gradient steps in both generator and discriminator parameters. We show that even though GAN optimization does \\emph{not} correspond to a convex-concave game (even for simple parameterizations), under proper conditions, equilibrium points of this optimization procedure are still \\emph{locally asymptotically stable} for the traditional GAN formulation. On the other hand, we show that the recently proposed Wasserstein GAN can have non-convergent limit cycles near equilibrium. Motivated by this stability analysis, we propose an additional regularization term for gradient descent GAN updates, which \\emph{is} able to guarantee local stability for both the WGAN and the traditional GAN, and also shows practical promise in speeding up convergence and addressing mode collapse.\n",
      "------------ 66 2019_1125 -------------\n",
      "In this paper, we introduce various mechanisms to obtain accelerated first-order stochastic optimization algorithms when the objective function is convex or strongly convex. Specifically, we extend the Catalyst approach originally designed for deterministic objectives to the stochastic setting. Given an optimization method with mild convergence guarantees for strongly convex problems, the challenge is to accelerate convergence to a noise-dominated region, and then achieve convergence with an optimal worst-case complexity depending on the noise variance of the gradients. A side contribution of our work is also a generic analysis that can handle inexact proximal operators, providing new insights about the robustness of stochastic algorithms when the proximal operator cannot be exactly computed.\n",
      "------------ 67 2019_353 -------------\n",
      "Wasserstein distance-based distributionally robust optimization (DRO) has received much attention lately due to its ability to provide a robustness interpretation of various learning models. Moreover, many of the DRO problems that arise in the learning context admits exact convex reformulations and hence can be tackled by off-the-shelf solvers. Nevertheless, the use of such solvers severely limits the applicability of DRO in large-scale learning problems, as they often rely on general purpose interior-point algorithms. On the other hand, there are very few works that attempt to develop fast iterative methods to solve these DRO problems, which typically possess complicated structures. In this paper, we take a first step towards resolving the above difficulty by developing a first-order algorithmic framework for tackling a class of Wasserstein distance-based distributionally robust logistic regression (DRLR) problem. Specifically, we propose a novel linearized proximal ADMM to solve the DRLR problem, whose objective is convex but consists of a smooth term plus two non-separable non-smooth terms. We prove that our method enjoys a sublinear convergence rate. Furthermore, we conduct three different experiments to show its superb performance on both synthetic and real-world datasets. In particular, our method can achieve the same accuracy up to 800+ times faster than the standard off-the-shelf solver.\n",
      "------------ 68 2017_5 -------------\n",
      "Due to their simplicity and excellent performance, parallel asynchronous variants of stochastic gradient descent have become popular methods to solve a wide range of large-scale optimization problems on multi-core architectures. Yet, despite their practical success, support for nonsmooth objectives is still lacking, making them unsuitable for many problems of interest in machine learning, such as the Lasso, group Lasso or empirical risk minimization with convex constraints. In this work, we propose and analyze ProxASAGA, a fully asynchronous sparse method inspired by SAGA, a variance reduced incremental gradient algorithm. The proposed method is easy to implement and significantly outperforms the state of the art on several nonsmooth, large-scale problems. We prove that our method achieves a theoretical linear speedup with respect to the sequential version under assumptions on the sparsity of gradients and block-separability of the proximal term. Empirical benchmarks on a multi-core architecture illustrate practical speedups of up to 12x on a 20-core machine.\n",
      "------------ 69 2019_439 -------------\n",
      "The exploration bonus is an effective approach to manage the exploration-exploitation trade-off in Markov Decision Processes (MDPs). While it has been analyzed in infinite-horizon discounted and finite-horizon problems, we focus on designing and analysing the exploration bonus in the more challenging infinite-horizon undiscounted setting. We first introduce SCAL+, a variant of SCAL (Fruit et al. 2018), that uses a suitable exploration bonus to solve any discrete unknown weakly-communicating MDP for which an upper bound c\n",
      "on the span of the optimal bias function is known. We prove that SCAL+ enjoys the same regret guarantees as SCAL, which relies on the less efficient extended value iteration approach. Furthermore, we leverage the flexibility provided by the exploration bonus scheme to generalize SCAL+ to smooth MDPs with continuous state space and discrete actions. We show that the resulting algorithm (SCCAL+) achieves the same regret bound as UCCRL (Ortner and Ryabko, 2012) while being the first implementable algorithm for this setting.\n",
      "------------ 70 2015_330 -------------\n",
      "Deep learning presents notorious computational challenges. These challenges include, but are not limited to, the non-convexity of learning objectives and estimating the quantities needed for optimization algorithms, such as gradients. While we do not address the non-convexity, we present an optimization solution that ex- ploits the so far unused “geometry” in the objective function in order to best make use of the estimated gradients. Previous work attempted similar goals with preconditioned methods in the Euclidean space, such as L-BFGS, RMSprop, and ADA-grad. In stark contrast, our approach combines a non-Euclidean gradient method with preconditioning. We provide evidence that this combination more accurately captures the geometry of the objective function compared to prior work. We theoretically formalize our arguments and derive novel preconditioned non-Euclidean algorithms. The results are promising in both computational time and quality when applied to Restricted Boltzmann Machines, Feedforward Neural Nets, and Convolutional Neural Nets.\n",
      "------------ 71 2019_917 -------------\n",
      "Model-free deep reinforcement learning (RL) algorithms have been widely used for a range of complex control tasks. However, slow convergence and sample inefficiency remain challenging problems in RL, especially when handling continuous and high-dimensional state spaces. To tackle this problem, we propose a general acceleration method for model-free, off-policy deep RL algorithms by drawing the idea underlying regularized Anderson acceleration (RAA), which is an effective approach to accelerating the solving of fixed point problems with perturbations. Specifically, we first explain how policy iteration can be applied directly with Anderson acceleration. Then we extend RAA to the case of deep RL by introducing a regularization term to control the impact of perturbation induced by function approximation errors. We further propose two strategies, i.e., progressive update and adaptive restart, to enhance the performance. The effectiveness of our method is evaluated on a variety of benchmark tasks, including Atari 2600 and MuJoCo. Experimental results show that our approach substantially improves both the learning speed and final performance of state-of-the-art deep RL algorithms.\n",
      "------------ 72 2019_334 -------------\n",
      "Recent works have shown that stochastic gradient descent (SGD) achieves the fast convergence rates of full-batch gradient descent for over-parameterized models satisfying certain interpolation conditions. However, the step-size used in these works depends on unknown quantities and SGD's practical performance heavily relies on the choice of this step-size. We propose to use line-search techniques to automatically set the step-size when training models that can interpolate the data. In the interpolation setting, we prove that SGD with a stochastic variant of the classic Armijo line-search attains the deterministic convergence rates for both convex and strongly-convex functions. Under additional assumptions, SGD with Armijo line-search is shown to achieve fast convergence for non-convex functions. Furthermore, we show that stochastic extra-gradient with a Lipschitz line-search attains linear convergence for an important class of non-convex functions and saddle-point problems satisfying interpolation. To improve the proposed methods' practical performance, we give heuristics to use larger step-sizes and acceleration. We compare the proposed algorithms against numerous optimization methods on standard classification tasks using both kernel methods and deep networks. The proposed methods result in competitive performance across all models and datasets, while being robust to the precise choices of hyper-parameters. For multi-class classification using deep networks, SGD with Armijo line-search results in both faster convergence and better generalization.\n",
      "------------ 73 2015_376 -------------\n",
      "We introduce a generic scheme for accelerating first-order optimization methods in the sense of Nesterov, which builds upon a new analysis of the accelerated proximal point algorithm. Our approach consists of minimizing a convex objective by approximately solving a sequence of well-chosen auxiliary problems, leading to faster convergence. This strategy applies to a large class of algorithms, including gradient descent, block coordinate descent, SAG, SAGA, SDCA, SVRG, Finito/MISO, and their proximal variants. For all of these methods, we provide acceleration and explicit support for non-strongly convex objectives. In addition to theoretical speed-up, we also show that acceleration is useful in practice, especially for ill-conditioned problems where we measure significant improvements.\n",
      "------------ 74 2017_329 -------------\n",
      "Linear Dynamical Systems (LDSs) are fundamental tools for modeling spatio-temporal data in various disciplines. Though rich in modeling, analyzing LDSs is not free of difficulty, mainly because LDSs do not comply with Euclidean geometry and hence conventional learning techniques can not be applied directly. In this paper, we propose an efficient projected gradient descent method to minimize a general form of a loss function and demonstrate how clustering and sparse coding with LDSs can be solved by the proposed method efficiently. To this end, we first derive a novel canonical form for representing the parameters of an LDS, and then show how gradient-descent updates through the projection on the space of LDSs can be achieved dexterously. In contrast to previous studies, our solution avoids any approximation in LDS modeling or during the optimization process. Extensive experiments reveal the superior performance of the proposed method in terms of the convergence and classification accuracy over state-of-the-art techniques.\n",
      "------------ 75 2015_363 -------------\n",
      "Marginal MAP inference involves making MAP predictions in systems defined with latent variables or missing information. It is significantly more difficult than pure marginalization and MAP tasks, for which a large class of efficient and convergent variational algorithms, such as dual decomposition, exist. In this work, we generalize dual decomposition to a generic powered-sum inference task, which includes marginal MAP, along with pure marginalization and MAP, as special cases. Our method is based on a block coordinate descent algorithm on a new convex decomposition bound, that is guaranteed to converge monotonically, and can be parallelized efficiently. We demonstrate our approach on various inference queries over real-world problems from the UAI approximate inference challenge, showing that our framework is faster and more reliable than previous methods.\n",
      "------------ 76 2015_350 -------------\n",
      "We propose a new primal-dual algorithmic framework for a prototypical constrained convex optimization template. The algorithmic instances of our framework are universal since they can automatically adapt to the unknown Holder continuity degree and constant within the dual formulation. They are also guaranteed to have optimal convergence rates in the objective residual and the feasibility gap for each Holder smoothness degree. In contrast to existing primal-dual algorithms, our framework avoids the proximity operator of the objective function. We instead leverage computationally cheaper, Fenchel-type operators, which are the main workhorses of the generalized conditional gradient (GCG)-type methods. In contrast to the GCG-type methods, our framework does not require the objective function to be differentiable, and can also process additional general linear inclusion constraints, while guarantees the convergence rate on the primal problem.\n",
      "------------ 77 2017_261 -------------\n",
      "We propose a DC proximal Newton algorithm for solving nonconvex regularized sparse learning problems in high dimensions. Our proposed algorithm integrates the proximal newton algorithm with multi-stage convex relaxation based on the difference of convex (DC) programming, and enjoys both strong computational and statistical guarantees. Specifically, by leveraging a sophisticated characterization of sparse modeling structures (i.e., local restricted strong convexity and Hessian smoothness), we prove that within each stage of convex relaxation, our proposed algorithm achieves (local) quadratic convergence, and eventually obtains a sparse approximate local optimum with optimal statistical properties after only a few convex relaxations. Numerical experiments are provided to support our theory.\n",
      "------------ 78 2015_46 -------------\n",
      "Recent years have witnessed the superiority of non-convex sparse learning formulations over their convex counterparts in both theory and practice. However, due to the non-convexity and non-smoothness of the regularizer, how to efficiently solve the non-convex optimization problem for large-scale data is still quite challenging. In this paper, we propose an efficient \\underline{H}ybrid \\underline{O}ptimization algorithm for \\underline{NO}n convex \\underline{R}egularized problems (HONOR). Specifically, we develop a hybrid scheme which effectively integrates a Quasi-Newton (QN) step and a Gradient Descent (GD) step. Our contributions are as follows: (1) HONOR incorporates the second-order information to greatly speed up the convergence, while it avoids solving a regularized quadratic programming and only involves matrix-vector multiplications without explicitly forming the inverse Hessian matrix. (2) We establish a rigorous convergence analysis for HONOR, which shows that convergence is guaranteed even for non-convex problems, while it is typically challenging to analyze the convergence for non-convex problems. (3) We conduct empirical studies on large-scale data sets and results demonstrate that HONOR converges significantly faster than state-of-the-art algorithms.\n",
      "------------ 79 2016_85 -------------\n",
      "We study the stochastic optimization of canonical correlation analysis (CCA), whose objective is nonconvex and does not decouple over training samples. Although several stochastic gradient based optimization algorithms have been recently proposed to solve this problem, no global convergence guarantee was provided by any of them. Inspired by the alternating least squares/power iterations formulation of CCA, and the shift-and-invert preconditioning method for PCA, we propose two globally convergent meta-algorithms for CCA, both of which transform the original problem into sequences of least squares problems that need only be solved approximately. We instantiate the meta-algorithms with state-of-the-art SGD methods and obtain time complexities that significantly improve upon that of previous work. Experimental results demonstrate their superior performance.\n",
      "------------ 80 2017_153 -------------\n",
      "We present an approach towards convex optimization that relies on a novel scheme which converts adaptive online algorithms into offline methods. In the offline optimization setting, our derived methods are shown to obtain favourable adaptive guarantees which depend on the harmonic sum of the queried gradients. We further show that our methods implicitly adapt to the objective's structure: in the smooth case fast convergence rates are ensured without any prior knowledge of the smoothness parameter, while still maintaining guarantees in the non-smooth setting. Our approach has a natural extension to the stochastic setting, resulting in a lazy version of SGD (stochastic GD), where minibathces are chosen adaptively depending on the magnitude of the gradients. Thus providing a principled approach towards choosing minibatch sizes.\n",
      "------------ 81 2016_47 -------------\n",
      "Factorizing low-rank matrices has many applications in machine learning and statistics. For probabilistic models in the Bayes optimal setting, a general expression for the mutual information has been proposed using heuristic statistical physics computations, and proven in few specific cases. Here, we show how to rigorously prove the conjectured formula for the symmetric rank-one case. This allows to express the minimal mean-square-error and to characterize the detectability phase transitions in a large set of estimation problems ranging from community detection to sparse PCA. We also show that for a large set of parameters, an iterative algorithm called approximate message-passing is Bayes optimal. There exists, however, a gap between what currently known polynomial algorithms can do and what is expected information theoretically. Additionally, the proof technique has an interest of its own and exploits three essential ingredients: the interpolation method introduced in statistical physics by Guerra, the analysis of the approximate message-passing algorithm and the theory of spatial coupling and threshold saturation in coding. Our approach is generic and applicable to other open problems in statistical estimation where heuristic statistical physics predictions are available.\n",
      "------------ 82 2015_319 -------------\n",
      "Independent Component Analysis (ICA) is a popular model for blind signal separation. The ICA model assumes that a number of independent source signals are linearly mixed to form the observed signals. We propose a new algorithm, PEGI (for pseudo-Euclidean Gradient Iteration), for provable model recovery for ICA with Gaussian noise. The main technical innovation of the algorithm is to use a fixed point iteration in a pseudo-Euclidean (indefinite “inner product”) space. The use of this indefinite “inner product” resolves technical issues common to several existing algorithms for noisy ICA. This leads to an algorithm which is conceptually simple, efficient and accurate in testing.Our second contribution is combining PEGI with the analysis of objectives for optimal recovery in the noisy ICA model. It has been observed that the direct approach of demixing with the inverse of the mixing matrix is suboptimal for signal recovery in terms of the natural Signal to Interference plus Noise Ratio (SINR) criterion. There have been several partial solutions proposed in the ICA literature. It turns out that any solution to the mixing matrix reconstruction problem can be used to construct an SINR-optimal ICA demixing, despite the fact that SINR itself cannot be computed from data. That allows us to obtain a practical and provably SINR-optimal recovery method for ICA with arbitrary Gaussian noise.\n",
      "------------ 83 2015_242 -------------\n",
      "Abstract We propose a family of non-uniform sampling strategies to provably speed up a class of stochastic optimization algorithms with linear convergence including Stochastic Variance Reduced Gradient (SVRG) and Stochastic Dual Coordinate Ascent (SDCA). For a large family of penalized empirical risk minimization problems, our methods exploit data dependent local smoothness of the loss functions near the optimum, while maintaining convergence guarantees. Our bounds are the first to quantify the advantage gained from local smoothness which are significant for some problems significantly better. Empirically, we provide thorough numerical results to back up our theory. Additionally we present algorithms exploiting local smoothness in more aggressive ways, which perform even better in practice.\n",
      "------------ 84 2019_646 -------------\n",
      "The adaptive momentum method (AdaMM), which uses past gradients to update descent directions and learning rates simultaneously, has become one of the most popular first-order optimization methods for solving machine learning problems. However, AdaMM is not suited for solving black-box optimization problems, where explicit gradient forms are difficult or infeasible to obtain. In this paper, we propose a zeroth-order AdaMM (ZO-AdaMM) algorithm, that generalizes AdaMM to the gradient-free regime. We show that the convergence rate of ZO-AdaMM for both convex and nonconvex optimization is roughly a factor of O(\\sqrt{d})\n",
      "worse than that of the first-order AdaMM algorithm, where d\n",
      "is problem size. In particular, we provide a deep understanding on why Mahalanobis distance matters in convergence of ZO-AdaMM and other AdaMM-type methods. As a byproduct, our analysis makes the first step toward understanding adaptive learning rate methods for nonconvex constrained optimization.Furthermore, we demonstrate two applications, designing per-image and universal adversarial attacks from black-box neural networks, respectively. We perform extensive experiments on ImageNet and empirically show that ZO-AdaMM converges much faster to a solution of high accuracy compared with 6\n",
      "state-of-the-art ZO optimization methods.\n",
      "------------ 85 2018_849 -------------\n",
      "Motivated by applications in Optimization, Game Theory, and the training of Generative Adversarial Networks, the convergence properties of first order methods in min-max problems have received extensive study. It has been recognized that they may cycle, and there is no good understanding of their limit points when they do not. When they converge, do they converge to local min-max solutions? We characterize the limit points of two basic first order methods, namely Gradient Descent/Ascent (GDA) and Optimistic Gradient Descent Ascent (OGDA). We show that both dynamics avoid unstable critical points for almost all initializations. Moreover, for small step sizes and under mild assumptions, the set of OGDA-stable critical points is a superset of GDA-stable critical points, which is a superset of local min-max solutions (strict in some cases). The connecting thread is that the behavior of these dynamics can be studied from a dynamical systems perspective.\n",
      "------------ 86 2017_5 -------------\n",
      "Due to their simplicity and excellent performance, parallel asynchronous variants of stochastic gradient descent have become popular methods to solve a wide range of large-scale optimization problems on multi-core architectures. Yet, despite their practical success, support for nonsmooth objectives is still lacking, making them unsuitable for many problems of interest in machine learning, such as the Lasso, group Lasso or empirical risk minimization with convex constraints. In this work, we propose and analyze ProxASAGA, a fully asynchronous sparse method inspired by SAGA, a variance reduced incremental gradient algorithm. The proposed method is easy to implement and significantly outperforms the state of the art on several nonsmooth, large-scale problems. We prove that our method achieves a theoretical linear speedup with respect to the sequential version under assumptions on the sparsity of gradients and block-separability of the proximal term. Empirical benchmarks on a multi-core architecture illustrate practical speedups of up to 12x on a 20-core machine.\n",
      "------------ 87 2015_46 -------------\n",
      "Recent years have witnessed the superiority of non-convex sparse learning formulations over their convex counterparts in both theory and practice. However, due to the non-convexity and non-smoothness of the regularizer, how to efficiently solve the non-convex optimization problem for large-scale data is still quite challenging. In this paper, we propose an efficient \\underline{H}ybrid \\underline{O}ptimization algorithm for \\underline{NO}n convex \\underline{R}egularized problems (HONOR). Specifically, we develop a hybrid scheme which effectively integrates a Quasi-Newton (QN) step and a Gradient Descent (GD) step. Our contributions are as follows: (1) HONOR incorporates the second-order information to greatly speed up the convergence, while it avoids solving a regularized quadratic programming and only involves matrix-vector multiplications without explicitly forming the inverse Hessian matrix. (2) We establish a rigorous convergence analysis for HONOR, which shows that convergence is guaranteed even for non-convex problems, while it is typically challenging to analyze the convergence for non-convex problems. (3) We conduct empirical studies on large-scale data sets and results demonstrate that HONOR converges significantly faster than state-of-the-art algorithms.\n",
      "------------ 88 2016_308 -------------\n",
      "Semidefinite programs (SDP's) can be solved in polynomial time by interior point methods, but scalability can be an issue. To address this shortcoming, over a decade ago, Burer and Monteiro proposed to solve SDP's with few equality constraints via rank-restricted, non-convex surrogates. Remarkably, for some applications, local optimization methods seem to converge to global optima of these non-convex surrogates reliably. Although some theory supports this empirical success, a complete explanation of it remains an open question. In this paper, we consider a class of SDP's which includes applications such as max-cut, community detection in the stochastic block model, robust PCA, phase retrieval and synchronization of rotations. We show that the low-rank Burer-Monteiro formulation of SDP's in that class almost never has any spurious local optima.\n",
      "------------ 89 2019_1125 -------------\n",
      "In this paper, we introduce various mechanisms to obtain accelerated first-order stochastic optimization algorithms when the objective function is convex or strongly convex. Specifically, we extend the Catalyst approach originally designed for deterministic objectives to the stochastic setting. Given an optimization method with mild convergence guarantees for strongly convex problems, the challenge is to accelerate convergence to a noise-dominated region, and then achieve convergence with an optimal worst-case complexity depending on the noise variance of the gradients. A side contribution of our work is also a generic analysis that can handle inexact proximal operators, providing new insights about the robustness of stochastic algorithms when the proximal operator cannot be exactly computed.\n",
      "------------ 90 2019_353 -------------\n",
      "Wasserstein distance-based distributionally robust optimization (DRO) has received much attention lately due to its ability to provide a robustness interpretation of various learning models. Moreover, many of the DRO problems that arise in the learning context admits exact convex reformulations and hence can be tackled by off-the-shelf solvers. Nevertheless, the use of such solvers severely limits the applicability of DRO in large-scale learning problems, as they often rely on general purpose interior-point algorithms. On the other hand, there are very few works that attempt to develop fast iterative methods to solve these DRO problems, which typically possess complicated structures. In this paper, we take a first step towards resolving the above difficulty by developing a first-order algorithmic framework for tackling a class of Wasserstein distance-based distributionally robust logistic regression (DRLR) problem. Specifically, we propose a novel linearized proximal ADMM to solve the DRLR problem, whose objective is convex but consists of a smooth term plus two non-separable non-smooth terms. We prove that our method enjoys a sublinear convergence rate. Furthermore, we conduct three different experiments to show its superb performance on both synthetic and real-world datasets. In particular, our method can achieve the same accuracy up to 800+ times faster than the standard off-the-shelf solver.\n",
      "------------ 91 2015_33 -------------\n",
      "Maximum a-posteriori (MAP) inference is an important task for many applications. Although the standard formulation gives rise to a hard combinatorial optimization problem, several effective approximations have been proposed and studied in recent years. We focus on linear programming (LP) relaxations, which have achieved state-of-the-art performance in many applications. However, optimization of the resulting program is in general challenging due to non-smoothness and complex non-separable constraints.Therefore, in this work we study the benefits of augmenting the objective function of the relaxation with strong convexity. Specifically, we introduce strong convexity by adding a quadratic term to the LP relaxation objective. We provide theoretical guarantees for the resulting programs, bounding the difference between their optimal value and the original optimum. Further, we propose suitable optimization algorithms and analyze their convergence.\n",
      "------------ 92 2019_255 -------------\n",
      "Decentralized optimization is a powerful paradigm that finds applications in engineering and learning design. This work studies decentralized composite optimization problems with non-smooth regularization terms. Most existing gradient-based proximal decentralized methods are known to converge to the optimal solution with sublinear rates, and it remains unclear whether this family of methods can achieve global linear convergence. To tackle this problem, this work assumes the non-smooth regularization term is common across all networked agents, which is the case for many machine learning problems. Under this condition, we design a proximal gradient decentralized algorithm whose fixed point coincides with the desired minimizer. We then provide a concise proof that establishes its linear convergence. In the absence of the non-smooth term, our analysis technique covers the well known EXTRA algorithm and provides useful bounds on the convergence rate and step-size.\n",
      "------------ 93 2018_78 -------------\n",
      "Graph matching has received persistent attention over decades, which can be formulated as a quadratic assignment problem (QAP). We show that a large family of functions, which we define as Separable Functions, can approximate discrete graph matching in the continuous domain asymptotically by varying the approximation controlling parameters. We also study the properties of global optimality and devise convex/concave-preserving extensions to the widely used Lawler's QAP form. Our theoretical findings show the potential for deriving new algorithms and techniques for graph matching. We deliver solvers based on two specific instances of Separable Functions, and the state-of-the-art performance of our method is verified on popular benchmarks.\n",
      "------------ 94 2018_547 -------------\n",
      "Stochastic gradient descent (SGD) remains the method of choice for deep learning, despite the limitations arising for ill-behaved objective functions. In cases where it could be estimated, the natural gradient has proven very effective at mitigating the catastrophic effects of pathological curvature in the objective function, but little is known theoretically about its convergence properties, and it has yet to find a practical implementation that would scale to very deep and large networks. Here, we derive an exact expression for the natural gradient in deep linear networks, which exhibit pathological curvature similar to the nonlinear case. We provide for the first time an analytical solution for its convergence rate, showing that the loss decreases exponentially to the global minimum in parameter space. Our expression for the natural gradient is surprisingly simple, computationally tractable, and explains why some approximations proposed previously work well in practice. This opens new avenues for approximating the natural gradient in the nonlinear case, and we show in preliminary experiments that our online natural gradient descent outperforms SGD on MNIST autoencoding while sharing its computational simplicity.\n",
      "------------ 95 2016_47 -------------\n",
      "Factorizing low-rank matrices has many applications in machine learning and statistics. For probabilistic models in the Bayes optimal setting, a general expression for the mutual information has been proposed using heuristic statistical physics computations, and proven in few specific cases. Here, we show how to rigorously prove the conjectured formula for the symmetric rank-one case. This allows to express the minimal mean-square-error and to characterize the detectability phase transitions in a large set of estimation problems ranging from community detection to sparse PCA. We also show that for a large set of parameters, an iterative algorithm called approximate message-passing is Bayes optimal. There exists, however, a gap between what currently known polynomial algorithms can do and what is expected information theoretically. Additionally, the proof technique has an interest of its own and exploits three essential ingredients: the interpolation method introduced in statistical physics by Guerra, the analysis of the approximate message-passing algorithm and the theory of spatial coupling and threshold saturation in coding. Our approach is generic and applicable to other open problems in statistical estimation where heuristic statistical physics predictions are available.\n",
      "------------ 96 2016_5 -------------\n",
      "Stochastic gradient methods for machine learning and optimization problems are usually analyzed assuming data points are sampled *with* replacement. In contrast, sampling *without* replacement is far less understood, yet in practice it is very common, often easier to implement, and usually performs better. In this paper, we provide competitive convergence guarantees for without-replacement sampling under several scenarios, focusing on the natural regime of few passes over the data. Moreover, we describe a useful application of these results in the context of distributed optimization with randomly-partitioned data, yielding a nearly-optimal algorithm for regularized least squares (in terms of both communication complexity and runtime complexity) under broad parameter regimes. Our proof techniques combine ideas from stochastic optimization, adversarial online learning and transductive learning theory, and can potentially be applied to other stochastic optimization and learning problems.\n",
      "------------ 97 2015_330 -------------\n",
      "Deep learning presents notorious computational challenges. These challenges include, but are not limited to, the non-convexity of learning objectives and estimating the quantities needed for optimization algorithms, such as gradients. While we do not address the non-convexity, we present an optimization solution that ex- ploits the so far unused “geometry” in the objective function in order to best make use of the estimated gradients. Previous work attempted similar goals with preconditioned methods in the Euclidean space, such as L-BFGS, RMSprop, and ADA-grad. In stark contrast, our approach combines a non-Euclidean gradient method with preconditioning. We provide evidence that this combination more accurately captures the geometry of the objective function compared to prior work. We theoretically formalize our arguments and derive novel preconditioned non-Euclidean algorithms. The results are promising in both computational time and quality when applied to Restricted Boltzmann Machines, Feedforward Neural Nets, and Convolutional Neural Nets.\n",
      "------------ 98 2019_851 -------------\n",
      "For optimization of a large sum of functions in a distributed computing environment, we present a novel communication efficient Newton-type algorithm that enjoys a variety of advantages over similar existing methods. Our algorithm, DINGO, is derived by optimization of the gradient's norm as a surrogate function. DINGO does not impose any specific form on the underlying functions and its application range extends far beyond convexity and smoothness. The underlying sub-problems of DINGO are simple linear least-squares, for which a plethora of efficient algorithms exist. DINGO involves a few hyper-parameters that are easy to tune and we theoretically show that a strict reduction in the surrogate objective is guaranteed, regardless of the selected hyper-parameters.\n",
      "------------ 99 2016_397 -------------\n",
      "The Hard Thresholding Pursuit (HTP) is a class of truncated gradient descent methods for finding sparse solutions of \\ell_0\n",
      "-constrained loss minimization problems. The HTP-style methods have been shown to have strong approximation guarantee and impressive numerical performance in high dimensional statistical learning applications. However, the current theoretical treatment of these methods has traditionally been restricted to the analysis of parameter estimation consistency. It remains an open problem to analyze the support recovery performance (a.k.a., sparsistency) of this type of methods for recovering the global minimizer of the original NP-hard problem. In this paper, we bridge this gap by showing, for the first time, that exact recovery of the global sparse minimizer is possible for HTP-style methods under restricted strong condition number bounding conditions. We further show that HTP-style methods are able to recover the support of certain relaxed sparse solutions without assuming bounded restricted strong condition number. Numerical results on simulated data confirms our theoretical predictions.\n",
      "------------ 100 2019_755 -------------\n",
      "Low-rank matrix factorization is a problem of broad importance, owing to the ubiquity of low-rank models in machine learning contexts. In spite of its non- convexity, this problem has a well-behaved geometric landscape, permitting local search algorithms such as gradient descent to converge to global minimizers. In this paper, we study low-rank matrix factorization in the distributed setting, where local variables at each node encode parts of the overall matrix factors, and consensus is encouraged among certain such variables. We identify conditions under which this new problem also has a well-behaved geometric landscape, and we propose an extension of distributed gradient descent (DGD) to solve this problem. The favorable landscape allows us to prove convergence to global optimality with exact consensus, a stronger result than what is provided by off-the-shelf DGD theory.\n",
      "------------ 101 2015_62 -------------\n",
      "We study the estimation of low rank matrices via nonconvex optimization. Compared with convex relaxation, nonconvex optimization exhibits superior empirical performance for large scale instances of low rank matrix estimation. However, the understanding of its theoretical guarantees are limited. In this paper, we define the notion of projected oracle divergence based on which we establish sufficient conditions for the success of nonconvex optimization. We illustrate the consequences of this general framework for matrix sensing and completion. In particular, we prove that a broad class of nonconvex optimization algorithms, including alternating minimization and gradient-type methods, geometrically converge to the global optimum and exactly recover the true low rank matrices under standard conditions.\n",
      "------------ 102 2018_445 -------------\n",
      "Neural networks have been used prominently in several machine learning and statistics applications. In general, the underlying optimization of neural networks is non-convex which makes analyzing their performance challenging. In this paper, we take another approach to this problem by constraining the network such that the corresponding optimization landscape has good theoretical properties without significantly compromising performance. In particular, for two-layer neural networks we introduce Porcupine Neural Networks (PNNs) whose weight vectors are constrained to lie over a finite set of lines. We show that most local optima of PNN optimizations are global while we have a characterization of regions where bad local optimizers may exist. Moreover, our theoretical and empirical results suggest that an unconstrained neural network can be approximated using a polynomially-large PNN.\n",
      "------------ 103 2015_127 -------------\n",
      "Kernel methods represent one of the most powerful tools in machine learning to tackle problems expressed in terms of function values and derivatives due to their capability to represent and model complex relations. While these methods show good versatility, they are computationally intensive and have poor scalability to large data as they require operations on Gram matrices. In order to mitigate this serious computational limitation, recently randomized constructions have been proposed in the literature, which allow the application of fast linear algorithms. Random Fourier features (RFF) are among the most popular and widely applied constructions: they provide an easily computable, low-dimensional feature representation for shift-invariant kernels. Despite the popularity of RFFs, very little is understood theoretically about their approximation quality. In this paper, we provide a detailed finite-sample theoretical analysis about the approximation quality of RFFs by (i) establishing optimal (in terms of the RFF dimension, and growing set size) performance guarantees in uniform norm, and (ii) presenting guarantees in L^r (1 ≤ r < ∞) norms. We also propose an RFF approximation to derivatives of a kernel with a theoretical study on its approximation quality.\n",
      "------------ 104 2017_349 -------------\n",
      "Non-convex optimization with local search heuristics has been widely used in machine learning, achieving many state-of-art results. It becomes increasingly important to understand why they can work for these NP-hard problems on typical data. The landscape of many objective functions in learning has been conjectured to have the geometric property that all local optima are (approximately) global optima'', and thus they can be solved efficiently by local search algorithms. However, establishing such property can be very difficult. In this paper, we analyze the optimization landscape of the random over-complete tensor decomposition problem, which has many applications in unsupervised leaning, especially in learning latent variable models. In practice, it can be efficiently solved by gradient ascent on a non-convex objective. We show that for any small constant \\epsilon > 0, among the set of points with function values (1+\\epsilon)-factor larger than the expectation of the function, all the local maxima are approximate global maxima. Previously, the best-known result only characterizes the geometry in small neighborhoods around the true components. Our result implies that even with an initialization that is barely better than the random guess, the gradient ascent algorithm is guaranteed to solve this problem. Our main technique uses Kac-Rice formula and random matrix theory. To our best knowledge, this is the first time when Kac-Rice formula is successfully applied to counting the number of local minima of a highly-structured random polynomial with dependent coefficients.\n",
      "------------ 105 2018_804 -------------\n",
      "We investigate 1) the rate at which refined properties of the empirical risk---in particular, gradients---converge to their population counterparts in standard non-convex learning tasks, and 2) the consequences of this convergence for optimization. Our analysis follows the tradition of norm-based capacity control. We propose vector-valued Rademacher complexities as a simple, composable, and user-friendly tool to derive dimension-free uniform convergence bounds for gradients in non-convex learning problems. As an application of our techniques, we give a new analysis of batch gradient descent methods for non-convex generalized linear models and non-convex robust regression, showing how to use any algorithm that finds approximate stationary points to obtain optimal sample complexity, even when dimension is high or possibly infinite and multiple passes over the dataset are allowed. Moving to non-smooth models we show----in contrast to the smooth case---that even for a single ReLU it is not possible to obtain dimension-independent convergence rates for gradients in the worst case. On the positive side, it is still possible to obtain dimension-independent rates under a new type of distributional assumption.\n",
      "------------ 106 2017_580 -------------\n",
      "Early stopping of iterative algorithms is a widely-used form of regularization in statistical learning, commonly used in conjunction with boosting and related gradient-type algorithms. Although consistency results have been established in some settings, such estimators are less well-understood than their analogues based on penalized regularization. In this paper, for a relatively broad class of loss functions and boosting algorithms (including L^2\n",
      "-boost, LogitBoost and AdaBoost, among others), we connect the performance of a stopped iterate to the localized Rademacher/Gaussian complexity of the associated function class. This connection allows us to show that local fixed point analysis, now standard in the analysis of penalized estimators, can be used to derive optimal stopping rules. We derive such stopping rules in detail for various kernel classes, and illustrate the correspondence of our theory with practice for Sobolev kernel classes.\n",
      "------------ 107 2017_313 -------------\n",
      "Error bound, an inherent property of an optimization problem, has recently revived in the development of algorithms with improved global convergence without strong convexity. The most studied error bound is the quadratic error bound, which generalizes strong convexity and is satisfied by a large family of machine learning problems. Quadratic error bound have been leveraged to achieve linear convergence in many first-order methods including the stochastic variance reduced gradient (SVRG) method, which is one of the most important stochastic optimization methods in machine learning. However, the studies along this direction face the critical issue that the algorithms must depend on an unknown growth parameter (a generalization of strong convexity modulus) in the error bound. This parameter is difficult to estimate exactly and the algorithms choosing this parameter heuristically do not have theoretical convergence guarantee. To address this issue, we propose novel SVRG methods that automatically search for this unknown parameter on the fly of optimization while still obtain almost the same convergence rate as when this parameter is known. We also analyze the convergence property of SVRG methods under H\\\"{o}lderian error bound, which generalizes the quadratic error bound.\n",
      "------------ 108 2018_138 -------------\n",
      "We study stochastic composite mirror descent, a class of scalable algorithms able to exploit the geometry and composite structure of a problem. We consider both convex and strongly convex objectives with non-smooth loss functions, for each of which we establish high-probability convergence rates optimal up to a logarithmic factor. We apply the derived computational error bounds to study the generalization performance of multi-pass stochastic gradient descent (SGD) in a non-parametric setting. Our high-probability generalization bounds enjoy a logarithmical dependency on the number of passes provided that the step size sequence is square-summable, which improves the existing bounds in expectation with a polynomial dependency and therefore gives a strong justification on the ability of multi-pass SGD to overcome overfitting. Our analysis removes boundedness assumptions on subgradients often imposed in the literature. Numerical results are reported to support our theoretical findings.\n",
      "------------ 109 2019_439 -------------\n",
      "The exploration bonus is an effective approach to manage the exploration-exploitation trade-off in Markov Decision Processes (MDPs). While it has been analyzed in infinite-horizon discounted and finite-horizon problems, we focus on designing and analysing the exploration bonus in the more challenging infinite-horizon undiscounted setting. We first introduce SCAL+, a variant of SCAL (Fruit et al. 2018), that uses a suitable exploration bonus to solve any discrete unknown weakly-communicating MDP for which an upper bound c\n",
      "on the span of the optimal bias function is known. We prove that SCAL+ enjoys the same regret guarantees as SCAL, which relies on the less efficient extended value iteration approach. Furthermore, we leverage the flexibility provided by the exploration bonus scheme to generalize SCAL+ to smooth MDPs with continuous state space and discrete actions. We show that the resulting algorithm (SCCAL+) achieves the same regret bound as UCCRL (Ortner and Ryabko, 2012) while being the first implementable algorithm for this setting.\n",
      "------------ 110 2018_653 -------------\n",
      "Generative Adversarial Networks (GANs) are one of the most practical methods for learning data distributions. A popular GAN formulation is based on the use of Wasserstein distance as a metric between probability distributions. Unfortunately, minimizing the Wasserstein distance between the data distribution and the generative model distribution is a computationally challenging problem as its objective is non-convex, non-smooth, and even hard to compute. In this work, we show that obtaining gradient information of the smoothed Wasserstein GAN formulation, which is based on regularized Optimal Transport (OT), is computationally effortless and hence one can apply first order optimization methods to minimize this objective. Consequently, we establish theoretical convergence guarantee to stationarity for a proposed class of GAN optimization algorithms. Unlike the original non-smooth formulation, our algorithm only requires solving the discriminator to approximate optimality. We apply our method to learning MNIST digits as well as CIFAR-10 images. Our experiments show that our method is computationally efficient and generates images comparable to the state of the art algorithms given the same architecture and computational power.\n",
      "------------ 111 2017_73 -------------\n",
      "Greedy optimization methods such as Matching Pursuit (MP) and Frank-Wolfe (FW) algorithms regained popularity in recent years due to their simplicity, effectiveness and theoretical guarantees. MP and FW address optimization over the linear span and the convex hull of a set of atoms, respectively. In this paper, we consider the intermediate case of optimization over the convex cone, parametrized as the conic hull of a generic atom set, leading to the first principled definitions of non-negative MP algorithms for which we give explicit convergence rates and demonstrate excellent empirical performance. In particular, we derive sublinear (O(1/t)) convergence on general smooth and convex objectives, and linear convergence (O(e^{-t})) on strongly convex objectives, in both cases for general sets of atoms. Furthermore, we establish a clear correspondence of our algorithms to known algorithms from the MP and FW literature. Our novel algorithms and analyses target general atom sets and general objective functions, and hence are directly applicable to a large variety of learning settings.\n",
      "------------ 112 2015_260 -------------\n",
      "Nonlinear component analysis such as kernel Principle Component Analysis (KPCA) and kernel Canonical Correlation Analysis (KCCA) are widely used in machine learning, statistics and data analysis, but they can not scale up to big datasets. Recent attempts have employed random feature approximations to convert the problem to the primal form for linear computational complexity. However, to obtain high quality solutions, the number of random features should be the same order of magnitude as the number of data points, making such approach not directly applicable to the regime with millions of data points.We propose a simple, computationally efficient, and memory friendly algorithm based on the\n",
      "doubly stochastic gradients'' to scale up a range of kernel nonlinear component analysis, such as kernel PCA, CCA and SVD. Despite the \\emph{non-convex} nature of these problems, our method enjoys theoretical guarantees that it converges at the rate \\Otil(1/t)\n",
      "to the global optimum, even for the top k\n",
      "eigen subspace. Unlike many alternatives, our algorithm does not require explicit orthogonalization, which is infeasible on big datasets. We demonstrate the effectiveness and scalability of our algorithm on large scale synthetic and real world datasets.\n",
      "------------ 113 2018_888 -------------\n",
      "An Euler discretization of the Langevin diffusion is known to converge to the global minimizers of certain convex and non-convex optimization problems. We show that this property holds for any suitably smooth diffusion and that different diffusions are suitable for optimizing different classes of convex and non-convex functions. This allows us to design diffusions suitable for globally optimizing convex and non-convex functions not covered by the existing Langevin theory. Our non-asymptotic analysis delivers computable optimization and integration error bounds based on easily accessed properties of the objective and chosen diffusion. Central to our approach are new explicit Stein factor bounds on the solutions of Poisson equations. We complement these results with improved optimization guarantees for targets other than the standard Gibbs measure.\n",
      "------------ 114 2018_147 -------------\n",
      "We present the first accelerated randomized algorithm for solving linear systems in Euclidean spaces. One essential problem of this type is the matrix inversion problem. In particular, our algorithm can be specialized to invert positive definite matrices in such a way that all iterates (approximate solutions) generated by the algorithm are positive definite matrices themselves. This opens the way for many applications in the field of optimization and machine learning. As an application of our general theory, we develop the first accelerated (deterministic and stochastic) quasi-Newton updates. Our updates lead to provably more aggressive approximations of the inverse Hessian, and lead to speed-ups over classical non-accelerated rules in numerical experiments. Experiments with empirical risk minimization show that our rules can accelerate training of machine learning models.\n",
      "------------ 115 2017_211 -------------\n",
      "We consider the problem of bandit optimization, inspired by stochastic optimization and online learning problems with bandit feedback. In this problem, the objective is to minimize a global loss function of all the actions, not necessarily a cumulative loss. This framework allows us to study a very general class of problems, with applications in statistics, machine learning, and other fields. To solve this problem, we analyze the Upper-Confidence Frank-Wolfe algorithm, inspired by techniques for bandits and convex optimization. We give theoretical guarantees for the performance of this algorithm over various classes of functions, and discuss the optimality of these results.\n",
      "------------ 116 2016_47 -------------\n",
      "Factorizing low-rank matrices has many applications in machine learning and statistics. For probabilistic models in the Bayes optimal setting, a general expression for the mutual information has been proposed using heuristic statistical physics computations, and proven in few specific cases. Here, we show how to rigorously prove the conjectured formula for the symmetric rank-one case. This allows to express the minimal mean-square-error and to characterize the detectability phase transitions in a large set of estimation problems ranging from community detection to sparse PCA. We also show that for a large set of parameters, an iterative algorithm called approximate message-passing is Bayes optimal. There exists, however, a gap between what currently known polynomial algorithms can do and what is expected information theoretically. Additionally, the proof technique has an interest of its own and exploits three essential ingredients: the interpolation method introduced in statistical physics by Guerra, the analysis of the approximate message-passing algorithm and the theory of spatial coupling and threshold saturation in coding. Our approach is generic and applicable to other open problems in statistical estimation where heuristic statistical physics predictions are available.\n",
      "------------ 117 2019_365 -------------\n",
      "The estimation of an f-divergence between two probability distributions based on samples is a fundamental problem in statistics and machine learning. Most works study this problem under very weak assumptions, in which case it is provably hard. We consider the case of stronger structural assumptions that are commonly satisfied in modern machine learning, including representation learning and generative modelling with autoencoder architectures. Under these assumptions we propose and study an estimator that can be easily implemented, works well in high dimensions, and enjoys faster rates of convergence. We verify the behavior of our estimator empirically in both synthetic and real-data experiments, and discuss its direct implications for total correlation, entropy, and mutual information estimation.\n",
      "------------ 118 2015_322 -------------\n",
      "We provide a theoretical framework for analyzing basis function construction for linear value function approximation in Markov Decision Processes (MDPs). We show that important existing methods, such as Krylov bases and Bellman-error-based methods are a special case of the general framework we develop. We provide a general algorithmic framework for computing basis function refinements which “respect” the dynamics of the environment, and we derive approximation error bounds that apply for any algorithm respecting this general framework. We also show how, using ideas related to bisimulation metrics, one can translate basis refinement into a process of finding “prototypes” that are diverse enough to represent the given MDP.\n",
      "------------ 119 2017_107 -------------\n",
      "Spectral decomposition of the Koopman operator is attracting attention as a tool for the analysis of nonlinear dynamical systems. Dynamic mode decomposition is a popular numerical algorithm for Koopman spectral analysis; however, we often need to prepare nonlinear observables manually according to the underlying dynamics, which is not always possible since we may not have any a priori knowledge about them. In this paper, we propose a fully data-driven method for Koopman spectral analysis based on the principle of learning Koopman invariant subspaces from observed data. To this end, we propose minimization of the residual sum of squares of linear least-squares regression to estimate a set of functions that transforms data into a form in which the linear regression fits well. We introduce an implementation with neural networks and evaluate performance empirically using nonlinear dynamical systems and applications.\n",
      "------------ 120 2016_466 -------------\n",
      "Observable operator models (OOMs) and related models are one of the most important and powerful tools for modeling and analyzing stochastic systems. They exactly describe dynamics of finite-rank systems and can be efficiently and consistently estimated through spectral learning under the assumption of identically distributed data. In this paper, we investigate the properties of spectral learning without this assumption due to the requirements of analyzing large-time scale systems, and show that the equilibrium dynamics of a system can be extracted from nonequilibrium observation data by imposing an equilibrium constraint. In addition, we propose a binless extension of spectral learning for continuous data. In comparison with the other continuous-valued spectral algorithms, the binless algorithm can achieve consistent estimation of equilibrium dynamics with only linear complexity.\n",
      "------------ 121 2017_320 -------------\n",
      "Factorization machines and polynomial networks are supervised polynomial models based on an efficient low-rank decomposition. We extend these models to the multi-output setting, i.e., for learning vector-valued functions, with application to multi-class or multi-task problems. We cast this as the problem of learning a 3-way tensor whose slices share a common basis and propose a convex formulation of that problem. We then develop an efficient conditional gradient algorithm and prove its global convergence, despite the fact that it involves a non-convex basis selection step. On classification tasks, we show that our algorithm achieves excellent accuracy with much sparser models than existing methods. On recommendation system tasks, we show how to combine our algorithm with a reduction from ordinal regression to multi-output classification and show that the resulting algorithm outperforms simple baselines in terms of ranking accuracy.\n",
      "------------ 122 2019_1125 -------------\n",
      "In this paper, we introduce various mechanisms to obtain accelerated first-order stochastic optimization algorithms when the objective function is convex or strongly convex. Specifically, we extend the Catalyst approach originally designed for deterministic objectives to the stochastic setting. Given an optimization method with mild convergence guarantees for strongly convex problems, the challenge is to accelerate convergence to a noise-dominated region, and then achieve convergence with an optimal worst-case complexity depending on the noise variance of the gradients. A side contribution of our work is also a generic analysis that can handle inexact proximal operators, providing new insights about the robustness of stochastic algorithms when the proximal operator cannot be exactly computed.\n",
      "------------ 123 2016_530 -------------\n",
      "We develop methods for rapidly identifying important components of a convex optimization problem for the purpose of achieving fast convergence times. By considering a novel problem formulation—the minimization of a sum of piecewise functions—we describe a principled and general mechanism for exploiting piecewise linear structure in convex optimization. This result leads to a theoretically justified working set algorithm and a novel screening test, which generalize and improve upon many prior results on exploiting structure in convex optimization. In empirical comparisons, we study the scalability of our methods. We find that screening scales surprisingly poorly with the size of the problem, while our working set algorithm convincingly outperforms alternative approaches.\n",
      "------------ 124 2016_422 -------------\n",
      "We investigate the statistical performance and computational efficiency of the alternating minimization procedure for nonparametric tensor learning. Tensor modeling has been widely used for capturing the higher order relations between multimodal data sources. In addition to a linear model, a nonlinear tensor model has been received much attention recently because of its high flexibility. We consider an alternating minimization procedure for a general nonlinear model where the true function consists of components in a reproducing kernel Hilbert space (RKHS). In this paper, we show that the alternating minimization method achieves linear convergence as an optimization algorithm and that the generalization error of the resultant estimator yields the minimax optimality. We apply our algorithm to some multitask learning problems and show that the method actually shows favorable performances.\n",
      "------------ 125 2019_820 -------------\n",
      "We introduce a framework to study the transformation of problems with manifold constraints into unconstrained problems through parametrizations in terms of a Euclidean space. We call these parametrizations trivializations. We prove conditions under which a trivialization is sound in the context of gradient-based optimization and we show how two large families of trivializations have overall favorable properties, but also suffer from a performance issue. We then introduce dynamic trivializations, which solve this problem, and we show how these form a family of optimization methods that lie between trivializations and Riemannian gradient descent, and combine the benefits of both of them. We then show how to implement these two families of trivializations in practice for different matrix manifolds. To this end, we prove a formula for the gradient of the exponential of matrices, which can be of practical interest on its own. Finally, we show how dynamic trivializations improve the performance of existing methods on standard tasks designed to test long-term memory within neural networks.\n",
      "------------ 126 2015_197 -------------\n",
      "Selecting the optimal subset from a large set of variables is a fundamental problem in various learning tasks such as feature selection, sparse regression, dictionary learning, etc. In this paper, we propose the POSS approach which employs evolutionary Pareto optimization to find a small-sized subset with good performance. We prove that for sparse regression, POSS is able to achieve the best-so-far theoretically guaranteed approximation performance efficiently. Particularly, for the \\emph{Exponential Decay} subclass, POSS is proven to achieve an optimal solution. Empirical study verifies the theoretical results, and exhibits the superior performance of POSS to greedy and convex relaxation methods.\n",
      "------------ 127 2019_422 -------------\n",
      "We study two time-scale linear stochastic approximation algorithms, which can be used to model well-known reinforcement learning algorithms such as GTD, GTD2, and TDC. We present finite-time performance bounds for the case where the learning rate is fixed. The key idea in obtaining these bounds is to use a Lyapunov function motivated by singular perturbation theory for linear differential equations. We use the bound to design an adaptive learning rate scheme which significantly improves the convergence rate over the known optimal polynomial decay rule in our experiments, and can be used to potentially improve the performance of any other schedule where the learning rate is changed at pre-determined time instants.\n",
      "------------ 128 2016_490 -------------\n",
      "We address the problem of recovering a high-dimensional but structured vector from linear observations in a general setting where the vector can come from an arbitrary union of subspaces. This setup includes well-studied problems such as compressive sensing and low-rank matrix recovery. We show how to design more efficient algorithms for the union-of subspace recovery problem by using *approximate* projections. Instantiating our general framework for the low-rank matrix recovery problem gives the fastest provable running time for an algorithm with optimal sample complexity. Moreover, we give fast approximate projections for 2D histograms, another well-studied low-dimensional model of data. We complement our theoretical results with experiments demonstrating that our framework also leads to improved time and sample complexity empirically.\n",
      "------------ 129 2017_218 -------------\n",
      "We consider the problem of solving a large-scale Quadratically Constrained Quadratic Program. Such problems occur naturally in many scientific and web applications. Although there are efficient methods which tackle this problem, they are mostly not scalable. In this paper, we develop a method that transforms the quadratic constraint into a linear form by a sampling a set of low-discrepancy points. The transformed problem can then be solved by applying any state-of-the-art large-scale solvers. We show the convergence of our approximate solution to the true solution as well as some finite sample error bounds. Experimental results are also shown to prove scalability in practice.\n",
      "------------ 130 2015_62 -------------\n",
      "We study the estimation of low rank matrices via nonconvex optimization. Compared with convex relaxation, nonconvex optimization exhibits superior empirical performance for large scale instances of low rank matrix estimation. However, the understanding of its theoretical guarantees are limited. In this paper, we define the notion of projected oracle divergence based on which we establish sufficient conditions for the success of nonconvex optimization. We illustrate the consequences of this general framework for matrix sensing and completion. In particular, we prove that a broad class of nonconvex optimization algorithms, including alternating minimization and gradient-type methods, geometrically converge to the global optimum and exactly recover the true low rank matrices under standard conditions.\n",
      "------------ 131 2019_892 -------------\n",
      "Gradients of neural networks can be computed efficiently for any architecture, but some applications require computing differential operators with higher time complexity. We describe a family of neural network architectures that allow easy access to a family of differential operators involving \\emph{dimension-wise derivatives}, and we show how to modify the backward computation graph to compute them efficiently. We demonstrate the use of these operators for solving root-finding subproblems in implicit ODE solvers, exact density evaluation for continuous normalizing flows, and evaluating the Fokker-Planck equation for training stochastic differential equation models.\n",
      "------------ 132 2016_5 -------------\n",
      "Stochastic gradient methods for machine learning and optimization problems are usually analyzed assuming data points are sampled *with* replacement. In contrast, sampling *without* replacement is far less understood, yet in practice it is very common, often easier to implement, and usually performs better. In this paper, we provide competitive convergence guarantees for without-replacement sampling under several scenarios, focusing on the natural regime of few passes over the data. Moreover, we describe a useful application of these results in the context of distributed optimization with randomly-partitioned data, yielding a nearly-optimal algorithm for regularized least squares (in terms of both communication complexity and runtime complexity) under broad parameter regimes. Our proof techniques combine ideas from stochastic optimization, adversarial online learning and transductive learning theory, and can potentially be applied to other stochastic optimization and learning problems.\n",
      "------------ 133 2017_473 -------------\n",
      "Learning a regression function using censored or interval-valued output data is an important problem in fields such as genomics and medicine. The goal is to learn a real-valued prediction function, and the training output labels indicate an interval of possible values. Whereas most existing algorithms for this task are linear models, in this paper we investigate learning nonlinear tree models. We propose to learn a tree by minimizing a margin-based discriminative objective function, and we provide a dynamic programming algorithm for computing the optimal solution in log-linear time. We show empirically that this algorithm achieves state-of-the-art speed and prediction accuracy in a benchmark of several data sets.\n",
      "------------ 134 2017_673 -------------\n",
      "We revisit the classical analysis of generative vs discriminative models for general exponential families, and high-dimensional settings. Towards this, we develop novel technical machinery, including a notion of separability of general loss functions, which allow us to provide a general framework to obtain l∞ convergence rates for general M-estimators. We use this machinery to analyze l∞ and l2 convergence rates of generative and discriminative models, and provide insights into their nuanced behaviors in high-dimensions. Our results are also applicable to differential parameter estimation, where the quantity of interest is the difference between generative model parameters.\n",
      "------------ 135 2017_192 -------------\n",
      "Technological breakthroughs allow us to collect data with increasing spatio-temporal resolution from complex interaction systems. The combination of high-resolution observations, expressive dynamic models, and efficient machine learning algorithms can lead to crucial insights into complex interaction dynamics and the functions of these systems. In this paper, we formulate the dynamics of a complex interacting network as a stochastic process driven by a sequence of events, and develop expectation propagation algorithms to make inferences from noisy observations. To avoid getting stuck at a local optimum, we formulate the problem of minimizing Bethe free energy as a constrained primal problem and take advantage of the concavity of dual problem in the feasible domain of dual variables guaranteed by duality theorem. Our expectation propagation algorithms demonstrate better performance in inferring the interaction dynamics in complex transportation networks than competing models such as particle filter, extended Kalman filter, and deep neural networks.\n",
      "------------ 136 2016_535 -------------\n",
      "Tensor networks are approximations of high-order tensors which are efficient to work with and have been very successful for physics and mathematics applications. We demonstrate how algorithms for optimizing tensor networks can be adapted to supervised learning tasks by using matrix product states (tensor trains) to parameterize non-linear kernel learning models. For the MNIST data set we obtain less than 1% test set classification error. We discuss an interpretation of the additional structure imparted by the tensor network to the learned model.\n",
      "------------ 137 2017_329 -------------\n",
      "Linear Dynamical Systems (LDSs) are fundamental tools for modeling spatio-temporal data in various disciplines. Though rich in modeling, analyzing LDSs is not free of difficulty, mainly because LDSs do not comply with Euclidean geometry and hence conventional learning techniques can not be applied directly. In this paper, we propose an efficient projected gradient descent method to minimize a general form of a loss function and demonstrate how clustering and sparse coding with LDSs can be solved by the proposed method efficiently. To this end, we first derive a novel canonical form for representing the parameters of an LDS, and then show how gradient-descent updates through the projection on the space of LDSs can be achieved dexterously. In contrast to previous studies, our solution avoids any approximation in LDS modeling or during the optimization process. Extensive experiments reveal the superior performance of the proposed method in terms of the convergence and classification accuracy over state-of-the-art techniques.\n",
      "------------ 138 2019_900 -------------\n",
      "Computing approximate nearest neighbors in high dimensional spaces is a central problem in large-scale data mining with a wide range of applications in machine learning and data science. A popular and effective technique in computing nearest neighbors approximately is the locality-sensitive hashing (LSH) scheme. In this paper, we aim to develop LSH schemes for distance functions that measure the distance between two probability distributions, particularly for f-divergences as well as a generalization to capture mutual information loss. First, we provide a general framework to design LHS schemes for f-divergence distance functions and develop LSH schemes for the generalized Jensen-Shannon divergence and triangular discrimination in this framework. We show a two-sided approximation result for approximation of the generalized Jensen-Shannon divergence by the Hellinger distance, which may be of independent interest. Next, we show a general method of reducing the problem of designing an LSH scheme for a Krein kernel (which can be expressed as the difference of two positive definite kernels) to the problem of maximum inner product search. We exemplify this method by applying it to the mutual information loss, due to its several important applications such as model compression.\n",
      "------------ 139 2019_371 -------------\n",
      "Quantum information is a promising new paradigm for fast computations that can provide substantial speedups for many algorithms we use today. Among them, quantum machine learning is one of the most exciting applications of quantum computers. In this paper, we introduce q-means, a new quantum algorithm for clustering. It is a quantum version of a robust k-means algorithm, with similar convergence and precision guarantees. We also design a method to pick the initial centroids equivalent to the classical k-means++ method. Our algorithm provides currently an exponential speedup in the number of points of the dataset, compared to the classical k-means algorithm. We also detail the running time of q-means when applied to well-clusterable datasets. We provide a detailed runtime analysis and numerical simulations for specific datasets. Along with the algorithm, the theorems and tools introduced in this paper can be reused for various applications in quantum machine learning.\n",
      "------------ 140 2017_324 -------------\n",
      "We study the problem of designing models for machine learning tasks defined on sets. In contrast to the traditional approach of operating on fixed dimensional vectors, we consider objective functions defined on sets and are invariant to permutations. Such problems are widespread, ranging from the estimation of population statistics, to anomaly detection in piezometer data of embankment dams, to cosmology. Our main theorem characterizes the permutation invariant objective functions and provides a family of functions to which any permutation invariant objective function must belong. This family of functions has a special structure which enables us to design a deep network architecture that can operate on sets and which can be deployed on a variety of scenarios including both unsupervised and supervised learning tasks. We demonstrate the applicability of our method on population statistic estimation, point cloud classification, set expansion, and outlier detection.\n",
      "------------ 141 2017_543 -------------\n",
      "In this paper, we introduce and analyze Discriminative State-Space Models for forecasting non-stationary time series. We provide data-dependent generalization guarantees for learning these models based on the recently introduced notion of discrepancy. We provide an in-depth analysis of the complexity of such models. Finally, we also study the generalization guarantees for several structural risk minimization approaches to this problem and provide an efficient implementation for one of them which is based on a convex objective.\n",
      "------------ 142 2016_492 -------------\n",
      "We propose and analyze a regularization approach for structured prediction problems. We characterize a large class of loss functions that allows to naturally embed structured outputs in a linear space. We exploit this fact to design learning algorithms using a surrogate loss approach and regularization techniques. We prove universal consistency and finite sample bounds characterizing the generalization properties of the proposed method. Experimental results are provided to demonstrate the practical usefulness of the proposed approach.\n",
      "------------ 143 2019_652 -------------\n",
      "Decision tree algorithms have been among the most popular algorithms for interpretable (transparent) machine learning since the early 1980's. The problem that has plagued decision tree algorithms since their inception is their lack of optimality, or lack of guarantees of closeness to optimality: decision tree algorithms are often greedy or myopic, and sometimes produce unquestionably suboptimal models. Hardness of decision tree optimization is both a theoretical and practical obstacle, and even careful mathematical programming approaches have not been able to solve these problems efficiently. This work introduces the first practical algorithm for optimal decision trees for binary variables. The algorithm is a co-design of analytical bounds that reduce the search space and modern systems techniques, including data structures and a custom bit-vector library. We highlight possible steps to improving the scalability and speed of future generations of this algorithm based on insights from our theory and experiments.\n",
      "------------ 144 2016_444 -------------\n",
      "The move from hand-designed features to learned features in machine learning has been wildly successful. In spite of this, optimization algorithms are still designed by hand. In this paper we show how the design of an optimization algorithm can be cast as a learning problem, allowing the algorithm to learn to exploit structure in the problems of interest in an automatic way. Our learned algorithms, implemented by LSTMs, outperform generic, hand-designed competitors on the tasks for which they are trained, and also generalize well to new tasks with similar structure. We demonstrate this on a number of tasks, including simple convex problems, training neural networks, and styling images with neural art.\n",
      "------------ 145 2016_191 -------------\n",
      "Many machine learning applications involve jointly predicting multiple mutually dependent output variables. Learning to search is a family of methods where the complex decision problem is cast into a sequence of decisions via a search space. Although these methods have shown promise both in theory and in practice, implementing them has been burdensomely awkward. In this paper, we show the search space can be defined by an arbitrary imperative program, turning learning to search into a credit assignment compiler. Altogether with the algorithmic improvements for the compiler, we radically reduce the complexity of programming and the running time. We demonstrate the feasibility of our approach on multiple joint prediction tasks. In all cases, we obtain accuracies as high as alternative approaches, at drastically reduced execution and programming time.\n",
      "------------ 146 2015_264 -------------\n",
      "Theoretical and empirical evidence indicates that the depth of neural networks is crucial for their success. However, training becomes more difficult as depth increases, and training of very deep networks remains an open problem. Here we introduce a new architecture designed to overcome this. Our so-called highway networks allow unimpeded information flow across many layers on information highways. They are inspired by Long Short-Term Memory recurrent networks and use adaptive gating units to regulate the information flow. Even with hundreds of layers, highway networks can be trained directly through simple gradient descent. This enables the study of extremely deep and efficient architectures.\n",
      "------------ 147 2016_180 -------------\n",
      "The diverse world of machine learning applications has given rise to a plethora of algorithms and optimization methods, finely tuned to the specific regression or classification task at hand. We reduce the complexity of algorithm design for machine learning by reductions: we develop reductions that take a method developed for one setting and apply it to the entire spectrum of smoothness and strong-convexity in applications. Furthermore, unlike existing results, our new reductions are OPTIMAL and more PRACTICAL. We show how these new reductions give rise to new and faster running times on training linear classifiers for various families of loss functions, and conclude with experiments showing their successes also in practice.\n",
      "------------ 148 2015_21 -------------\n",
      "Despite the recent achievements in machine learning, we are still very far from achieving real artificial intelligence. In this paper, we discuss the limitations of standard deep learning approaches and show that some of these limitations can be overcome by learning how to grow the complexity of a model in a structured way. Specifically, we study the simplest sequence prediction problems that are beyond the scope of what is learnable with standard recurrent networks, algorithmically generated sequences which can only be learned by models which have the capacity to count and to memorize sequences. We show that some basic algorithms can be learned from sequential data using a recurrent network associated with a trainable memory.\n",
      "------------ 149 2016_469 -------------\n",
      "Gaussian Process bandit optimization has emerged as a powerful tool for optimizing noisy black box functions. One example in machine learning is hyper-parameter optimization where each evaluation of the target function may require training a model which may involve days or even weeks of computation. Most methods for this so-called “Bayesian optimization” only allow sequential exploration of the parameter space. However, it is often desirable to propose batches or sets of parameter values to explore simultaneously, especially when there are large parallel processing facilities at our disposal. Batch methods require modeling the interaction between the different evaluations in the batch, which can be expensive in complex scenarios. In this paper, we propose a new approach for parallelizing Bayesian optimization by modeling the diversity of a batch via Determinantal point processes (DPPs) whose kernels are learned automatically. This allows us to generalize a previous result as well as prove better regret bounds based on DPP sampling. Our experiments on a variety of synthetic and real-world robotics and hyper-parameter optimization tasks indicate that our DPP-based methods, especially those based on DPP sampling, outperform state-of-the-art methods.\n",
      "------------ 150 2018_551 -------------\n",
      "Bayesian optimization is a powerful tool for global optimization of expensive functions. One of its key components is the underlying probabilistic model used for the objective function f. In practice, however, it is often unclear how one should appropriately choose a model, especially when gathering data is expensive. In this work, we introduce a novel automated Bayesian optimization approach that dynamically selects promising models for explaining the observed data using Bayesian Optimization in the model space. Crucially, we account for the uncertainty in the choice of model; our method is capable of using multiple models to represent its current belief about f and subsequently using this information for decision making. We argue, and demonstrate empirically, that our approach automatically finds suitable models for the objective function, which ultimately results in more-efficient optimization.\n",
      "------------ 151 2018_82 -------------\n",
      "Most artificial intelligence models are limited in their ability to solve new tasks faster, without forgetting previously acquired knowledge. The recently emerging paradigm of continual learning aims to solve this issue, in which the model learns various tasks in a sequential fashion. In this work, a novel approach for continual learning is proposed, which searches for the best neural architecture for each coming task via sophisticatedly designed reinforcement learning strategies. We name it as Reinforced Continual Learning. Our method not only has good performance on preventing catastrophic forgetting but also fits new tasks well. The experiments on sequential classification tasks for variants of MNIST and CIFAR-100 datasets demonstrate that the proposed approach outperforms existing continual learning alternatives for deep networks.\n",
      "------------ 152 2019_428 -------------\n",
      "Reinforcement learning is known to be sample inefficient, preventing its application to many real-world problems, especially with high dimensional observations like images. Transferring knowledge from other auxiliary tasks is a powerful tool for improving the learning efficiency. However, the usage of auxiliary tasks has been limited so far due to the difficulty in selecting and combining different auxiliary tasks. In this work, we propose a principled online learning algorithm that dynamically combines different auxiliary tasks to speed up training for reinforcement learning. Our method is based on the idea that auxiliary tasks should provide gradient directions that, in the long term, help to decrease the loss of the main task. We show in various environments that our algorithm can effectively combine a variety of different auxiliary tasks and achieves significant speedup compared to previous heuristic approches of adapting auxiliary task weights.\n",
      "------------ 153 2016_393 -------------\n",
      "Clustering is a fundamental step in many information-retrieval and data-mining applications. Detecting clusters in graphs is also a key tool for finding the community structure in social and behavioral networks. In many of these applications, the input graph evolves over time in a continual and decentralized manner, and, to maintain a good clustering, the clustering algorithm needs to repeatedly probe the graph. Furthermore, there are often limitations on the frequency of such probes, either imposed explicitly by the online platform (e.g., in the case of crawling proprietary social networks like twitter) or implicitly because of resource limitations (e.g., in the case of crawling the web). In this paper, we study a model of clustering on evolving graphs that captures this aspect of the problem. Our model is based on the classical stochastic block model, which has been used to assess rigorously the quality of various static clustering methods. In our model, the algorithm is supposed to reconstruct the planted clustering, given the ability to query for small pieces of local information about the graph, at a limited rate. We design and analyze clustering algorithms that work in this model, and show asymptotically tight upper and lower bounds on their accuracy. Finally, we perform simulations, which demonstrate that our main asymptotic results hold true also in practice.\n",
      "------------ 154 2019_195 -------------\n",
      "Although optimization is the longstanding, algorithmic backbone of machine learning new models still require the time-consuming implementation of new solvers. As a result, there are thousands of implementations of optimization algorithms for machine learning problems. A natural question is, if it is always necessary to implement a new solver, or is there one algorithm that is sufficient for most models. Common belief suggests that such a one-algorithm-fits-all approach cannot work, because this algorithm cannot exploit model specific structure. At least, a generic algorithm cannot be efficient and robust on a wide variety of problems. Here, we challenge this common belief. We have designed and implemented the optimization framework GENO (GENeric Optimization) that combines a modeling language with a generic solver. GENO takes the declaration of an optimization problem and generates a solver for the specified problem class. The framework is flexible enough to encompass most of the classical machine learning problems. We show on a wide variety of classical but also some recently suggested problems that the automatically generated solvers are (1) as efficient as well engineered, specialized solvers, (2) more efficient by a decent margin than recent state-of-the-art solvers, and (3) orders of magnitude more efficient than classical modeling language plus solver approaches.\n",
      "------------ 155 2015_295 -------------\n",
      "Active learning methods automatically adapt data collection by selecting the most informative samples in order to accelerate machine learning. Because of this, real-world testing and comparing active learning algorithms requires collecting new datasets (adaptively), rather than simply applying algorithms to benchmark datasets, as is the norm in (passive) machine learning research. To facilitate the development, testing and deployment of active learning for real applications, we have built an open-source software system for large-scale active learning research and experimentation. The system, called NEXT, provides a unique platform for real-world, reproducible active learning research. This paper details the challenges of building the system and demonstrates its capabilities with several experiments. The results show how experimentation can help expose strengths and weaknesses of active learning algorithms, in sometimes unexpected and enlightening ways.\n",
      "------------ 156 2017_619 -------------\n",
      "We present a novel parallelisation scheme that simplifies the adaptation of learning algorithms to growing amounts of data as well as growing needs for accurate and confident predictions in critical applications. In contrast to other parallelisation techniques, it can be applied to a broad class of learning algorithms without further mathematical derivations and without writing dedicated code, while at the same time maintaining theoretical performance guarantees. Moreover, our parallelisation scheme is able to reduce the runtime of many learning algorithms to polylogarithmic time on quasi-polynomially many processing units. This is a significant step towards a general answer to an open question on efficient parallelisation of machine learning algorithms in the sense of Nick's Class (NC). The cost of this parallelisation is in the form of a larger sample complexity. Our empirical study confirms the potential of our parallelisation scheme with fixed numbers of processors and instances in realistic application scenarios.\n",
      "------------ 157 2019_1358 -------------\n",
      "When training complex neural networks, memory usage can be an important bottleneck. The question of when to rematerialize, i.e., to recompute intermediate values rather than retaining them in memory, becomes critical to achieving the best time and space efficiency. In this work we consider the rematerialization problem and devise efficient algorithms that use structural characterizations of computation graphs---treewidth and pathwidth---to obtain provably efficient rematerialization schedules. Our experiments demonstrate the performance of these algorithms on many common deep learning models.\n",
      "------------ 158 2017_81 -------------\n",
      "In recent years, great progress has been made in a variety of application domains thanks to the development of increasingly deeper neural networks. Unfortunately, the huge number of units of these networks makes them expensive both computationally and memory-wise. To overcome this, exploiting the fact that deep networks are over-parametrized, several compression strategies have been proposed. These methods, however, typically start from a network that has been trained in a standard manner, without considering such a future compression. In this paper, we propose to explicitly account for compression in the training process. To this end, we introduce a regularizer that encourages the parameter matrix of each layer to have low rank during training. We show that accounting for compression during training allows us to learn much more compact, yet at least as effective, models than state-of-the-art compression techniques.\n",
      "------------ 159 2018_301 -------------\n",
      "Progress in machine learning is measured by careful evaluation on problems of outstanding common interest. However, the proliferation of benchmark suites and environments, adversarial attacks, and other complications has diluted the basic evaluation model by overwhelming researchers with choices. Deliberate or accidental cherry picking is increasingly likely, and designing well-balanced evaluation suites requires increasing effort. In this paper we take a step back and propose Nash averaging. The approach builds on a detailed analysis of the algebraic structure of evaluation in two basic scenarios: agent-vs-agent and agent-vs-task. The key strength of Nash averaging is that it automatically adapts to redundancies in evaluation data, so that results are not biased by the incorporation of easy tasks or weak agents. Nash averaging thus encourages maximally inclusive evaluation -- since there is no harm (computational cost aside) from including all available tasks and agents.\n",
      "------------ 160 2018_308 -------------\n",
      "In order to achieve state-of-the-art performance, modern machine learning techniques require careful data pre-processing and hyperparameter tuning. Moreover, given the ever increasing number of machine learning models being developed, model selection is becoming increasingly important. Automating the selection and tuning of machine learning pipelines, which can include different data pre-processing methods and machine learning models, has long been one of the goals of the machine learning community. In this paper, we propose to solve this meta-learning task by combining ideas from collaborative filtering and Bayesian optimization. Specifically, we use a probabilistic matrix factorization model to transfer knowledge across experiments performed in hundreds of different datasets and use an acquisition function to guide the exploration of the space of possible ML pipelines. In our experiments, we show that our approach quickly identifies high-performing pipelines across a wide range of datasets, significantly outperforming the current state-of-the-art.\n",
      "------------ 161 2019_1145 -------------\n",
      "Discovery of causal relations from observational data is essential for many disciplines of science and real-world applications. However, unlike other machine learning algorithms, whose development has been greatly fostered by a large amount of available benchmark datasets, causal discovery algorithms are notoriously difficult to be systematically evaluated because few datasets with known ground-truth causal relations are available. In this work, we handle the problem of evaluating causal discovery algorithms by building a flexible simulator in the medical setting. We develop a neuropathic pain diagnosis simulator, inspired by the fact that the biological processes of neuropathic pathophysiology are well studied with well-understood causal influences. Our simulator exploits the causal graph of the neuropathic pain pathology and its parameters in the generator are estimated from real-life patient cases. We show that the data generated from our simulator have similar statistics as real-world data. As a clear advantage, the simulator can produce infinite samples without jeopardizing the privacy of real-world patients. Our simulator provides a natural tool for evaluating various types of causal discovery algorithms, including those to deal with practical issues in causal discovery, such as unknown confounders, selection bias, and missing data. Using our simulator, we have evaluated extensively causal discovery algorithms under various settings.\n",
      "------------ 162 2017_607 -------------\n",
      "The design of good heuristics or approximation algorithms for NP-hard combinatorial optimization problems often requires significant specialized knowledge and trial-and-error. Can we automate this challenging, tedious process, and learn the algorithms instead? In many real-world applications, it is typically the case that the same optimization problem is solved again and again on a regular basis, maintaining the same problem structure but differing in the data. This provides an opportunity for learning heuristic algorithms that exploit the structure of such recurring problems. In this paper, we propose a unique combination of reinforcement learning and graph embedding to address this challenge. The learned greedy policy behaves like a meta-algorithm that incrementally constructs a solution, and the action is determined by the output of a graph embedding network capturing the current state of the solution. We show that our framework can be applied to a diverse range of optimization problems over graphs, and learns effective algorithms for the Minimum Vertex Cover, Maximum Cut and Traveling Salesman problems.\n",
      "------------ 163 2018_13 -------------\n",
      "As machine learning becomes more widely used in practice, we need new methods to build complex intelligent systems that integrate learning with existing software, and with domain knowledge encoded as rules. As a case study, we present such a system that learns to parse Newtonian physics problems in textbooks. This system, Nuts&Bolts, learns a pipeline process that incorporates existing code, pre-learned machine learning models, and human engineered rules. It jointly trains the entire pipeline to prevent propagation of errors, using a combination of labelled and unlabelled data. Our approach achieves a good performance on the parsing task, outperforming the simple pipeline and its variants. Finally, we also show how Nuts&Bolts can be used to achieve improvements on a relation extraction task and on the end task of answering Newtonian physics problems.\n",
      "------------ 164 2016_403 -------------\n",
      "Better understanding of the potential benefits of information transfer and representation learning is an important step towards the goal of building intelligent systems that are able to persist in the world and learn over time. In this work, we consider a setting where the learner encounters a stream of tasks but is able to retain only limited information from each encountered task, such as a learned predictor. In contrast to most previous works analyzing this scenario, we do not make any distributional assumptions on the task generating process. Instead, we formulate a complexity measure that captures the diversity of the observed tasks. We provide a lifelong learning algorithm with error guarantees for every observed task (rather than on average). We show sample complexity reductions in comparison to solving every task in isolation in terms of our task complexity measure. Further, our algorithmic framework can naturally be viewed as learning a representation from encountered tasks with a neural network.\n",
      "------------ 165 2019_225 -------------\n",
      "This paper considers the problem of efficient exploration of unseen environments, a key challenge in AI. We propose a learning to explore' framework where we learn a policy from a distribution of environments. At test time, presented with an unseen environment from the same distribution, the policy aims to generalize the exploration strategy to visit the maximum number of unique states in a limited number of steps. We particularly focus on environments with graph-structured state-spaces that are encountered in many important real-world applications like software testing and map building. We formulate this task as a reinforcement learning problem where the\n",
      "exploration' agent is rewarded for transitioning to previously unseen environment states and employ a graph-structured memory to encode the agent's past trajectory. Experimental results demonstrate that our approach is extremely effective for exploration of spatial maps; and when applied on the challenging problems of coverage-guided software-testing of domain-specific programs and real-world mobile applications, it outperforms methods that have been hand-engineered by human experts.\n",
      "------------ 166 2019_96 -------------\n",
      "“Thinking in pictures,” [1] i.e., spatial-temporal reasoning, effortless and instantaneous for humans, is believed to be a significant ability to perform logical induction and a crucial factor in the intellectual history of technology development. Modern Artificial Intelligence (AI), fueled by massive datasets, deeper models, and mighty computation, has come to a stage where (super-)human-level performances are observed in certain specific tasks. However, current AI's ability in “thinking in pictures” is still far lacking behind. In this work, we study how to improve machines' reasoning ability on one challenging task of this kind: Raven's Progressive Matrices (RPM). Specifically, we borrow the very idea of “contrast effects” from the field of psychology, cognition, and education to design and train a permutation-invariant model. Inspired by cognitive studies, we equip our model with a simple inference module that is jointly trained with the perception backbone. Combining all the elements, we propose the Contrastive Perceptual Inference network (CoPINet) and empirically demonstrate that CoPINet sets the new state-of-the-art for permutation-invariant models on two major datasets. We conclude that spatial-temporal reasoning depends on envisaging the possibilities consistent with the relations between objects and can be solved from pixel-level inputs.\n",
      "------------ 167 2018_410 -------------\n",
      "Huge scale machine learning problems are nowadays tackled by distributed optimization algorithms, i.e. algorithms that leverage the compute power of many devices for training. The communication overhead is a key bottleneck that hinders perfect scalability. Various recent works proposed to use quantization or sparsification techniques to reduce the amount of data that needs to be communicated, for instance by only sending the most significant entries of the stochastic gradient (top-k sparsification). Whilst such schemes showed very promising performance in practice, they have eluded theoretical analysis so far. In this work we analyze Stochastic Gradient Descent (SGD) with k-sparsification or compression (for instance top-k or random-k) and show that this scheme converges at the same rate as vanilla SGD when equipped with error compensation (keeping track of accumulated errors in memory). That is, communication can be reduced by a factor of the dimension of the problem (sometimes even more) whilst still converging at the same rate. We present numerical experiments to illustrate the theoretical findings and the good scalability for distributed applications.\n",
      "------------ 168 2018_372 -------------\n",
      "Deep neural networks, and in particular recurrent networks, are promising candidates to control autonomous agents that interact in real-time with the physical world. However, this requires a seamless integration of temporal features into the network’s architecture. For the training of and inference with recurrent neural networks, they are usually rolled out over time, and different rollouts exist. Conventionally during inference, the layers of a network are computed in a sequential manner resulting in sparse temporal integration of information and long response times. In this study, we present a theoretical framework to describe rollouts, the level of model-parallelization they induce, and demonstrate differences in solving specific tasks. We prove that certain rollouts, also for networks with only skip and no recurrent connections, enable earlier and more frequent responses, and show empirically that these early responses have better performance. The streaming rollout maximizes these properties and enables a fully parallel execution of the network reducing runtime on massively parallel devices. Finally, we provide an open-source toolbox to design, train, evaluate, and interact with streaming rollouts.\n",
      "------------ 169 2018_16 -------------\n",
      "Similarity search is a fundamental problem in computing science with various applications and has attracted significant research attention, especially in large-scale search with high dimensions. Motivated by the evidence in biological science, our work develops a novel approach for similarity search. Fundamentally different from existing methods that typically reduce the dimension of the data to lessen the computational complexity and speed up the search, our approach projects the data into an even higher-dimensional space while ensuring the sparsity of the data in the output space, with the objective of further improving precision and speed. Specifically, our approach has two key steps. Firstly, it computes the optimal sparse lifting for given input samples and increases the dimension of the data while approximately preserving their pairwise similarity. Secondly, it seeks the optimal lifting operator that best maps input samples to the optimal sparse lifting. Computationally, both steps are modeled as optimization problems that can be efficiently and effectively solved by the Frank-Wolfe algorithm. Simple as it is, our approach has reported significantly improved results in empirical evaluations, and exhibited its high potentials in solving practical problems.\n",
      "------------ 170 2017_257 -------------\n",
      "Multi-agent predictive modeling is an essential step for understanding physical, social and team-play systems. Recently, Interaction Networks (INs) were proposed for the task of modeling multi-agent physical systems. One of the drawbacks of INs is scaling with the number of interactions in the system (typically quadratic or higher order in the number of agents). In this paper we introduce VAIN, a novel attentional architecture for multi-agent predictive modeling that scales linearly with the number of agents. We show that VAIN is effective for multi-agent predictive modeling. Our method is evaluated on tasks from challenging multi-agent prediction domains: chess and soccer, and outperforms competing multi-agent approaches.\n"
     ]
    }
   ],
   "source": [
    "for i, (a,d) in enumerate(zip(selectedArticles, selectedDid)):\n",
    "    print(\"------------ %s %s -------------\" % (str(i), str(d)))\n",
    "    \n",
    "    print(a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "171"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(selectedIds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "171"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(selectedArticles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(selectedArticles) == set(list(np.array(allAbs)[list(set(selectedIds))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "155"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(selectedArticles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "155"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(list(np.array(allAbs)[list(set(selectedIds))])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "selectedArticles = len(set(selectedArticles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['In many statistical problems, a more coarse-grained model may be suitable for population-level behaviour, whereas a more detailed model is appropriate for accurate modelling of individual behaviour. This raises the question of how to integrate both types of models. Methods such as posterior regularization follow the idea of generalized moment matching, in that they allow matchingexpectations between two models, but sometimes both models are most conveniently expressed as latent variable models. We propose latent Bayesian melding, which is motivated by averaging the distributions over populations statistics of both the individual-level and the population-level models under a logarithmic opinion pool framework. In a case study on electricity disaggregation, which is a type of single-channel blind source separation problem, we show that latent Bayesian melding leads to significantly more accurate predictions than an approach based solely on generalized moment matching.',\n",
       " 'In many learning problems, ranging from clustering to ranking through metric learning, empirical estimates of the risk functional consist of an average over tuples (e.g., pairs or triplets) of observations, rather than over individual observations. In this paper, we focus on how to best implement a stochastic approximation approach to solve such risk minimization problems. We argue that in the large-scale setting, gradient estimates should be obtained by sampling tuples of data points with replacement (incomplete U-statistics) instead of sampling data points without replacement (complete U-statistics based on subsamples). We develop a theoretical framework accounting for the substantial impact of this strategy on the generalization ability of the prediction model returned by the Stochastic Gradient Descent (SGD) algorithm. It reveals that the method we promote achieves a much better trade-off between statistical accuracy and computational cost. Beyond the rate bound analysis, experiments on AUC maximization and metric learning provide strong empirical evidence of the superiority of the proposed approach.',\n",
       " 'The method of random projection has been a popular tool for data compression, similarity search, and machine learning. In many practical scenarios, applying quantization on randomly projected data could be very helpful to further reduce storage cost and facilitate more efficient retrievals, while only suffering from little loss in accuracy. In real-world applications, however, data collected from different sources may be quantized under different schemes, which calls for a need to study the asymmetric quantization problem. In this paper, we investigate the cosine similarity estimators derived in such setting under the Lloyd-Max (LM) quantization scheme. We thoroughly analyze the biases and variances of a series of estimators including the basic simple estimators, their normalized versions, and their debiased versions. Furthermore, by studying the monotonicity, we show that the expectation of proposed estimators increases with the true cosine similarity, on a broader family of stair-shaped quantizers. Experiments on nearest neighbor search justify the theory and illustrate the effectiveness of our proposed estimators.',\n",
       " 'We consider the off-policy estimation problem of estimating the expected reward of a target policy using samples collected by a different behavior policy. Importance sampling (IS) has been a key technique to derive (nearly) unbiased estimators, but is known to suffer from an excessively high variance in long-horizon problems. In the extreme case of in infinite-horizon problems, the variance of an IS-based estimator may even be unbounded. In this paper, we propose a new off-policy estimation method that applies IS directly on the stationary state-visitation distributions to avoid the exploding variance issue faced by existing estimators.Our key contribution is a novel approach to estimating the density ratio of two stationary distributions, with trajectories sampled from only the behavior distribution. We develop a mini-max loss function for the estimation problem, and derive a closed-form solution for the case of RKHS. We support our method with both theoretical and empirical analyses.',\n",
       " 'While a typical supervised learning framework assumes that the inputs and the outputs are measured at the same levels of granularity, many applications, including global mapping of disease, only have access to outputs at a much coarser level than that of the inputs. Aggregation of outputs makes generalization to new inputs much more difficult. We consider an approach to this problem based on variational learning with a model of output aggregation and Gaussian processes, where aggregation leads to intractability of the standard evidence lower bounds. We propose new bounds and tractable approximations, leading to improved prediction accuracy and scalability to large datasets, while explicitly taking uncertainty into account. We develop a framework which extends to several types of likelihoods, including the Poisson model for aggregated count data. We apply our framework to a challenging and important problem, the fine-scale spatial modelling of malaria incidence, with over 1 million observations.',\n",
       " 'Many probabilistic models of interest in scientific computing and machine learning have expensive, black-box likelihoods that prevent the application of standard techniques for Bayesian inference, such as MCMC, which would require access to the gradient or a large number of likelihood evaluations. We introduce here a novel sample-efficient inference framework, Variational Bayesian Monte Carlo (VBMC). VBMC combines variational inference with Gaussian-process based, active-sampling Bayesian quadrature, using the latter to efficiently approximate the intractable integral in the variational objective. Our method produces both a nonparametric approximation of the posterior distribution and an approximate lower bound of the model evidence, useful for model selection. We demonstrate VBMC both on several synthetic likelihoods and on a neuronal model with data from real neurons. Across all tested problems and dimensions (up to D = 10), VBMC performs consistently well in reconstructing the posterior and the model evidence with a limited budget of likelihood evaluations, unlike other methods that work only in very low dimensions. Our framework shows great promise as a novel tool for posterior and model inference with expensive, black-box likelihoods.',\n",
       " 'A learned generative model often produces biased statistics relative to the underlying data distribution. A standard technique to correct this bias is importance sampling, where samples from the model are weighted by the likelihood ratio under model and true distributions. When the likelihood ratio is unknown, it can be estimated by training a probabilistic classifier to distinguish samples from the two distributions. We employ this likelihood-free importance weighting method to correct for the bias in generative models. We find that this technique consistently improves standard goodness-of-fit metrics for evaluating the sample quality of state-of-the-art deep generative models, suggesting reduced bias. Finally, we demonstrate its utility on representative applications in a) data augmentation for classification using generative adversarial networks, and b) model-based policy evaluation using off-policy data.',\n",
       " 'The simplest and most widely applied method for guaranteeing differential privacy is to add instance-independent noise to a statistic of interest that is scaled to its global sensitivity. However, global sensitivity is a worst-case notion that is often too conservative for realized dataset instances. We provide methods for scaling noise in an instance-dependent way and demonstrate that they provide greater accuracy under average-case distributional assumptions. Specifically, we consider the basic problem of privately estimating the mean of a real distribution from i.i.d. samples. The standard empirical mean estimator can have arbitrarily-high global sensitivity. We propose the trimmed mean estimator, which interpolates between the mean and the median, as a way of attaining much lower sensitivity on average while losing very little in terms of statistical accuracy. To privately estimate the trimmed mean, we revisit the smooth sensitivity framework of Nissim, Raskhodnikova, and Smith (STOC 2007), which provides a framework for using instance-dependent sensitivity. We propose three new additive noise distributions which provide concentrated differential privacy when scaled to smooth sensitivity. We provide theoretical and experimental evidence showing that our noise distributions compare favorably to others in the literature, in particular, when applied to the mean estimation problem.',\n",
       " 'Causal inference in randomized experiments typically assumes that the units of randomization and the units of analysis are one and the same. In some applications, however, these two roles are played by distinct entities linked by a bipartite graph. The key challenge in such bipartite settings is how to avoid interference bias, which would typically arise if we simply randomized the treatment at the level of analysis units. One effective way of minimizing interference bias in standard experiments is through cluster randomization, but this design has not been studied in the bipartite setting where conventional clustering schemes can lead to poorly powered experiments. This paper introduces a novel clustering objective and a corresponding algorithm that partitions a bipartite graph so as to maximize the statistical power of a bipartite experiment on that graph. Whereas previous work relied on balanced partitioning, our formulation suggests the use of a correlation clustering objective. We use a publicly-available graph of Amazon user-item reviews to validate our solution and illustrate how it substantially increases the statistical power in bipartite experiments.',\n",
       " 'The inference of the causal relationship between a pair of observed variables is a fundamental problem in science, and most existing approaches are based on one single causal model. In practice, however, observations are often collected from multiple sources with heterogeneous causal models due to certain uncontrollable factors, which renders causal analysis results obtained by a single model skeptical. In this paper, we generalize the Additive Noise Model (ANM) to a mixture model, which consists of a finite number of ANMs, and provide the condition of its causal identifiability. To conduct model estimation, we propose Gaussian Process Partially Observable Model (GPPOM), and incorporate independence enforcement into it to learn latent parameter associated with each observation. Causal inference and clustering according to the underlying generating mechanisms of the mixture model are addressed in this work. Experiments on synthetic and real data demonstrate the effectiveness of our proposed approach.',\n",
       " \"Variational inference is increasingly being addressed with stochastic optimization. In this setting, the gradient's variance plays a crucial role in the optimization procedure, since high variance gradients lead to poor convergence. A popular approach used to reduce gradient's variance involves the use of control variates. Despite the good results obtained, control variates developed for variational inference are typically looked at in isolation. In this paper we clarify the large number of control variates that are available by giving a systematic view of how they are derived. We also present a Bayesian risk minimization framework in which the quality of a procedure for combining control variates is quantified by its effect on optimization convergence rates, which leads to a very simple combination rule. Results show that combining a large number of control variates this way significantly improves the convergence of inference over using the typical gradient estimators or a reduced number of control variates.\",\n",
       " 'Recent advances in generative modeling have led to an increased interest in the study of statistical divergences as means of model comparison. Commonly used evaluation methods, such as the Frechet Inception Distance (FID), correlate well with the perceived quality of samples and are sensitive to mode dropping. However, these metrics are unable to distinguish between different failure cases since they only yield one-dimensional scores. We propose a novel definition of precision and recall for distributions which disentangles the divergence into two separate dimensions. The proposed notion is intuitive, retains desirable properties, and naturally leads to an efficient algorithm that can be used to evaluate generative models. We relate this notion to total variation as well as to recent evaluation metrics such as Inception Score and FID. To demonstrate the practical utility of the proposed approach we perform an empirical study on several variants of Generative Adversarial Networks and Variational Autoencoders. In an extensive set of experiments we show that the proposed metric is able to disentangle the quality of generated samples from the coverage of the target distribution.',\n",
       " 'Many modern data analysis problems involve inferences from streaming data. However, streaming data is not easily amenable to the standard probabilistic modeling approaches, which assume that we condition on finite data. We develop population variational Bayes, a new approach for using Bayesian modeling to analyze streams of data. It approximates a new type of distribution, the population posterior, which combines the notion of a population distribution of the data with Bayesian inference in a probabilistic model. We study our method with latent Dirichlet allocation and Dirichlet process mixtures on several large-scale data sets.',\n",
       " 'We consider the semi-supervised clustering problem where crowdsourcing provides noisy information about the pairwise comparisons on a small subset of data, i.e., whether a sample pair is in the same cluster. We propose a new approach that includes a deep generative model (DGM) to characterize low-level features of the data, and a statistical relational model for noisy pairwise annotations on its subset. The two parts share the latent variables. To make the model automatically trade-off between its complexity and fitting data, we also develop its fully Bayesian variant. The challenge of inference is addressed by fast (natural-gradient) stochastic variational inference algorithms, where we effectively combine variational message passing for the relational part and amortized learning of the DGM under a unified framework. Empirical results on synthetic and real-world datasets show that our model outperforms previous crowdsourced clustering methods.',\n",
       " 'Many applications of machine learning involve the analysis of large data frames -- matrices collecting heterogeneous measurements (binary, numerical, counts, etc.) across samples -- with missing values. Low-rank models, as studied by Udell et al. (2016), are popular in this framework for tasks such as visualization, clustering and missing value imputation. Yet, available methods with statistical guarantees and efficient optimization do not allow explicit modeling of main additive effects such as row and column, or covariate effects. In this paper, we introduce a low-rank interaction and sparse additive effects (LORIS) model which combines matrix regression on a dictionary and low-rank design, to estimate main effects and interactions simultaneously. We provide statistical guarantees in the form of upper bounds on the estimation error of both components. Then, we introduce a mixed coordinate gradient descent (MCGD) method which provably converges sub-linearly to an optimal solution and is computationally efficient for large scale data sets. We show on simulated and survey data that the method has a clear advantage over current practices.',\n",
       " 'In regression tasks, aleatoric uncertainty is commonly addressed by considering a parametric distribution of the output variable, which is based on strong assumptions such as symmetry, unimodality or by supposing a restricted shape. These assumptions are too limited in scenarios where complex shapes, strong skews or multiple modes are present. In this paper, we propose a generic deep learning framework that learns an Uncountable Mixture of Asymmetric Laplacians (UMAL), which will allow us to estimate heterogeneous distributions of the output variable and shows its connections to quantile regression. Despite having a fixed number of parameters, the model can be interpreted as an infinite mixture of components, which yields a flexible approximation for heterogeneous distributions. Apart from synthetic cases, we apply this model to room price forecasting and to predict financial operations in personal bank accounts. We demonstrate that UMAL produces proper distributions, which allows us to extract richer insights and to sharpen decision-making.',\n",
       " \"Conditional Density Estimation (CDE) models deal with estimating conditional distributions. The conditions imposed on the distribution are the inputs of the model. CDE is a challenging task as there is a fundamental trade-off between model complexity, representational capacity and overfitting. In this work, we propose to extend the model's input with latent variables and use Gaussian processes (GP) to map this augmented input onto samples from the conditional distribution. Our Bayesian approach allows for the modeling of small datasets, but we also provide the machinery for it to be applied to big data using stochastic variational inference. Our approach can be used to model densities even in sparse data regions, and allows for sharing learned structure between conditions. We illustrate the effectiveness and wide-reaching applicability of our model on a variety of real-world problems, such as spatio-temporal density estimation of taxi drop-offs, non-Gaussian noise modeling, and few-shot learning on omniglot images.\",\n",
       " 'Estimating treatment effects from observational data is challenging due to the missing counterfactuals. Matching is an effective strategy to tackle this problem. The widely used matching estimators such as nearest neighbor matching (NNM) pair the treated units with the most similar control units in terms of covariates, and then estimate treatment effects accordingly. However, the existing matching estimators have poor performance when the distributions of control and treatment groups are unbalanced. Moreover, theoretical analysis suggests that the bias of causal effect estimation would increase with the dimension of covariates. In this paper, we aim to address these problems by learning low-dimensional balanced and nonlinear representations (BNR) for observational data. In particular, we convert counterfactual prediction as a classification problem, develop a kernel learning model with domain adaptation constraint, and design a novel matching estimator. The dimension of covariates will be significantly reduced after projecting data to a low-dimensional subspace. Experiments on several synthetic and real-world datasets demonstrate the effectiveness of our approach.',\n",
       " \"The correlation between events is ubiquitous and important for temporal events modelling. In many cases, the correlation exists between not only events' emitted observations, but also their arrival times. State space models (e.g., hidden Markov model) and stochastic interaction point process models (e.g., Hawkes process) have been studied extensively yet separately for the two types of correlations in the past. In this paper, we propose a Bayesian nonparametric approach that considers both types of correlations via unifying and generalizing hidden semi-Markov model and interaction point process model. The proposed approach can simultaneously model both the observations and arrival times of temporal events, and determine the number of latent states from data. A Metropolis-within-particle-Gibbs sampler with ancestor resampling is developed for efficient posterior inference. The approach is tested on both synthetic and real-world data with promising outcomes.\",\n",
       " 'Energy-based models (EBMs) are powerful probabilistic models, but suffer from intractable sampling and density evaluation due to the partition function. As a result, inference in EBMs relies on approximate sampling algorithms, leading to a mismatch between the model and inference. Motivated by this, we consider the sampler-induced distribution as the model of interest and maximize the likelihood of this model. This yields a class of energy-inspired models (EIMs) that incorporate learned energy functions while still providing exact samples and tractable log-likelihood lower bounds. We describe and evaluate three instantiations of such models based on truncated rejection sampling, self-normalized importance sampling, and Hamiltonian importance sampling. These models out-perform or perform comparably to the recently proposed Learned Accept/RejectSampling algorithm and provide new insights on ranking Noise Contrastive Estimation and Contrastive Predictive Coding. Moreover, EIMs allow us to generalize a recent connection between multi-sample variational lower bounds and auxiliary variable variational inference. We show how recent variational bounds can be unified with EIMs as the variational family.',\n",
       " 'Summarizing high-dimensional data using a small number of parameters is a ubiquitous first step in the analysis of neuronal population activity. Recently developed methods use \"targeted\" approaches that work by identifying multiple, distinct low-dimensional subspaces of activity that capture the population response to individual experimental task variables, such as the value of a presented stimulus or the behavior of the animal. These methods have gained attention because they decompose total neural activity into what are ostensibly different parts of a neuronal computation. However, existing targeted methods have been developed outside of the confines of probabilistic modeling, making some aspects of the procedures ad hoc, or limited in flexibility or interpretability. Here we propose a new model-based method for targeted dimensionality reduction based on a probabilistic generative model of the population response data. The low-dimensional structure of our model is expressed as a low-rank factorization of a linear regression model. We perform efficient inference using a combination of expectation maximization and direct maximization of the marginal likelihood. We also develop an efficient method for estimating the dimensionality of each subspace. We show that our approach outperforms alternative methods in both mean squared error of the parameter estimates, and in identifying the correct dimensionality of encoding using simulated data. We also show that our method provides more accurate inference of low-dimensional subspaces of activity than a competing algorithm, demixed PCA.',\n",
       " 'Conformal prediction is a technique for constructing prediction intervals that attain valid coverage in finite samples, without making distributional assumptions. Despite this appeal, existing conformal methods can be unnecessarily conservative because they form intervals of constant or weakly varying length across the input space. In this paper we propose a new method that is fully adaptive to heteroscedasticity. It combines conformal prediction with classical quantile regression, inheriting the advantages of both. We establish a theoretical guarantee of valid coverage, supplemented by extensive experiments on popular regression datasets. We compare the efficiency of conformalized quantile regression to other conformal methods, showing that our method tends to produce shorter intervals.',\n",
       " 'Learning-from-crowds aims to design proper aggregation strategies to infer the unknown true labels from the noisy labels provided by ordinary web workers. This paper presents max-margin majority voting (M^3V) to improve the discriminative ability of majority voting and further presents a Bayesian generalization to incorporate the flexibility of generative methods on modeling noisy observations with worker confusion matrices. We formulate the joint learning as a regularized Bayesian inference problem, where the posterior regularization is derived by maximizing the margin between the aggregated score of a potential true label and that of any alternative label. Our Bayesian model naturally covers the Dawid-Skene estimator and M^3V. Empirical results demonstrate that our methods are competitive, often achieving better results than state-of-the-art estimators.',\n",
       " 'Variational Inference is a powerful tool in the Bayesian modeling toolkit, however, its effectiveness is determined by the expressivity of the utilized variational distributions in terms of their ability to match the true posterior distribution. In turn, the expressivity of the variational family is largely limited by the requirement of having a tractable density function. To overcome this roadblock, we introduce a new family of variational upper bounds on a marginal log-density in the case of hierarchical models (also known as latent variable models). We then derive a family of increasingly tighter variational lower bounds on the otherwise intractable standard evidence lower bound for hierarchical variational distributions, enabling the use of more expressive approximate posteriors. We show that previously known methods, such as Hierarchical Variational Models, Semi-Implicit Variational Inference and Doubly Semi-Implicit Variational Inference can be seen as special cases of the proposed approach, and empirically demonstrate superior performance of the proposed method in a set of experiments.',\n",
       " 'We propose Lomax delegate racing (LDR) to explicitly model the mechanism of survival under competing risks and to interpret how the covariates accelerate or decelerate the time to event. LDR explains non-monotonic covariate effects by racing a potentially infinite number of sub-risks, and consequently relaxes the ubiquitous proportional-hazards assumption which may be too restrictive. Moreover, LDR is naturally able to model not only censoring, but also missing event times or event types. For inference, we develop a Gibbs sampler under data augmentation for moderately sized data, along with a stochastic gradient descent maximum a posteriori inference algorithm for big data applications. Illustrative experiments are provided on both synthetic and real datasets, and comparison with various benchmark algorithms for survival analysis with competing risks demonstrates distinguished performance of LDR.',\n",
       " 'Functional brain networks are well described and estimated from data with Gaussian Graphical Models (GGMs), e.g.\\\\ using sparse inverse covariance estimators. Comparing functional connectivity of subjects in two populations calls for comparing these estimated GGMs. Our goal is to identify differences in GGMs known to have similar structure. We characterize the uncertainty of differences with confidence intervals obtained using a parametric distribution on parameters of a sparse estimator. Sparse penalties enable statistical guarantees and interpretable models even in high-dimensional and low-sample settings. Characterizing the distributions of sparse models is inherently challenging as the penalties produce a biased estimator. Recent work invokes the sparsity assumptions to effectively remove the bias from a sparse estimator such as the lasso. These distributions can be used to give confidence intervals on edges in GGMs, and by extension their differences. However, in the case of comparing GGMs, these estimators do not make use of any assumed joint structure among the GGMs. Inspired by priors from brain functional connectivity we derive the distribution of parameter differences under a joint penalty when parameters are known to be sparse in the difference. This leads us to introduce the debiased multi-task fused lasso, whose distribution can be characterized in an efficient manner. We then show how the debiased lasso and multi-task fused lasso can be used to obtain confidence intervals on edge differences in GGMs. We validate the techniques proposed on a set of synthetic examples as well as neuro-imaging dataset created for the study of autism.',\n",
       " 'In personalized recommendation systems, it is important to predict preferences of a user on items that have not been seen by that user yet. Similarly, in revenue management, it is important to predict outcomes of comparisons among those items that have never been compared so far. The MultiNomial Logit model, a popular discrete choice model, captures the structure of the hidden preferences with a low-rank matrix. In order to predict the preferences, we want to learn the underlying model from noisy observations of the low-rank matrix, collected as revealed preferences in various forms of ordinal data. A natural approach to learn such a model is to solve a convex relaxation of nuclear norm minimization. We present the convex relaxation approach in two contexts of interest: collaborative ranking and bundled choice modeling. In both cases, we show that the convex relaxation is minimax optimal. We prove an upper bound on the resulting error with finite samples, and provide a matching information-theoretic lower bound.',\n",
       " 'Deep latent variable models (DLVMs) combine the approximation abilities of deep neural networks and the statistical foundations of generative models. Variational methods are commonly used for inference; however, the exact likelihood of these models has been largely overlooked. The purpose of this work is to study the general properties of this quantity and to show how they can be leveraged in practice. We focus on important inferential problems that rely on the likelihood: estimation and missing data imputation. First, we investigate maximum likelihood estimation for DLVMs: in particular, we show that most unconstrained models used for continuous data have an unbounded likelihood function. This problematic behaviour is demonstrated to be a source of mode collapse. We also show how to ensure the existence of maximum likelihood estimates, and draw useful connections with nonparametric mixture models. Finally, we describe an algorithm for missing data imputation using the exact conditional likelihood of a DLVM. On several data sets, our algorithm consistently and significantly outperforms the usual imputation scheme used for DLVMs.',\n",
       " 'Modern applications of machine learning (ML) deal with increasingly heterogeneous datasets comprised of data collected from overlapping latent subpopulations. As a result, traditional models trained over large datasets may fail to recognize highly predictive localized effects in favour of weakly predictive global patterns. This is a problem because localized effects are critical to developing individualized policies and treatment plans in applications ranging from precision medicine to advertising. To address this challenge, we propose to estimate sample-specific models that tailor inference and prediction at the individual level. In contrast to classical ML models that estimate a single, complex model (or only a few complex models), our approach produces a model personalized to each sample. These sample-specific models can be studied to understand subgroup dynamics that go beyond coarse-grained class labels. Crucially, our approach does not assume that relationships between samples (e.g. a similarity network) are known a priori. Instead, we use unmodeled covariates to learn a latent distance metric over the samples. We apply this approach to financial, biomedical, and electoral data as well as simulated data and show that sample-specific models provide fine-grained interpretations of complicated phenomena without sacrificing predictive accuracy compared to state-of-the-art models such as deep neural networks.',\n",
       " 'Generalized linear models (GLMs)---such as logistic regression, Poisson regression, and robust regression---provide interpretable models for diverse data types. Probabilistic approaches, particularly Bayesian ones, allow coherent estimates of uncertainty, incorporation of prior information, and sharing of power across experiments via hierarchical models. In practice, however, the approximate Bayesian methods necessary for inference have either failed to scale to large data sets or failed to provide theoretical guarantees on the quality of inference. We propose a new approach based on constructing polynomial approximate sufficient statistics for GLMs (PASS-GLM). We demonstrate that our method admits a simple algorithm as well as trivial streaming and distributed extensions that do not compound error across computations. We provide theoretical guarantees on the quality of point (MAP) estimates, the approximate posterior, and posterior mean and uncertainty estimates. We validate our approach empirically in the case of logistic regression using a quadratic approximation and show competitive performance with stochastic gradient descent, MCMC, and the Laplace approximation in terms of speed and multiple measures of accuracy---including on an advertising data set with 40 million data points and 20,000 covariates.']"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(np.array(allAbs)[np.argsort(D[:,-7])][:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 0 ------------\n",
      "Optimization is the algorithmic workhorse of a lot of modern AI and machine learning. Increasingly often, learning and decision tasks are formulated, analyzed and solved as large-scale optimization problems. As these problems grow in size (for example, due to increasing amounts of data or a desire to coordinate ever more decisions) existing algorithms struggle to find a sufficiently good solution within a short time. This research project aims at developing fundamental theory and algorithms for which are able to solve large-scale optimization problem at an unprecedented speed, eventually supporting decision-making in real-time.\n",
      "----------- 1 ------------\n",
      "Decision tree algorithms have been among the most popular algorithms for interpretable (transparent) machine learning since the early 1980's. The problem that has plagued decision tree algorithms since their inception is their lack of optimality, or lack of guarantees of closeness to optimality: decision tree algorithms are often greedy or myopic, and sometimes produce unquestionably suboptimal models. Hardness of decision tree optimization is both a theoretical and practical obstacle, and even careful mathematical programming approaches have not been able to solve these problems efficiently. This work introduces the first practical algorithm for optimal decision trees for binary variables. The algorithm is a co-design of analytical bounds that reduce the search space and modern systems techniques, including data structures and a custom bit-vector library. We highlight possible steps to improving the scalability and speed of future generations of this algorithm based on insights from our theory and experiments.\n",
      "----------- 2 ------------\n",
      "The move from hand-designed features to learned features in machine learning has been wildly successful. In spite of this, optimization algorithms are still designed by hand. In this paper we show how the design of an optimization algorithm can be cast as a learning problem, allowing the algorithm to learn to exploit structure in the problems of interest in an automatic way. Our learned algorithms, implemented by LSTMs, outperform generic, hand-designed competitors on the tasks for which they are trained, and also generalize well to new tasks with similar structure. We demonstrate this on a number of tasks, including simple convex problems, training neural networks, and styling images with neural art.\n",
      "----------- 3 ------------\n",
      "Many machine learning applications involve jointly predicting multiple mutually dependent output variables. Learning to search is a family of methods where the complex decision problem is cast into a sequence of decisions via a search space. Although these methods have shown promise both in theory and in practice, implementing them has been burdensomely awkward. In this paper, we show the search space can be defined by an arbitrary imperative program, turning learning to search into a credit assignment compiler. Altogether with the algorithmic improvements for the compiler, we radically reduce the complexity of programming and the running time. We demonstrate the feasibility of our approach on multiple joint prediction tasks. In all cases, we obtain accuracies as high as alternative approaches, at drastically reduced execution and programming time.\n",
      "----------- 4 ------------\n",
      "Theoretical and empirical evidence indicates that the depth of neural networks is crucial for their success. However, training becomes more difficult as depth increases, and training of very deep networks remains an open problem. Here we introduce a new architecture designed to overcome this. Our so-called highway networks allow unimpeded information flow across many layers on information highways. They are inspired by Long Short-Term Memory recurrent networks and use adaptive gating units to regulate the information flow. Even with hundreds of layers, highway networks can be trained directly through simple gradient descent. This enables the study of extremely deep and efficient architectures.\n",
      "----------- 5 ------------\n",
      "The diverse world of machine learning applications has given rise to a plethora of algorithms and optimization methods, finely tuned to the specific regression or classification task at hand. We reduce the complexity of algorithm design for machine learning by reductions: we develop reductions that take a method developed for one setting and apply it to the entire spectrum of smoothness and strong-convexity in applications. Furthermore, unlike existing results, our new reductions are OPTIMAL and more PRACTICAL. We show how these new reductions give rise to new and faster running times on training linear classifiers for various families of loss functions, and conclude with experiments showing their successes also in practice.\n",
      "----------- 6 ------------\n",
      "Despite the recent achievements in machine learning, we are still very far from achieving real artificial intelligence. In this paper, we discuss the limitations of standard deep learning approaches and show that some of these limitations can be overcome by learning how to grow the complexity of a model in a structured way. Specifically, we study the simplest sequence prediction problems that are beyond the scope of what is learnable with standard recurrent networks, algorithmically generated sequences which can only be learned by models which have the capacity to count and to memorize sequences. We show that some basic algorithms can be learned from sequential data using a recurrent network associated with a trainable memory.\n",
      "----------- 7 ------------\n",
      "Gaussian Process bandit optimization has emerged as a powerful tool for optimizing noisy black box functions. One example in machine learning is hyper-parameter optimization where each evaluation of the target function may require training a model which may involve days or even weeks of computation. Most methods for this so-called “Bayesian optimization” only allow sequential exploration of the parameter space. However, it is often desirable to propose batches or sets of parameter values to explore simultaneously, especially when there are large parallel processing facilities at our disposal. Batch methods require modeling the interaction between the different evaluations in the batch, which can be expensive in complex scenarios. In this paper, we propose a new approach for parallelizing Bayesian optimization by modeling the diversity of a batch via Determinantal point processes (DPPs) whose kernels are learned automatically. This allows us to generalize a previous result as well as prove better regret bounds based on DPP sampling. Our experiments on a variety of synthetic and real-world robotics and hyper-parameter optimization tasks indicate that our DPP-based methods, especially those based on DPP sampling, outperform state-of-the-art methods.\n",
      "----------- 8 ------------\n",
      "Bayesian optimization is a powerful tool for global optimization of expensive functions. One of its key components is the underlying probabilistic model used for the objective function f. In practice, however, it is often unclear how one should appropriately choose a model, especially when gathering data is expensive. In this work, we introduce a novel automated Bayesian optimization approach that dynamically selects promising models for explaining the observed data using Bayesian Optimization in the model space. Crucially, we account for the uncertainty in the choice of model; our method is capable of using multiple models to represent its current belief about f and subsequently using this information for decision making. We argue, and demonstrate empirically, that our approach automatically finds suitable models for the objective function, which ultimately results in more-efficient optimization.\n",
      "----------- 9 ------------\n",
      "Most artificial intelligence models are limited in their ability to solve new tasks faster, without forgetting previously acquired knowledge. The recently emerging paradigm of continual learning aims to solve this issue, in which the model learns various tasks in a sequential fashion. In this work, a novel approach for continual learning is proposed, which searches for the best neural architecture for each coming task via sophisticatedly designed reinforcement learning strategies. We name it as Reinforced Continual Learning. Our method not only has good performance on preventing catastrophic forgetting but also fits new tasks well. The experiments on sequential classification tasks for variants of MNIST and CIFAR-100 datasets demonstrate that the proposed approach outperforms existing continual learning alternatives for deep networks.\n",
      "----------- 10 ------------\n",
      "Reinforcement learning is known to be sample inefficient, preventing its application to many real-world problems, especially with high dimensional observations like images. Transferring knowledge from other auxiliary tasks is a powerful tool for improving the learning efficiency. However, the usage of auxiliary tasks has been limited so far due to the difficulty in selecting and combining different auxiliary tasks. In this work, we propose a principled online learning algorithm that dynamically combines different auxiliary tasks to speed up training for reinforcement learning. Our method is based on the idea that auxiliary tasks should provide gradient directions that, in the long term, help to decrease the loss of the main task. We show in various environments that our algorithm can effectively combine a variety of different auxiliary tasks and achieves significant speedup compared to previous heuristic approches of adapting auxiliary task weights.\n",
      "----------- 11 ------------\n",
      "Clustering is a fundamental step in many information-retrieval and data-mining applications. Detecting clusters in graphs is also a key tool for finding the community structure in social and behavioral networks. In many of these applications, the input graph evolves over time in a continual and decentralized manner, and, to maintain a good clustering, the clustering algorithm needs to repeatedly probe the graph. Furthermore, there are often limitations on the frequency of such probes, either imposed explicitly by the online platform (e.g., in the case of crawling proprietary social networks like twitter) or implicitly because of resource limitations (e.g., in the case of crawling the web). In this paper, we study a model of clustering on evolving graphs that captures this aspect of the problem. Our model is based on the classical stochastic block model, which has been used to assess rigorously the quality of various static clustering methods. In our model, the algorithm is supposed to reconstruct the planted clustering, given the ability to query for small pieces of local information about the graph, at a limited rate. We design and analyze clustering algorithms that work in this model, and show asymptotically tight upper and lower bounds on their accuracy. Finally, we perform simulations, which demonstrate that our main asymptotic results hold true also in practice.\n",
      "----------- 12 ------------\n",
      "Although optimization is the longstanding, algorithmic backbone of machine learning new models still require the time-consuming implementation of new solvers. As a result, there are thousands of implementations of optimization algorithms for machine learning problems. A natural question is, if it is always necessary to implement a new solver, or is there one algorithm that is sufficient for most models. Common belief suggests that such a one-algorithm-fits-all approach cannot work, because this algorithm cannot exploit model specific structure. At least, a generic algorithm cannot be efficient and robust on a wide variety of problems. Here, we challenge this common belief. We have designed and implemented the optimization framework GENO (GENeric Optimization) that combines a modeling language with a generic solver. GENO takes the declaration of an optimization problem and generates a solver for the specified problem class. The framework is flexible enough to encompass most of the classical machine learning problems. We show on a wide variety of classical but also some recently suggested problems that the automatically generated solvers are (1) as efficient as well engineered, specialized solvers, (2) more efficient by a decent margin than recent state-of-the-art solvers, and (3) orders of magnitude more efficient than classical modeling language plus solver approaches.\n",
      "----------- 13 ------------\n",
      "Active learning methods automatically adapt data collection by selecting the most informative samples in order to accelerate machine learning. Because of this, real-world testing and comparing active learning algorithms requires collecting new datasets (adaptively), rather than simply applying algorithms to benchmark datasets, as is the norm in (passive) machine learning research. To facilitate the development, testing and deployment of active learning for real applications, we have built an open-source software system for large-scale active learning research and experimentation. The system, called NEXT, provides a unique platform for real-world, reproducible active learning research. This paper details the challenges of building the system and demonstrates its capabilities with several experiments. The results show how experimentation can help expose strengths and weaknesses of active learning algorithms, in sometimes unexpected and enlightening ways.\n",
      "----------- 14 ------------\n",
      "We present a novel parallelisation scheme that simplifies the adaptation of learning algorithms to growing amounts of data as well as growing needs for accurate and confident predictions in critical applications. In contrast to other parallelisation techniques, it can be applied to a broad class of learning algorithms without further mathematical derivations and without writing dedicated code, while at the same time maintaining theoretical performance guarantees. Moreover, our parallelisation scheme is able to reduce the runtime of many learning algorithms to polylogarithmic time on quasi-polynomially many processing units. This is a significant step towards a general answer to an open question on efficient parallelisation of machine learning algorithms in the sense of Nick's Class (NC). The cost of this parallelisation is in the form of a larger sample complexity. Our empirical study confirms the potential of our parallelisation scheme with fixed numbers of processors and instances in realistic application scenarios.\n",
      "----------- 15 ------------\n",
      "When training complex neural networks, memory usage can be an important bottleneck. The question of when to rematerialize, i.e., to recompute intermediate values rather than retaining them in memory, becomes critical to achieving the best time and space efficiency. In this work we consider the rematerialization problem and devise efficient algorithms that use structural characterizations of computation graphs---treewidth and pathwidth---to obtain provably efficient rematerialization schedules. Our experiments demonstrate the performance of these algorithms on many common deep learning models.\n",
      "----------- 16 ------------\n",
      "In recent years, great progress has been made in a variety of application domains thanks to the development of increasingly deeper neural networks. Unfortunately, the huge number of units of these networks makes them expensive both computationally and memory-wise. To overcome this, exploiting the fact that deep networks are over-parametrized, several compression strategies have been proposed. These methods, however, typically start from a network that has been trained in a standard manner, without considering such a future compression. In this paper, we propose to explicitly account for compression in the training process. To this end, we introduce a regularizer that encourages the parameter matrix of each layer to have low rank during training. We show that accounting for compression during training allows us to learn much more compact, yet at least as effective, models than state-of-the-art compression techniques.\n",
      "----------- 17 ------------\n",
      "Progress in machine learning is measured by careful evaluation on problems of outstanding common interest. However, the proliferation of benchmark suites and environments, adversarial attacks, and other complications has diluted the basic evaluation model by overwhelming researchers with choices. Deliberate or accidental cherry picking is increasingly likely, and designing well-balanced evaluation suites requires increasing effort. In this paper we take a step back and propose Nash averaging. The approach builds on a detailed analysis of the algebraic structure of evaluation in two basic scenarios: agent-vs-agent and agent-vs-task. The key strength of Nash averaging is that it automatically adapts to redundancies in evaluation data, so that results are not biased by the incorporation of easy tasks or weak agents. Nash averaging thus encourages maximally inclusive evaluation -- since there is no harm (computational cost aside) from including all available tasks and agents.\n",
      "----------- 18 ------------\n",
      "In order to achieve state-of-the-art performance, modern machine learning techniques require careful data pre-processing and hyperparameter tuning. Moreover, given the ever increasing number of machine learning models being developed, model selection is becoming increasingly important. Automating the selection and tuning of machine learning pipelines, which can include different data pre-processing methods and machine learning models, has long been one of the goals of the machine learning community. In this paper, we propose to solve this meta-learning task by combining ideas from collaborative filtering and Bayesian optimization. Specifically, we use a probabilistic matrix factorization model to transfer knowledge across experiments performed in hundreds of different datasets and use an acquisition function to guide the exploration of the space of possible ML pipelines. In our experiments, we show that our approach quickly identifies high-performing pipelines across a wide range of datasets, significantly outperforming the current state-of-the-art.\n",
      "----------- 19 ------------\n",
      "Discovery of causal relations from observational data is essential for many disciplines of science and real-world applications. However, unlike other machine learning algorithms, whose development has been greatly fostered by a large amount of available benchmark datasets, causal discovery algorithms are notoriously difficult to be systematically evaluated because few datasets with known ground-truth causal relations are available. In this work, we handle the problem of evaluating causal discovery algorithms by building a flexible simulator in the medical setting. We develop a neuropathic pain diagnosis simulator, inspired by the fact that the biological processes of neuropathic pathophysiology are well studied with well-understood causal influences. Our simulator exploits the causal graph of the neuropathic pain pathology and its parameters in the generator are estimated from real-life patient cases. We show that the data generated from our simulator have similar statistics as real-world data. As a clear advantage, the simulator can produce infinite samples without jeopardizing the privacy of real-world patients. Our simulator provides a natural tool for evaluating various types of causal discovery algorithms, including those to deal with practical issues in causal discovery, such as unknown confounders, selection bias, and missing data. Using our simulator, we have evaluated extensively causal discovery algorithms under various settings.\n",
      "----------- 20 ------------\n",
      "The design of good heuristics or approximation algorithms for NP-hard combinatorial optimization problems often requires significant specialized knowledge and trial-and-error. Can we automate this challenging, tedious process, and learn the algorithms instead? In many real-world applications, it is typically the case that the same optimization problem is solved again and again on a regular basis, maintaining the same problem structure but differing in the data. This provides an opportunity for learning heuristic algorithms that exploit the structure of such recurring problems. In this paper, we propose a unique combination of reinforcement learning and graph embedding to address this challenge. The learned greedy policy behaves like a meta-algorithm that incrementally constructs a solution, and the action is determined by the output of a graph embedding network capturing the current state of the solution. We show that our framework can be applied to a diverse range of optimization problems over graphs, and learns effective algorithms for the Minimum Vertex Cover, Maximum Cut and Traveling Salesman problems.\n",
      "----------- 21 ------------\n",
      "As machine learning becomes more widely used in practice, we need new methods to build complex intelligent systems that integrate learning with existing software, and with domain knowledge encoded as rules. As a case study, we present such a system that learns to parse Newtonian physics problems in textbooks. This system, Nuts&Bolts, learns a pipeline process that incorporates existing code, pre-learned machine learning models, and human engineered rules. It jointly trains the entire pipeline to prevent propagation of errors, using a combination of labelled and unlabelled data. Our approach achieves a good performance on the parsing task, outperforming the simple pipeline and its variants. Finally, we also show how Nuts&Bolts can be used to achieve improvements on a relation extraction task and on the end task of answering Newtonian physics problems.\n",
      "----------- 22 ------------\n",
      "Better understanding of the potential benefits of information transfer and representation learning is an important step towards the goal of building intelligent systems that are able to persist in the world and learn over time. In this work, we consider a setting where the learner encounters a stream of tasks but is able to retain only limited information from each encountered task, such as a learned predictor. In contrast to most previous works analyzing this scenario, we do not make any distributional assumptions on the task generating process. Instead, we formulate a complexity measure that captures the diversity of the observed tasks. We provide a lifelong learning algorithm with error guarantees for every observed task (rather than on average). We show sample complexity reductions in comparison to solving every task in isolation in terms of our task complexity measure. Further, our algorithmic framework can naturally be viewed as learning a representation from encountered tasks with a neural network.\n",
      "----------- 23 ------------\n",
      "This paper considers the problem of efficient exploration of unseen environments, a key challenge in AI. We propose a learning to explore' framework where we learn a policy from a distribution of environments. At test time, presented with an unseen environment from the same distribution, the policy aims to generalize the exploration strategy to visit the maximum number of unique states in a limited number of steps. We particularly focus on environments with graph-structured state-spaces that are encountered in many important real-world applications like software testing and map building. We formulate this task as a reinforcement learning problem where the\n",
      "exploration' agent is rewarded for transitioning to previously unseen environment states and employ a graph-structured memory to encode the agent's past trajectory. Experimental results demonstrate that our approach is extremely effective for exploration of spatial maps; and when applied on the challenging problems of coverage-guided software-testing of domain-specific programs and real-world mobile applications, it outperforms methods that have been hand-engineered by human experts.\n",
      "----------- 24 ------------\n",
      "“Thinking in pictures,” [1] i.e., spatial-temporal reasoning, effortless and instantaneous for humans, is believed to be a significant ability to perform logical induction and a crucial factor in the intellectual history of technology development. Modern Artificial Intelligence (AI), fueled by massive datasets, deeper models, and mighty computation, has come to a stage where (super-)human-level performances are observed in certain specific tasks. However, current AI's ability in “thinking in pictures” is still far lacking behind. In this work, we study how to improve machines' reasoning ability on one challenging task of this kind: Raven's Progressive Matrices (RPM). Specifically, we borrow the very idea of “contrast effects” from the field of psychology, cognition, and education to design and train a permutation-invariant model. Inspired by cognitive studies, we equip our model with a simple inference module that is jointly trained with the perception backbone. Combining all the elements, we propose the Contrastive Perceptual Inference network (CoPINet) and empirically demonstrate that CoPINet sets the new state-of-the-art for permutation-invariant models on two major datasets. We conclude that spatial-temporal reasoning depends on envisaging the possibilities consistent with the relations between objects and can be solved from pixel-level inputs.\n",
      "----------- 25 ------------\n",
      "Huge scale machine learning problems are nowadays tackled by distributed optimization algorithms, i.e. algorithms that leverage the compute power of many devices for training. The communication overhead is a key bottleneck that hinders perfect scalability. Various recent works proposed to use quantization or sparsification techniques to reduce the amount of data that needs to be communicated, for instance by only sending the most significant entries of the stochastic gradient (top-k sparsification). Whilst such schemes showed very promising performance in practice, they have eluded theoretical analysis so far. In this work we analyze Stochastic Gradient Descent (SGD) with k-sparsification or compression (for instance top-k or random-k) and show that this scheme converges at the same rate as vanilla SGD when equipped with error compensation (keeping track of accumulated errors in memory). That is, communication can be reduced by a factor of the dimension of the problem (sometimes even more) whilst still converging at the same rate. We present numerical experiments to illustrate the theoretical findings and the good scalability for distributed applications.\n",
      "----------- 26 ------------\n",
      "Deep neural networks, and in particular recurrent networks, are promising candidates to control autonomous agents that interact in real-time with the physical world. However, this requires a seamless integration of temporal features into the network’s architecture. For the training of and inference with recurrent neural networks, they are usually rolled out over time, and different rollouts exist. Conventionally during inference, the layers of a network are computed in a sequential manner resulting in sparse temporal integration of information and long response times. In this study, we present a theoretical framework to describe rollouts, the level of model-parallelization they induce, and demonstrate differences in solving specific tasks. We prove that certain rollouts, also for networks with only skip and no recurrent connections, enable earlier and more frequent responses, and show empirically that these early responses have better performance. The streaming rollout maximizes these properties and enables a fully parallel execution of the network reducing runtime on massively parallel devices. Finally, we provide an open-source toolbox to design, train, evaluate, and interact with streaming rollouts.\n",
      "----------- 27 ------------\n",
      "Similarity search is a fundamental problem in computing science with various applications and has attracted significant research attention, especially in large-scale search with high dimensions. Motivated by the evidence in biological science, our work develops a novel approach for similarity search. Fundamentally different from existing methods that typically reduce the dimension of the data to lessen the computational complexity and speed up the search, our approach projects the data into an even higher-dimensional space while ensuring the sparsity of the data in the output space, with the objective of further improving precision and speed. Specifically, our approach has two key steps. Firstly, it computes the optimal sparse lifting for given input samples and increases the dimension of the data while approximately preserving their pairwise similarity. Secondly, it seeks the optimal lifting operator that best maps input samples to the optimal sparse lifting. Computationally, both steps are modeled as optimization problems that can be efficiently and effectively solved by the Frank-Wolfe algorithm. Simple as it is, our approach has reported significantly improved results in empirical evaluations, and exhibited its high potentials in solving practical problems.\n",
      "----------- 28 ------------\n",
      "Multi-agent predictive modeling is an essential step for understanding physical, social and team-play systems. Recently, Interaction Networks (INs) were proposed for the task of modeling multi-agent physical systems. One of the drawbacks of INs is scaling with the number of interactions in the system (typically quadratic or higher order in the number of agents). In this paper we introduce VAIN, a novel attentional architecture for multi-agent predictive modeling that scales linearly with the number of agents. We show that VAIN is effective for multi-agent predictive modeling. Our method is evaluated on tasks from challenging multi-agent prediction domains: chess and soccer, and outperforms competing multi-agent approaches.\n",
      "----------- 29 ------------\n",
      "Key to structured prediction is exploiting the problem's structure to simplify the learning process. A major challenge arises when data exhibit a local structure (i.e., are made\n",
      "by parts'') that can be leveraged to better approximate the relation between (parts of) the input and (parts of) the output. Recent literature on signal processing, and in particular computer vision, shows that capturing these aspects is indeed essential to achieve state-of-the-art performance. However, in this context algorithms are typically derived on a case-by-case basis. In this work we propose the first theoretical framework to deal with part-based data from a general perspective and study a novel method within the setting of statistical learning theory. Our analysis is novel in that it explicitly quantifies the benefits of leveraging the part-based structure of a problem on the learning rates of the proposed estimator.\n",
      "----------- 30 ------------\n",
      "Perception and reasoning are two representative abilities of intelligence that are integrated seamlessly during human problem-solving processes. In the area of artificial intelligence (AI), the two abilities are usually realised by machine learning and logic programming, respectively. However, the two categories of techniques were developed separately throughout most of the history of AI. In this paper, we present the abductive learning targeted at unifying the two AI paradigms in a mutually beneficial way, where the machine learning model learns to perceive primitive logic facts from data, while logical reasoning can exploit symbolic domain knowledge and correct the wrongly perceived facts for improving the machine learning models. Furthermore, we propose a novel approach to optimise the machine learning model and the logical reasoning model jointly. We demonstrate that by using abductive learning, machines can learn to recognise numbers and resolve unknown mathematical operations simultaneously from images of simple hand-written equations. Moreover, the learned models can be generalised to longer equations and adapted to different tasks, which is beyond the capability of state-of-the-art deep learning models.\n",
      "----------- 31 ------------\n",
      "Finding the maximum a-posteriori (MAP) assignment is a central task in graphical models. Since modern applications give rise to very large problem instances, there is increasing need for efficient solvers. In this work we propose to improve the efficiency of coordinate-minimization-based dual-decomposition solvers by running their updates asynchronously in parallel. In this case message-passing inference is performed by multiple processing units simultaneously without coordination, all reading and writing to shared memory. We analyze the convergence properties of the resulting algorithms and identify settings where speedup gains can be expected. Our numerical evaluations show that this approach indeed achieves significant speedups in common computer vision tasks.\n",
      "----------- 32 ------------\n",
      "Neural Architecture Search (NAS) has been quite successful in constructing state-of-the-art models on a variety of tasks. Unfortunately, the computational cost can make it difficult to scale. In this paper, we make the first attempt to study Meta Architecture Search which aims at learning a task-agnostic representation that can be used to speed up the process of architecture search on a large number of tasks. We propose the Bayesian Meta Architecture SEarch (BASE) framework which takes advantage of a Bayesian formulation of the architecture search problem to learn over an entire set of tasks simultaneously. We show that on Imagenet classification, we can find a model that achieves 25.7% top-1 error and 8.1% top-5 error by adapting the architecture in less than an hour from an 8 GPU days pretrained meta-network. By learning a good prior for NAS, our method dramatically decreases the required computation cost while achieving comparable performance to current state-of-the-art methods - even finding competitive models for unseen datasets with very quick adaptation. We believe our framework will open up new possibilities for efficient and massively scalable architecture search research across multiple tasks.\n",
      "----------- 33 ------------\n",
      "With the increasing popularity of machine learning techniques, it has become common to see prediction algorithms operating within some larger process. However, the criteria by which we train these algorithms often differ from the ultimate criteria on which we evaluate them. This paper proposes an end-to-end approach for learning probabilistic machine learning models in a manner that directly captures the ultimate task-based objective for which they will be used, within the context of stochastic programming. We present three experimental evaluations of the proposed approach: a classical inventory stock problem, a real-world electrical grid scheduling task, and a real-world energy storage arbitrage task. We show that the proposed approach can outperform both traditional modeling and purely black-box policy optimization approaches in these applications.\n",
      "----------- 34 ------------\n",
      "Online allocation problems have been widely studied due to their numerous practical applications (particularly to Internet advertising), as well as considerable theoretical interest. The main challenge in such problems is making assignment decisions in the face of uncertainty about future input; effective algorithms need to predict which constraints are most likely to bind, and learn the balance between short-term gain and the value of long-term resource availability. In many important applications, the algorithm designer is faced with multiple objectives to optimize. In particular, in online advertising it is fairly common to optimize multiple metrics, such as clicks, conversions, and impressions, as well as other metrics which may be largely uncorrelated such as ‘share of voice’, and ‘buyer surplus’. While there has been considerable work on multi-objective offline optimization (when the entire input is known in advance), very little is known about the online case, particularly in the case of adversarial input. In this paper, we give the first results for bi-objective online submodular optimization, providing almost matching upper and lower bounds for allocating items to agents with two submodular value functions. We also study practically relevant special cases of this problem related to Internet advertising, and obtain improved results. All our algorithms are nearly best possible, as well as being efficient and easy to implement in practice.\n",
      "----------- 35 ------------\n",
      "For sophisticated reinforcement learning (RL) systems to interact usefully with real-world environments, we need to communicate complex goals to these systems. In this work, we explore goals defined in terms of (non-expert) human preferences between pairs of trajectory segments. Our approach separates learning the goal from learning the behavior to achieve it. We show that this approach can effectively solve complex RL tasks without access to the reward function, including Atari games and simulated robot locomotion, while providing feedback on about 0.1% of our agent's interactions with the environment. This reduces the cost of human oversight far enough that it can be practically applied to state-of-the-art RL systems. To demonstrate the flexibility of our approach, we show that we can successfully train complex novel behaviors with about an hour of human time. These behaviors and environments are considerably more complex than any which have been previously learned from human feedback.\n",
      "----------- 36 ------------\n",
      "In black-box optimization, an agent repeatedly chooses a configuration to test, so as to find an optimal configuration. In many practical problems of interest, one would like to optimize several systems, or\n",
      "tasks'', simultaneously; however, in most of these scenarios the current task is determined by nature. In this work, we explore the\n",
      "offline'' case in which one is able to bypass nature and choose the next task to evaluate (e.g. via a simulator). Because some tasks may be easier to optimize and others may be more critical, it is crucial to leverage algorithms that not only consider which configurations to try next, but also which tasks to make evaluations for. In this work, we describe a theoretically grounded Bayesian optimization method to tackle this problem. We also demonstrate that if the model of the reward structure does a poor job of capturing variation in difficulty between tasks, then algorithms that actively pick tasks for evaluation may end up doing more harm than good. Following this, we show how our approach can be used for real world applications in science and engineering, including optimizing tokamak controls for nuclear fusion.\n",
      "----------- 37 ------------\n",
      "Knowledge tracing, where a machine models the knowledge of a student as they interact with coursework, is an established and significantly unsolved problem in computer supported education.In this paper we explore the benefit of using recurrent neural networks to model student learning.This family of models have important advantages over current state of the art methods in that they do not require the explicit encoding of human domain knowledge,and have a far more flexible functional form which can capture substantially more complex student interactions.We show that these neural networks outperform the current state of the art in prediction on real student data,while allowing straightforward interpretation and discovery of structure in the curriculum.These results suggest a promising new line of research for knowledge tracing.\n",
      "----------- 38 ------------\n",
      "Incremental life-long learning is a main challenge towards the long-standing goal of Artificial General Intelligence. In real-life settings, learning tasks arrive in a sequence and machine learning models must continually learn to increment already acquired knowledge. The existing incremental learning approaches fall well below the state-of-the-art cumulative models that use all training classes at once. In this paper, we propose a random path selection algorithm, called RPS-Net, that progressively chooses optimal paths for the new tasks while encouraging parameter sharing and reuse. Our approach avoids the overhead introduced by computationally expensive evolutionary and reinforcement learning based path selection strategies while achieving considerable performance gains. As an added novelty, the proposed model integrates knowledge distillation and retrospection along with the path selection strategy to overcome catastrophic forgetting. In order to maintain an equilibrium between previous and newly acquired knowledge, we propose a simple controller to dynamically balance the model plasticity. Through extensive experiments, we demonstrate that the proposed method surpasses the state-of-the-art performance on incremental learning and by utilizing parallel computation this method can run in constant time with nearly the same efficiency as a conventional deep convolutional neural network.\n",
      "----------- 39 ------------\n",
      "While recent developments in autonomous vehicle (AV) technology highlight substantial progress, we lack tools for rigorous and scalable testing. Real-world testing, the de facto evaluation environment, places the public in danger, and, due to the rare nature of accidents, will require billions of miles in order to statistically validate performance claims. We implement a simulation framework that can test an entire modern autonomous driving system, including, in particular, systems that employ deep-learning perception and control algorithms. Using adaptive importance-sampling methods to accelerate rare-event probability evaluation, we estimate the probability of an accident under a base distribution governing standard traffic behavior. We demonstrate our framework on a highway scenario, accelerating system evaluation by 2-20 times over naive Monte Carlo sampling methods and 10-300P times (where P is the number of processors) over real-world testing.\n",
      "----------- 40 ------------\n",
      "There is a growing interest in learning data representations that work well for many different types of problems and data. In this paper, we look in particular at the task of learning a single visual representation that can be successfully utilized in the analysis of very different types of images, from dog breeds to stop signs and digits. Inspired by recent work on learning networks that predict the parameters of another, we develop a tunable deep network architecture that, by means of adapter residual modules, can be steered on the fly to diverse visual domains. Our method achieves a high degree of parameter sharing while maintaining or even improving the accuracy of domain-specific representations. We also introduce the Visual Decathlon Challenge, a benchmark that evaluates the ability of representations to capture simultaneously ten very different visual domains and measures their ability to recognize well uniformly.\n",
      "----------- 41 ------------\n",
      "Intense recent discussions have focused on how to provide individuals with control over when their data can and cannot be used --- the EU’s Right To Be Forgotten regulation is an example of this effort. In this paper we initiate a framework studying what to do when it is no longer permissible to deploy models derivative from specific user data. In particular, we formulate the problem of efficiently deleting individual data points from trained machine learning models. For many standard ML models, the only way to completely remove an individual's data is to retrain the whole model from scratch on the remaining data, which is often not computationally practical. We investigate algorithmic principles that enable efficient data deletion in ML. For the specific setting of k\n",
      "-means clustering, we propose two provably deletion efficient algorithms which achieve an average of over 100\\times\n",
      "improvement in deletion efficiency across 6 datasets, while producing clusters of comparable statistical quality to a canonical k\n",
      "-means++ baseline.\n",
      "----------- 42 ------------\n",
      "Attempts to train a comprehensive artificial intelligence capable of solving multiple tasks have been impeded by a chronic problem called catastrophic forgetting. Although simply replaying all previous data alleviates the problem, it requires large memory and even worse, often infeasible in real world applications where the access to past data is limited. Inspired by the generative nature of the hippocampus as a short-term memory system in primate brain, we propose the Deep Generative Replay, a novel framework with a cooperative dual model architecture consisting of a deep generative model (“generator”) and a task solving model (“solver”). With only these two models, training data for previous tasks can easily be sampled and interleaved with those for a new task. We test our methods in several sequential learning settings involving image classification tasks.\n",
      "----------- 43 ------------\n",
      "The design of a reward function often poses a major practical challenge to real-world applications of reinforcement learning. Approaches such as inverse reinforcement learning attempt to overcome this challenge, but require expert demonstrations, which can be difficult or expensive to obtain in practice. We propose inverse event-based control, which generalizes inverse reinforcement learning methods to cases where full demonstrations are not needed, such as when only samples of desired goal states are available. Our method is grounded in an alternative perspective on control and reinforcement learning, where an agent's goal is to maximize the probability that one or more events will happen at some point in the future, rather than maximizing cumulative rewards. We demonstrate the effectiveness of our methods on continuous control tasks, with a focus on high-dimensional observations like images where rewards are hard or even impossible to specify.\n",
      "----------- 44 ------------\n",
      "An explosion of high-throughput DNA sequencing in the past decade has led to a surge of interest in population-scale inference with whole-genome data. Recent work in population genetics has centered on designing inference methods for relatively simple model classes, and few scalable general-purpose inference techniques exist for more realistic, complex models. To achieve this, two inferential challenges need to be addressed: (1) population data are exchangeable, calling for methods that efficiently exploit the symmetries of the data, and (2) computing likelihoods is intractable as it requires integrating over a set of correlated, extremely high-dimensional latent variables. These challenges are traditionally tackled by likelihood-free methods that use scientific simulators to generate datasets and reduce them to hand-designed, permutation-invariant summary statistics, often leading to inaccurate inference. In this work, we develop an exchangeable neural network that performs summary statistic-free, likelihood-free inference. Our framework can be applied in a black-box fashion across a variety of simulation-based tasks, both within and outside biology. We demonstrate the power of our approach on the recombination hotspot testing problem, outperforming the state-of-the-art.\n",
      "----------- 45 ------------\n",
      "Predicting the behavior of human participants in strategic settings is an important problem in many domains. Most existing work either assumes that participants are perfectly rational, or attempts to directly model each participant's cognitive processes based on insights from cognitive psychology and experimental economics. In this work, we present an alternative, a deep learning approach that automatically performs cognitive modeling without relying on such expert knowledge. We introduce a novel architecture that allows a single network to generalize across different input and output dimensions by using matrix units rather than scalar units, and show that its performance significantly outperforms that of the previous state of the art, which relies on expert-constructed features.\n",
      "----------- 46 ------------\n",
      "A key requirement in sequence to sequence processing is the modeling of long range dependencies. To this end, a vast majority of the state-of-the-art models use attention mechanism which is of O(n^2) complexity that leads to slow execution for long sequences. We introduce a new Shuffle-Exchange neural network model for sequence to sequence tasks which have O(log n) depth and O(n log n) total complexity. We show that this model is powerful enough to infer efficient algorithms for common algorithmic benchmarks including sorting, addition and multiplication. We evaluate our architecture on the challenging LAMBADA question answering dataset and compare it with the state-of-the-art models which use attention. Our model achieves competitive accuracy and scales to sequences with more than a hundred thousand of elements. We are confident that the proposed model has the potential for building more efficient architectures for processing large interrelated data in language modeling, music generation and other application domains.\n",
      "----------- 47 ------------\n",
      "The last decade has witnessed an explosion in the development of models, theory and computational algorithms for\n",
      "big data'' analysis. In particular, distributed inference has served as a natural and dominating paradigm for statistical inference. However, the existing literature on parallel inference almost exclusively focuses on Euclidean data and parameters. While this assumption is valid for many applications, it is increasingly more common to encounter problems where the data or the parameters lie on a non-Euclidean space, like a manifold for example. Our work aims to fill a critical gap in the literature by generalizing parallel inference algorithms to optimization on manifolds. We show that our proposed algorithm is both communication efficient and carries theoretical convergence guarantees. In addition, we demonstrate the performance of our algorithm to the estimation of Fr\\'echet means on simulated spherical data and the low-rank matrix completion problem over Grassmann manifolds applied to the Netflix prize data set.\n",
      "----------- 48 ------------\n",
      "Markov Decision Processes (MDPs), the mathematical framework underlying most algorithms in Reinforcement Learning (RL), are often used in a way that wrongfully assumes that the state of an agent's environment does not change during action selection. As RL systems based on MDPs begin to find application in real-world safety critical situations, this mismatch between the assumptions underlying classical MDPs and the reality of real-time computation may lead to undesirable outcomes. In this paper, we introduce a new framework, in which states and actions evolve simultaneously and show how it is related to the classical MDP formulation. We analyze existing algorithms under the new real-time formulation and show why they are suboptimal when used in real-time. We then use those insights to create a new algorithm Real-Time Actor Critic (RTAC) that outperforms the existing state-of-the-art continuous control algorithm Soft Actor Critic both in real-time and non-real-time settings.\n",
      "----------- 49 ------------\n",
      "Time series prediction problems are becoming increasingly high-dimensional in modern applications, such as climatology and demand forecasting. For example, in the latter problem, the number of items for which demand needs to be forecast might be as large as 50,000. In addition, the data is generally noisy and full of missing values. Thus, modern applications require methods that are highly scalable, and can deal with noisy data in terms of corruptions or missing values. However, classical time series methods usually fall short of handling these issues. In this paper, we present a temporal regularized matrix factorization (TRMF) framework which supports data-driven temporal learning and forecasting. We develop novel regularization schemes and use scalable matrix factorization methods that are eminently suited for high-dimensional time series data that has many missing values. Our proposed TRMF is highly general, and subsumes many existing approaches for time series analysis. We make interesting connections to graph regularization methods in the context of learning the dependencies in an autoregressive framework. Experimental results show the superiority of TRMF in terms of scalability and prediction quality. In particular, TRMF is two orders of magnitude faster than other methods on a problem of dimension 50,000, and generates better forecasts on real-world datasets such as Wal-mart E-commerce datasets.\n",
      "----------- 50 ------------\n",
      "Learning cooperative policies for multi-agent systems is often challenged by partial observability and a lack of coordination. In some settings, the structure of a problem allows a distributed solution with limited communication. Here, we consider a scenario where no communication is available, and instead we learn local policies for all agents that collectively mimic the solution to a centralized multi-agent static optimization problem. Our main contribution is an information theoretic framework based on rate distortion theory which facilitates analysis of how well the resulting fully decentralized policies are able to reconstruct the optimal solution. Moreover, this framework provides a natural extension that addresses which nodes an agent should communicate with to improve the performance of its individual policy.\n",
      "----------- 51 ------------\n",
      "Despite the recent progress in hyperparameter optimization (HPO), available benchmarks that resemble real-world scenarios consist of a few and very large problem instances that are expensive to solve. This blocks researchers and practitioners no only from systematically running large-scale comparisons that are needed to draw statistically significant results but also from reproducing experiments that were conducted before. This work proposes a method to alleviate these issues by means of a meta-surrogate model for HPO tasks trained on off-line generated data. The model combines a probabilistic encoder with a multi-task model such that it can generate inexpensive and realistic tasks of the class of problems of interest. We demonstrate that benchmarking HPO methods on samples of the generative model allows us to draw more coherent and statistically significant conclusions that can be reached orders of magnitude faster than using the original tasks. We provide evidence of our findings for various HPO methods on a wide class of problems.\n",
      "----------- 52 ------------\n",
      "Decentralized machine learning is a promising emerging paradigm in view of global challenges of data ownership and privacy. We consider learning of linear classification and regression models, in the setting where the training data is decentralized over many user devices, and the learning algorithm must run on-device, on an arbitrary communication network, without a central coordinator. We propose COLA, a new decentralized training algorithm with strong theoretical guarantees and superior practical performance. Our framework overcomes many limitations of existing methods, and achieves communication efficiency, scalability, elasticity as well as resilience to changes in data and allows for unreliable and heterogeneous participating devices.\n",
      "----------- 53 ------------\n",
      "This paper deals with price optimization, which is to find the best pricing strategy that maximizes revenue or profit, on the basis of demand forecasting models. Though recent advances in regression technologies have made it possible to reveal price-demand relationship of a number of multiple products, most existing price optimization methods, such as mixed integer programming formulation, cannot handle tens or hundreds of products because of their high computational costs. To cope with this problem, this paper proposes a novel approach based on network flow algorithms. We reveal a connection between supermodularity of the revenue and cross elasticity of demand. On the basis of this connection, we propose an efficient algorithm that employs network flow algorithms. The proposed algorithm can handle hundreds or thousands of products, and returns an exact optimal solution under an assumption regarding cross elasticity of demand. Even in case in which the assumption does not hold, the proposed algorithm can efficiently find approximate solutions as good as can other state-of-the-art methods, as empirical results show.\n",
      "----------- 54 ------------\n",
      "Engineered proteins offer the potential to solve many problems in biomedicine, energy, and materials science, but creating designs that succeed is difficult in practice. A significant aspect of this challenge is the complex coupling between protein sequence and 3D structure, with the task of finding a viable design often referred to as the inverse protein folding problem. We develop relational language models for protein sequences that directly condition on a graph specification of the target structure. Our approach efficiently captures the complex dependencies in proteins by focusing on those that are long-range in sequence but local in 3D space. Our framework significantly improves in both speed and robustness over conventional and deep-learning-based methods for structure-based protein sequence design, and takes a step toward rapid and targeted biomolecular design with the aid of deep generative models.\n",
      "----------- 55 ------------\n",
      "Storing data in synthetic DNA offers the possibility of improving information density and durability by several orders of magnitude compared to current storage technologies. However, DNA data storage requires a computationally intensive process to retrieve the data. In particular, a crucial step in the data retrieval pipeline involves clustering billions of strings with respect to edit distance. Datasets in this domain have many notable properties, such as containing a very large number of small clusters that are well-separated in the edit distance metric space. In this regime, existing algorithms are unsuitable because of either their long running time or low accuracy. To address this issue, we present a novel distributed algorithm for approximately computing the underlying clusters. Our algorithm converges efficiently on any dataset that satisfies certain separability properties, such as those coming from DNA data storage systems. We also prove that, under these assumptions, our algorithm is robust to outliers and high levels of noise. We provide empirical justification of the accuracy, scalability, and convergence of our algorithm on real and synthetic data. Compared to the state-of-the-art algorithm for clustering DNA sequences, our algorithm simultaneously achieves higher accuracy and a 1000x speedup on three real datasets.\n",
      "----------- 56 ------------\n",
      "Deep networks are commonly used to model dynamical systems, predicting how the state of a system will evolve over time (either autonomously or in response to control inputs). Despite the predictive power of these systems, it has been difficult to make formal claims about the basic properties of the learned systems. In this paper, we propose an approach for learning dynamical systems that are guaranteed to be stable over the entire state space. The approach works by jointly learning a dynamics model and Lyapunov function that guarantees non-expansiveness of the dynamics under the learned Lyapunov function. We show that such learning systems are able to model simple dynamical systems and can be combined with additional deep generative models to learn complex dynamics, such as video textures, in a fully end-to-end fashion.\n",
      "----------- 57 ------------\n",
      "The state-of-the-art hardware platforms for training deep neural networks are moving from traditional single precision (32-bit) computations towards 16 bits of precision - in large part due to the high energy efficiency and smaller bit storage associated with using reduced-precision representations. However, unlike inference, training with numbers represented with less than 16 bits has been challenging due to the need to maintain fidelity of the gradient computations during back-propagation. Here we demonstrate, for the first time, the successful training of deep neural networks using 8-bit floating point numbers while fully maintaining the accuracy on a spectrum of deep learning models and datasets. In addition to reducing the data and computation precision to 8 bits, we also successfully reduce the arithmetic precision for additions (used in partial product accumulation and weight updates) from 32 bits to 16 bits through the introduction of a number of key ideas including chunk-based accumulation and floating point stochastic rounding. The use of these novel techniques lays the foundation for a new generation of hardware training platforms with the potential for 2-4 times improved throughput over today's systems.\n",
      "----------- 58 ------------\n",
      "Potential based reward shaping is a powerful technique for accelerating convergence of reinforcement learning algorithms. Typically, such information includes an estimate of the optimal value function and is often provided by a human expert or other sources of domain knowledge. However, this information is often biased or inaccurate and can mislead many reinforcement learning algorithms. In this paper, we apply Bayesian Model Combination with multiple experts in a way that learns to trust a good combination of experts as training progresses. This approach is both computationally efficient and general, and is shown numerically to improve convergence across discrete and continuous domains and different reinforcement learning algorithms.\n",
      "----------- 59 ------------\n",
      "Smart portable applications increasingly rely on edge computing due to privacy and latency concerns. But guaranteeing always-on functionality comes with two major challenges: heavily resource-constrained hardware; and dynamic application conditions. Probabilistic models present an ideal solution to these challenges: they are robust to missing data, allow for joint predictions and have small data needs. In addition, ongoing efforts in field of tractable learning have resulted in probabilistic models with strict inference efficiency guarantees. However, the current notions of tractability are often limited to model complexity, disregarding the hardware's specifications and constraints. We propose a novel resource-aware cost metric that takes into consideration the hardware's properties in determining whether the inference task can be efficiently deployed. We use this metric to evaluate the performance versus resource trade-off relevant to the application of interest, and we propose a strategy that selects the device-settings that can optimally meet users' requirements. We showcase our framework on a mobile activity recognition scenario, and on a variety of benchmark datasets representative of the field of tractable learning and of the applications of interest.\n",
      "----------- 60 ------------\n",
      "Distributed implementations of mini-batch stochastic gradient descent (SGD) suffer from communication overheads, attributed to the high frequency of gradient updates inherent in small-batch training. Training with large batches can reduce these overheads; however it besets the convergence of the algorithm and the generalization performance. In this work, we take a first step towards analyzing how the structure (width and depth) of a neural network affects the performance of large-batch training. We present new theoretical results which suggest that--for a fixed number of parameters--wider networks are more amenable to fast large-batch training compared to deeper ones. We provide extensive experiments on residual and fully-connected neural networks which suggest that wider networks can be trained using larger batches without incurring a convergence slow-down, unlike their deeper variants.\n",
      "----------- 61 ------------\n",
      "We introduce algorithmic assurance, the problem of testing whether machine learning algorithms are conforming to their intended design goal. We address this problem by proposing an efficient framework for algorithmic testing. To provide assurance, we need to efficiently discover scenarios where an algorithm decision deviates maximally from its intended gold standard. We mathematically formulate this task as an optimisation problem of an expensive, black-box function. We use an active learning approach based on Bayesian optimisation to solve this optimisation problem. We extend this framework to algorithms with vector-valued outputs by making appropriate modification in Bayesian optimisation via the EXP3 algorithm. We theoretically analyse our methods for convergence. Using two real-world applications, we demonstrate the efficiency of our methods. The significance of our problem formulation and initial solutions is that it will serve as the foundation in assuring humans about machines making complex decisions.\n",
      "----------- 62 ------------\n",
      "Forecasting high-dimensional time series plays a crucial role in many applications such as demand forecasting and financial predictions. Modern datasets can have millions of correlated time-series that evolve together, i.e they are extremely high dimensional (one dimension for each individual time-series). There is a need for exploiting global patterns and coupling them with local calibration for better prediction. However, most recent deep learning approaches in the literature are one-dimensional, i.e, even though they are trained on the whole dataset, during prediction, the future forecast for a single dimension mainly depends on past values from the same dimension. In this paper, we seek to correct this deficiency and propose DeepGLO, a deep forecasting model which thinks globally and acts locally. In particular, DeepGLO is a hybrid model that combines a global matrix factorization model regularized by a temporal convolution network, along with another temporal network that can capture local properties of each time-series and associated covariates. Our model can be trained effectively on high-dimensional but diverse time series, where different time series can have vastly different scales, without a priori normalization or rescaling. Empirical results demonstrate that DeepGLO can outperform state-of-the-art approaches; for example, we see more than 25% improvement in WAPE over other methods on a public dataset that contains more than 100K-dimensional time series.\n",
      "----------- 63 ------------\n",
      "In this work we study the problem of using machine-learned predictions to improve performance of online algorithms. We consider two classical problems, ski rental and non-clairvoyant job scheduling, and obtain new online algorithms that use predictions to make their decisions. These algorithms are oblivious to the performance of the predictor, improve with better predictions, but do not degrade much if the predictions are poor.\n",
      "----------- 64 ------------\n",
      "Currently, deep neural networks are deployed on low-power portable devices by first training a full-precision model using powerful hardware, and then deriving a corresponding low-precision model for efficient inference on such systems. However, training models directly with coarsely quantized weights is a key step towards learning on embedded platforms that have limited computing resources, memory capacity, and power consumption. Numerous recent publications have studied methods for training quantized networks, but these studies have mostly been empirical. In this work, we investigate training methods for quantized neural networks from a theoretical viewpoint. We first explore accuracy guarantees for training methods under convexity assumptions. We then look at the behavior of these algorithms for non-convex problems, and show that training algorithms that exploit high-precision representations have an important greedy search phase that purely quantized training methods lack, which explains the difficulty of training using low-precision arithmetic.\n",
      "----------- 65 ------------\n",
      "Inverse optimization is a powerful paradigm for learning preferences and restrictions that explain the behavior of a decision maker, based on a set of external signal and the corresponding decision pairs. However, most inverse optimization algorithms are designed specifically in batch setting, where all the data is available in advance. As a consequence, there has been rare use of these methods in an online setting suitable for real-time applications. In this paper, we propose a general framework for inverse optimization through online learning. Specifically, we develop an online learning algorithm that uses an implicit update rule which can handle noisy data. Moreover, under additional regularity assumptions in terms of the data and the model, we prove that our algorithm converges at a rate of \\mathcal{O}(1/\\sqrt{T})\n",
      "and is statistically consistent. In our experiments, we show the online learning approach can learn the parameters with great accuracy and is very robust to noises, and achieves a dramatic improvement in computational efficacy over the batch learning approach.\n",
      "----------- 66 ------------\n",
      "Recent learning-to-plan methods have shown promising results on planning directly from observation space. Yet, their ability to plan for long-horizon tasks is limited by the accuracy of the prediction model. On the other hand, classical symbolic planners show remarkable capabilities in solving long-horizon tasks, but they require predefined symbolic rules and symbolic states, restricting their real-world applicability. In this work, we combine the benefits of these two paradigms and propose a learning-to-plan method that can directly generate a long-term symbolic plan conditioned on high-dimensional observations. We borrow the idea of regression (backward) planning from classical planning literature and introduce Regression Planning Networks (RPN), a neural network architecture that plans backward starting at a task goal and generates a sequence of intermediate goals that reaches the current observation. We show that our model not only inherits many favorable traits from symbolic planning --including the ability to solve previously unseen tasks-- but also can learn from visual inputs in an end-to-end manner. We evaluate the capabilities of RPN in a grid world environment and a simulated 3D kitchen environment featuring complex visual scenes and long task horizon, and show that it achieves near-optimal performance in completely new task instances.\n",
      "----------- 67 ------------\n",
      "We propose a generic algorithmic building block to accelerate training of machine learning models on heterogeneous compute systems. Our scheme allows to efficiently employ compute accelerators such as GPUs and FPGAs for the training of large-scale machine learning models, when the training data exceeds their memory capacity. Also, it provides adaptivity to any system's memory hierarchy in terms of size and processing speed. Our technique is built upon novel theoretical insights regarding primal-dual coordinate methods, and uses duality gap information to dynamically decide which part of the data should be made available for fast processing. To illustrate the power of our approach we demonstrate its performance for training of generalized linear models on a large-scale dataset exceeding the memory size of a modern GPU, showing an order-of-magnitude speedup over existing approaches.\n",
      "----------- 68 ------------\n",
      "In many machine learning applications, there are multiple decision-makers involved, both automated and human. The interaction between these agents often goes unaddressed in algorithmic development. In this work, we explore a simple version of this interaction with a two-stage framework containing an automated model and an external decision-maker. The model can choose to say PASS, and pass the decision downstream, as explored in rejection learning. We extend this concept by proposing \"learning to defer\", which generalizes rejection learning by considering the effect of other agents in the decision-making process. We propose a learning algorithm which accounts for potential biases held by external decision-makers in a system. Experiments demonstrate that learning to defer can make systems not only more accurate but also less biased. Even when working with inconsistent or biased users, we show that deferring models still greatly improve the accuracy and/or fairness of the entire system.\n",
      "----------- 69 ------------\n",
      "Despite impressive recent advances in reinforcement learning (RL), its deployment in real-world physical systems is often complicated by unexpected events, limited data, and the potential for expensive failures. In this paper, we describe an application of RL “in the wild” to the task of regulating temperatures and airflow inside a large-scale data center (DC). Adopting a data-driven, model-based approach, we demonstrate that an RL agent with little prior knowledge is able to effectively and safely regulate conditions on a server floor after just a few hours of exploration, while improving operational efficiency relative to existing PID controllers.\n",
      "----------- 70 ------------\n",
      "In many machine learning applications, one needs to interactively select a sequence of items (e.g., recommending movies based on a user's feedback) or make sequential decisions in a certain order (e.g., guiding an agent through a series of states). Not only do sequences already pose a dauntingly large search space, but we must also take into account past observations, as well as the uncertainty of future outcomes. Without further structure, finding an optimal sequence is notoriously challenging, if not completely intractable. In this paper, we view the problem of adaptive and sequential decision making through the lens of submodularity and propose an adaptive greedy policy with strong theoretical guarantees. Additionally, to demonstrate the practical utility of our results, we run experiments on Amazon product recommendation and Wikipedia link prediction tasks.\n",
      "----------- 71 ------------\n",
      "In sequential decision making, it is often important and useful for end users to understand the underlying patterns or causes that lead to the corresponding decisions. However, typical deep reinforcement learning algorithms seldom provide such information due to their black-box nature. In this paper, we present a probabilistic model, Q-LDA, to uncover latent patterns in text-based sequential decision processes. The model can be understood as a variant of latent topic models that are tailored to maximize total rewards; we further draw an interesting connection between an approximate maximum-likelihood estimation of Q-LDA and the celebrated Q-learning algorithm. We demonstrate in the text-game domain that our proposed method not only provides a viable mechanism to uncover latent patterns in decision processes, but also obtains state-of-the-art rewards in these games.\n",
      "----------- 72 ------------\n",
      "The collection and analysis of telemetry data from user's devices is routinely performed by many software companies. Telemetry collection leads to improved user experience but poses significant risks to users' privacy. Locally differentially private (LDP) algorithms have recently emerged as the main tool that allows data collectors to estimate various population statistics, while preserving privacy. The guarantees provided by such algorithms are typically very strong for a single round of telemetry collection, but degrade rapidly when telemetry is collected regularly. In particular, existing LDP algorithms are not suitable for repeated collection of counter data such as daily app usage statistics. In this paper, we develop new LDP mechanisms geared towards repeated collection of counter data, with formal privacy guarantees even after being executed for an arbitrarily long period of time. For two basic analytical tasks, mean estimation and histogram estimation, our LDP mechanisms for repeated data collection provide estimates with comparable or even the same accuracy as existing single-round LDP collection mechanisms. We conduct empirical evaluation on real-world counter datasets to verify our theoretical results. Our mechanisms have been deployed by Microsoft to collect telemetry across millions of devices.\n",
      "----------- 73 ------------\n",
      "Bayesian optimization is a prominent method for optimizing expensive to evaluate black-box functions that is prominently applied to tuning the hyperparameters of machine learning algorithms. Despite its successes, the prototypical Bayesian optimization approach - using Gaussian process models - does not scale well to either many hyperparameters or many function evaluations. Attacking this lack of scalability and flexibility is thus one of the key challenges of the field. We present a general approach for using flexible parametric models (neural networks) for Bayesian optimization, staying as close to a truly Bayesian treatment as possible. We obtain scalability through stochastic gradient Hamiltonian Monte Carlo, whose robustness we improve via a scale adaptation. Experiments including multi-task Bayesian optimization with 21 tasks, parallel optimization of deep neural networks and deep reinforcement learning show the power and flexibility of this approach.\n",
      "----------- 74 ------------\n",
      "Bayesian optimization has recently emerged as a popular method for the sample-efficient optimization of expensive black-box functions. However, the application to high-dimensional problems with several thousand observations remains challenging, and on difficult problems Bayesian optimization is often not competitive with other paradigms. In this paper we take the view that this is due to the implicit homogeneity of the global probabilistic models and an overemphasized exploration that results from global acquisition. This motivates the design of a local probabilistic approach for global optimization of large-scale high-dimensional problems. We propose the TuRBO algorithm that fits a collection of local models and performs a principled global allocation of samples across these models via an implicit bandit approach. A comprehensive evaluation demonstrates that TuRBO outperforms state-of-the-art methods from machine learning and operations research on problems spanning reinforcement learning, robotics, and the natural sciences.\n",
      "----------- 75 ------------\n",
      "Predicting future video frames is extremely challenging, as there are many factors of variation that make up the dynamics of how frames change through time. Previously proposed solutions require complex inductive biases inside network architectures with highly specialized computation, including segmentation masks, optical flow, and foreground and background separation. In this work, we question if such handcrafted architectures are necessary and instead propose a different approach: finding minimal inductive bias for video prediction while maximizing network capacity. We investigate this question by performing the first large-scale empirical study and demonstrate state-of-the-art performance by learning large models on three different datasets: one for modeling object interactions, one for modeling human motion, and one for modeling car driving.\n",
      "----------- 76 ------------\n",
      "Polynomial inequalities lie at the heart of many mathematical disciplines. In this paper, we consider the fundamental computational task of automatically searching for proofs of polynomial inequalities. We adopt the framework of semi-algebraic proof systems that manipulate polynomial inequalities via elementary inference rules that infer new inequalities from the premises. These proof systems are known to be very powerful, but searching for proofs remains a major difficulty. In this work, we introduce a machine learning based method to search for a dynamic proof within these proof systems. We propose a deep reinforcement learning framework that learns an embedding of the polynomials and guides the choice of inference rules, taking the inherent symmetries of the problem as an inductive bias. We compare our approach with powerful and widely-studied linear programming hierarchies based on static proof systems, and show that our method reduces the size of the linear program by several orders of magnitude while also improving performance. These results hence pave the way towards augmenting powerful and well-studied semi-algebraic proof systems with machine learning guiding strategies for enhancing the expressivity of such proof systems.\n",
      "----------- 77 ------------\n",
      "Motivated by the success of reinforcement learning (RL) for discrete-time tasks such as AlphaGo and Atari games, there has been a recent surge of interest in using RL for continuous-time control of physical systems (cf. many challenging tasks in OpenAI Gym and DeepMind Control Suite). Since discretization of time is susceptible to error, it is methodologically more desirable to handle the system dynamics directly in continuous time. However, very few techniques exist for continuous-time RL and they lack flexibility in value function approximation. In this paper, we propose a novel framework for model-based continuous-time value function approximation in reproducing kernel Hilbert spaces. The resulting framework is so flexible that it can accommodate any kind of kernel-based approach, such as Gaussian processes and kernel adaptive filters, and it allows us to handle uncertainties and nonstationarity without prior knowledge about the environment or what basis functions to employ. We demonstrate the validity of the presented framework through experiments.\n",
      "----------- 78 ------------\n",
      "Learning meaningful and compact representations with disentangled semantic aspects is considered to be of key importance in representation learning. Since real-world data is notoriously costly to collect, many recent state-of-the-art disentanglement models have heavily relied on synthetic toy data-sets. In this paper, we propose a novel data-set which consists of over 1 million images of physical 3D objects with seven factors of variation, such as object color, shape, size and position. In order to be able to control all the factors of variation precisely, we built an experimental platform where the objects are being moved by a robotic arm. In addition, we provide two more datasets which consist of simulations of the experimental setup. These datasets provide for the first time the possibility to systematically investigate how well different disentanglement methods perform on real data in comparison to simulation, and how simulated data can be leveraged to build better representations of the real world. We provide a first experimental study of these questions and our results indicate that learned models transfer poorly, but that model and hyperparameter selection is an effective means of transferring information to the real world.\n",
      "----------- 79 ------------\n",
      "Expected improvement and other acquisition functions widely used in Bayesian optimization use a \"one-step\" assumption: they value objective function evaluations assuming no future evaluations will be performed. Because we usually evaluate over multiple steps, this assumption may leave substantial room for improvement. Existing theory gives acquisition functions looking multiple steps in the future but calculating them requires solving a high-dimensional continuous-state continuous-action Markov decision process (MDP). Fast exact solutions of this MDP remain out of reach of today's methods. As a result, previous two- and multi-step lookahead Bayesian optimization algorithms are either too expensive to implement in most practical settings or resort to heuristics that may fail to fully realize the promise of two-step lookahead. This paper proposes a computationally efficient algorithm that provides an accurate solution to the two-step lookahead Bayesian optimization problem in seconds to at most several minutes of computation per batch of evaluations. The resulting acquisition function provides increased query efficiency and robustness compared with previous two- and multi-step lookahead methods in both single-threaded and batch experiments. This unlocks the value of two-step lookahead in practice. We demonstrate the value of our algorithm with extensive experiments on synthetic test functions and real-world problems.\n",
      "----------- 80 ------------\n",
      "Computationally intensive distributed and parallel computing is often bottlenecked by a small set of slow workers known as stragglers. In this paper, we utilize the emerging idea of\n",
      "coded computation'' to design a novel error-correcting-code inspired technique for solving linear inverse problems under specific iterative methods in a parallelized implementation affected by stragglers. Example machine-learning applications include inverse problems such as personalized PageRank and sampling on graphs. We provably show that our coded-computation technique can reduce the mean-squared error under a computational deadline constraint. In fact, the ratio of mean-squared error of replication-based and coded techniques diverges to infinity as the deadline increases. Our experiments for personalized PageRank performed on real systems and real social networks show that this ratio can be as large as 10^4\n",
      ". Further, unlike coded-computation techniques proposed thus far, our strategy combines outputs of all workers, including the stragglers, to produce more accurate estimates at the computational deadline. This also ensures that the accuracy degrades\n",
      "gracefully'' in the event that the number of stragglers is large.\n",
      "----------- 81 ------------\n",
      "Many practical perception systems exist within larger processes which often include interactions with users or additional components that are capable of evaluating the quality of predicted solutions. In these contexts, it is beneficial to provide these oracle mechanisms with multiple highly likely hypotheses rather than a single prediction. In this work, we pose the task of producing multiple outputs as a learning problem over an ensemble of deep networks -- introducing a novel stochastic gradient descent based approach to minimize the loss with respect to an oracle. Our method is simple to implement, agnostic to both architecture and loss function, and parameter-free. Our approach achieves lower oracle error compared to existing methods on a wide range of tasks and deep architectures. We also show qualitatively that solutions produced from our approach often provide interpretable representations of task ambiguity.\n",
      "----------- 82 ------------\n",
      "As deep learning applications continue to become more diverse, an interesting question arises: Can general problem solving arise from jointly learning several such diverse tasks? To approach this question, deep multi-task learning is extended in this paper to the setting where there is no obvious overlap between task architectures. The idea is that any set of (architecture,task) pairs can be decomposed into a set of potentially related subproblems, whose sharing is optimized by an efficient stochastic algorithm. The approach is first validated in a classic synthetic multi-task learning benchmark, and then applied to sharing across disparate architectures for vision, NLP, and genomics tasks. It discovers regularities across these domains, encodes them into sharable modules, and combines these modules systematically to improve performance in the individual tasks. The results confirm that sharing learned functionality across diverse domains and architectures is indeed beneficial, thus establishing a key ingredient for general problem solving in the future.\n",
      "----------- 83 ------------\n",
      "Deep learning has become increasingly popular in both supervised and unsupervised machine learning thanks to its outstanding empirical performance. However, because of their intrinsic complexity, most deep learning methods are largely treated as black box tools with little interpretability. Even though recent attempts have been made to facilitate the interpretability of deep neural networks (DNNs), existing methods are susceptible to noise and lack of robustness. Therefore, scientists are justifiably cautious about the reproducibility of the discoveries, which is often related to the interpretability of the underlying statistical models. In this paper, we describe a method to increase the interpretability and reproducibility of DNNs by incorporating the idea of feature selection with controlled error rate. By designing a new DNN architecture and integrating it with the recently proposed knockoffs framework, we perform feature selection with a controlled error rate, while maintaining high power. This new method, DeepPINK (Deep feature selection using Paired-Input Nonlinear Knockoffs), is applied to both simulated and real data sets to demonstrate its empirical utility.\n",
      "----------- 84 ------------\n",
      "In a number of disciplines, the data (e.g., graphs, manifolds) to be analyzed are non-Euclidean in nature. Geometric deep learning corresponds to techniques that generalize deep neural network models to such non-Euclidean spaces. Several recent papers have shown how convolutional neural networks (CNNs) can be extended to learn with graph-based data. In this work, we study the setting where the data (or measurements) are ordered, longitudinal or temporal in nature and live on a Riemannian manifold -- this setting is common in a variety of problems in statistical machine learning, vision and medical imaging. We show how recurrent statistical recurrent network models can be defined in such spaces. We give an efficient algorithm and conduct a rigorous analysis of its statistical properties. We perform extensive numerical experiments demonstrating competitive performance with state of the art methods but with significantly less number of parameters. We also show applications to a statistical analysis task in brain imaging, a regime where deep neural network models have only been utilized in limited ways.\n",
      "----------- 85 ------------\n",
      "Goal-oriented reinforcement learning has recently been a practical framework for robotic manipulation tasks, in which an agent is required to reach a certain goal defined by a function on the state space. However, the sparsity of such reward definition makes traditional reinforcement learning algorithms very inefficient. Hindsight Experience Replay (HER), a recent advance, has greatly improved sample efficiency and practical applicability for such problems. It exploits previous replays by constructing imaginary goals in a simple heuristic way, acting like an implicit curriculum to alleviate the challenge of sparse reward signal. In this paper, we introduce Hindsight Goal Generation (HGG), a novel algorithmic framework that generates valuable hindsight goals which are easy for an agent to achieve in the short term and are also potential for guiding the agent to reach the actual goal in the long term. We have extensively evaluated our goal generation algorithm on a number of robotic manipulation tasks and demonstrated substantially improvement over the original HER in terms of sample efficiency.\n",
      "----------- 86 ------------\n",
      "Faced with saturation of Moore's law and increasing size and dimension of data, system designers have increasingly resorted to parallel and distributed computing to reduce computation time of machine-learning algorithms. However, distributed computing is often bottle necked by a small fraction of slow processors called \"stragglers\" that reduce the speed of computation because the fusion node has to wait for all processors to complete their processing. To combat the effect of stragglers, recent literature proposes introducing redundancy in computations across processors, e.g., using repetition-based strategies or erasure codes. The fusion node can exploit this redundancy by completing the computation using outputs from only a subset of the processors, ignoring the stragglers. In this paper, we propose a novel technique - that we call \"Short-Dot\" - to introduce redundant computations in a coding theory inspired fashion, for computing linear transforms of long vectors. Instead of computing long dot products as required in the original linear transform, we construct a larger number of redundant and short dot products that can be computed more efficiently at individual processors. Further, only a subset of these short dot products are required at the fusion node to finish the computation successfully. We demonstrate through probabilistic analysis as well as experiments on computing clusters that Short-Dot offers significant speed-up compared to existing techniques. We also derive trade-offs between the length of the dot-products and the resilience to stragglers (number of processors required to finish), for any such strategy and compare it to that achieved by our strategy.\n",
      "----------- 87 ------------\n",
      "Designing rewards for Reinforcement Learning (RL) is challenging because it needs to convey the desired task, be efficient to optimize, and be easy to compute. The latter is particularly problematic when applying RL to robotics, where detecting whether the desired configuration is reached might require considerable supervision and instrumentation. Furthermore, we are often interested in being able to reach a wide range of configurations, hence setting up a different reward every time might be unpractical. Methods like Hindsight Experience Replay (HER) have recently shown promise to learn policies able to reach many goals, without the need of a reward. Unfortunately, without tricks like resetting to points along the trajectory, HER might require many samples to discover how to reach certain areas of the state-space. In this work we propose a novel algorithm goalGAIL, which incorporates demonstrations to drastically speed up the convergence to a policy able to reach any goal, surpassing the performance of an agent trained with other Imitation Learning algorithms. Furthermore, we show our method can also be used when the available expert trajectories do not contain the actions or when the expert is suboptimal, which makes it applicable when only kinesthetic, third person or noisy demonstration is available.\n",
      "----------- 88 ------------\n",
      "We consider the problem of multiple agents sensing and acting in environments with the goal of maximising their shared utility. In these environments, agents must learn communication protocols in order to share information that is needed to solve the tasks. By embracing deep neural networks, we are able to demonstrate end-to-end learning of protocols in complex environments inspired by communication riddles and multi-agent computer vision problems with partial observability. We propose two approaches for learning in these domains: Reinforced Inter-Agent Learning (RIAL) and Differentiable Inter-Agent Learning (DIAL). The former uses deep Q-learning, while the latter exploits the fact that, during learning, agents can backpropagate error derivatives through (noisy) communication channels. Hence, this approach uses centralised learning but decentralised execution. Our experiments introduce new environments for studying the learning of communication protocols and present a set of engineering innovations that are essential for success in these domains.\n",
      "----------- 89 ------------\n",
      "Federated learning poses new statistical and systems challenges in training machine learning models over distributed networks of devices. In this work, we show that multi-task learning is naturally suited to handle the statistical challenges of this setting, and propose a novel systems-aware optimization method, MOCHA, that is robust to practical systems issues. Our method and theory for the first time consider issues of high communication cost, stragglers, and fault tolerance for distributed multi-task learning. The resulting method achieves significant speedups compared to alternatives in the federated setting, as we demonstrate through simulations on real-world federated datasets.\n",
      "----------- 90 ------------\n",
      "Hierarchical reinforcement learning (HRL) is a promising approach to extend traditional reinforcement learning (RL) methods to solve more complex tasks. Yet, the majority of current HRL methods require careful task-specific design and on-policy training, making them difficult to apply in real-world scenarios. In this paper, we study how we can develop HRL algorithms that are general, in that they do not make onerous additional assumptions beyond standard RL algorithms, and efficient, in the sense that they can be used with modest numbers of interaction samples, making them suitable for real-world problems such as robotic control. For generality, we develop a scheme where lower-level controllers are supervised with goals that are learned and proposed automatically by the higher-level controllers. To address efficiency, we propose to use off-policy experience for both higher- and lower-level training. This poses a considerable challenge, since changes to the lower-level behaviors change the action space for the higher-level policy, and we introduce an off-policy correction to remedy this challenge. This allows us to take advantage of recent advances in off-policy model-free RL to learn both higher and lower-level policies using substantially fewer environment interactions than on-policy algorithms. We find that our resulting HRL agent is generally applicable and highly sample-efficient. Our experiments show that our method can be used to learn highly complex behaviors for simulated robots, such as pushing objects and utilizing them to reach target locations, learning from only a few million samples, equivalent to a few days of real-time interaction. In comparisons with a number of prior HRL methods, we find that our approach substantially outperforms previous state-of-the-art techniques.\n",
      "----------- 91 ------------\n",
      "Policy gradient methods are among the best Reinforcement Learning (RL) techniques to solve complex control problems. In real-world RL applications, it is common to have a good initial policy whose performance needs to be improved and it may not be acceptable to try bad policies during the learning process. Although several methods for choosing the step size exist, research paid less attention to determine the batch size, that is the number of samples used to estimate the gradient direction for each update of the policy parameters. In this paper, we propose a set of methods to jointly optimize the step and the batch sizes that guarantee (with high probability) to improve the policy performance after each update. Besides providing theoretical guarantees, we show numerical simulations to analyse the behaviour of our methods.\n",
      "----------- 92 ------------\n",
      "Stochastic convex optimization algorithms are the most popular way to train machine learning models on large-scale data. Scaling up the training process of these models is crucial, but the most popular algorithm, Stochastic Gradient Descent (SGD), is a serial method that is surprisingly hard to parallelize. In this paper, we propose an efficient distributed stochastic optimization method by combining adaptivity with variance reduction techniques. Our analysis yields a linear speedup in the number of machines, constant memory footprint, and only a logarithmic number of communication rounds. Critically, our approach is a black-box reduction that parallelizes any serial online learning algorithm, streamlining prior analysis and allowing us to leverage the significant progress that has been made in designing adaptive algorithms. In particular, we achieve optimal convergence rates without any prior knowledge of smoothness parameters, yielding a more robust algorithm that reduces the need for hyperparameter tuning. We implement our algorithm in the Spark distributed framework and exhibit dramatic performance gains on large-scale logistic regression problems.\n",
      "----------- 93 ------------\n",
      "Causal inference is central to many areas of artificial intelligence, including complex reasoning, planning, knowledge-base construction, robotics, explanation, and fairness. An active community of researchers develops and enhances algorithms that learn causal models from data, and this work has produced a series of impressive technical advances. However, evaluation techniques for causal modeling algorithms have remained somewhat primitive, limiting what we can learn from experimental studies of algorithm performance, constraining the types of algorithms and model representations that researchers consider, and creating a gap between theory and practice. We argue for more frequent use of evaluation techniques that examine interventional measures rather than structural or observational measures, and that evaluate those measures on empirical data rather than synthetic data. We survey the current practice in evaluation and show that the techniques we recommend are rarely used in practice. We show that such techniques are feasible and that data sets are available to conduct such evaluations. We also show that these techniques produce substantially different results than using structural measures and synthetic data.\n",
      "----------- 94 ------------\n",
      "The change-point detection problem seeks to identify distributional changes at an unknown change-point k* in a stream of data. This problem appears in many important practical settings involving personal data, including biosurveillance, fault detection, finance, signal detection, and security systems. The field of differential privacy offers data analysis tools that provide powerful worst-case privacy guarantees. We study the statistical problem of change-point problem through the lens of differential privacy. We give private algorithms for both online and offline change-point detection, analyze these algorithms theoretically, and then provide empirical validation of these results.\n",
      "----------- 95 ------------\n",
      "Learning in the space-time domain remains a very challenging problem in machine learning and computer vision. Current computational models for understanding spatio-temporal visual data are heavily rooted in the classical single-image based paradigm. It is not yet well understood how to integrate information in space and time into a single, general model. We propose a neural graph model, recurrent in space and time, suitable for capturing both the local appearance and the complex higher-level interactions of different entities and objects within the changing world scene. Nodes and edges in our graph have dedicated neural networks for processing information. Nodes operate over features extracted from local parts in space and time and over previous memory states. Edges process messages between connected nodes at different locations and spatial scales or between past and present time. Messages are passed iteratively in order to transmit information globally and establish long range interactions. Our model is general and could learn to recognize a variety of high level spatio-temporal concepts and be applied to different learning tasks. We demonstrate, through extensive experiments and ablation studies, that our model outperforms strong baselines and top published methods on recognizing complex activities in video. Moreover, we obtain state-of-the-art performance on the challenging Something-Something human-object interaction dataset.\n",
      "----------- 96 ------------\n",
      "Non-discrimination is a recognized objective in algorithmic decision making. In this paper, we introduce a novel probabilistic formulation of data pre-processing for reducing discrimination. We propose a convex optimization for learning a data transformation with three goals: controlling discrimination, limiting distortion in individual data samples, and preserving utility. We characterize the impact of limited sample size in accomplishing this objective. Two instances of the proposed optimization are applied to datasets, including one on real-world criminal recidivism. Results show that discrimination can be greatly reduced at a small cost in classification accuracy.\n",
      "----------- 97 ------------\n",
      "Optimizing task-related mathematical model is one of the most fundamental methodologies in statistic and learning areas. However, generally designed schematic iterations may hard to investigate complex data distributions in real-world applications. Recently, training deep propagations (i.e., networks) has gained promising performance in some particular tasks. Unfortunately, existing networks are often built in heuristic manners, thus lack of principled interpretations and solid theoretical supports. In this work, we provide a new paradigm, named Propagation and Optimization based Deep Model (PODM), to bridge the gaps between these different mechanisms (i.e., model optimization and deep propagation). On the one hand, we utilize PODM as a deeply trained solver for model optimization. Different from these existing network based iterations, which often lack theoretical investigations, we provide strict convergence analysis for PODM in the challenging nonconvex and nonsmooth scenarios. On the other hand, by relaxing the model constraints and performing end-to-end training, we also develop a PODM based strategy to integrate domain knowledge (formulated as models) and real data distributions (learned by networks), resulting in a generic ensemble framework for challenging real-world applications. Extensive experiments verify our theoretical results and demonstrate the superiority of PODM against these state-of-the-art approaches.\n",
      "----------- 98 ------------\n",
      "Knowledge distillation is to transfer the knowledge of a large neural network into a smaller one and has been shown to be effective especially when the amount of training data is limited or the size of the student model is very small. To transfer the knowledge, it is essential to observe the data that have been used to train the network since its knowledge is concentrated on a narrow manifold rather than the whole input space. However, the data are not accessible in many cases due to the privacy or confidentiality issues in medical, industrial, and military domains. To the best of our knowledge, there has been no approach that distills the knowledge of a neural network when no data are observable. In this work, we propose KegNet (Knowledge Extraction with Generative Networks), a novel approach to extract the knowledge of a trained deep neural network and to generate artificial data points that replace the missing training data in knowledge distillation. Experiments show that KegNet outperforms all baselines for data-free knowledge distillation. We provide the source code of our paper in https://github.com/snudatalab/KegNet.\n",
      "----------- 99 ------------\n",
      "Key to multitask learning is exploiting the relationships between different tasks to improve prediction performance. Most previous methods have focused on the case where tasks relations can be modeled as linear operators and regularization approaches can be used successfully. However, in practice assuming the tasks to be linearly related is often restrictive, and allowing for nonlinear structures is a challenge. In this paper, we tackle this issue by casting the problem within the framework of structured prediction. Our main contribution is a novel algorithm for learning multiple tasks which are related by a system of nonlinear equations that their joint outputs need to satisfy. We show that our algorithm can be efficiently implemented and study its generalization properties, proving universal consistency and learning rates. Our theoretical analysis highlights the benefits of non-linear multitask learning over learning the tasks independently. Encouraging experimental results show the benefits of the proposed method in practice.\n",
      "----------- 100 ------------\n",
      "Clustering large data is a fundamental problem with a vast number of applications. Due to the increasing size of data, practitioners interested in clustering have turned to distributed computation methods. In this work, we consider the widely used k-center clustering problem and its variant used to handle noisy data, k-center with outliers. In the noise-free setting we demonstrate how a previously-proposed distributed method is actually an O(1)-approximation algorithm, which accurately explains its strong empirical performance. Additionally, in the noisy setting, we develop a novel distributed algorithm that is also an O(1)-approximation. These algorithms are highly parallel and lend themselves to virtually any distributed computing framework. We compare both empirically against the best known noisy sequential clustering methods and show that both distributed algorithms are consistently close to their sequential versions. The algorithms are all one can hope for in distributed settings: they are fast, memory efficient and they match their sequential counterparts.\n",
      "----------- 101 ------------\n",
      "The task of program synthesis, or automatically generating programs that are consistent with a provided specification, remains a challenging task in artificial intelligence. As in other fields of AI, deep learning-based end-to-end approaches have made great advances in program synthesis. However, more so than other fields such as computer vision, program synthesis provides greater opportunities to explicitly exploit structured information such as execution traces, which contain a superset of the information input/output pairs. While they are highly useful for program synthesis, as execution traces are more difficult to obtain than input/output pairs, we use the insight that we can split the process into two parts: infer the trace from the input/output example, then infer the program from the trace. This simple modification leads to state-of-the-art results in program synthesis in the Karel domain, improving accuracy to 81.3% from the 77.12% of prior work.\n",
      "----------- 102 ------------\n",
      "Most robots lack the ability to learn new objects from past experiences. To migrate a robot to a new environment one must often completely re-generate the knowledge- base that it is running with. Since in open-ended domains the set of categories to be learned is not predefined, it is not feasible to assume that one can pre-program all object categories required by robots. Therefore, autonomous robots must have the ability to continuously execute learning and recognition in a concurrent and interleaved fashion. This paper proposes an open-ended 3D object recognition system which concurrently learns both the object categories and the statistical features for encoding objects. In particular, we propose an extension of Latent Dirichlet Allocation to learn structural semantic features (i.e. topics) from low-level feature co-occurrences for each category independently. Moreover, topics in each category are discovered in an unsupervised fashion and are updated incrementally using new object views. The approach contains similarities with the organization of the visual cortex and builds a hierarchy of increasingly sophisticated representations. Results show the fulfilling performance of this approach on different types of objects. Moreover, this system demonstrates the capability of learning from few training examples and competes with state-of-the-art systems.\n",
      "----------- 103 ------------\n",
      "We develop \\textit{parallel predictive entropy search} (PPES), a novel algorithm for Bayesian optimization of expensive black-box objective functions. At each iteration, PPES aims to select a \\textit{batch} of points which will maximize the information gain about the global maximizer of the objective. Well known strategies exist for suggesting a single evaluation point based on previous observations, while far fewer are known for selecting batches of points to evaluate in parallel. The few batch selection schemes that have been studied all resort to greedy methods to compute an optimal batch. To the best of our knowledge, PPES is the first non-greedy batch Bayesian optimization strategy. We demonstrate the benefit of this approach in optimization performance on both synthetic and real world applications, including problems in machine learning, rocket science and robotics.\n",
      "----------- 104 ------------\n",
      "Deep learning methods achieve state-of-the-art performance in many application scenarios. Yet, these methods require a significant amount of hyperparameters tuning in order to achieve the best results. In particular, tuning the learning rates in the stochastic optimization process is still one of the main bottlenecks. In this paper, we propose a new stochastic gradient descent procedure for deep networks that does not require any learning rate setting. Contrary to previous methods, we do not adapt the learning rates nor we make use of the assumed curvature of the objective function. Instead, we reduce the optimization process to a game of betting on a coin and propose a learning rate free optimal algorithm for this scenario. Theoretical convergence is proven for convex and quasi-convex functions and empirical evidence shows the advantage of our algorithm over popular stochastic gradient algorithms.\n",
      "----------- 105 ------------\n",
      "A key challenge in crowdsourcing is inferring the ground truth from noisy and unreliable data. To do so, existing approaches rely on collecting redundant information from the crowd, and aggregating it with some probabilistic method. However, oftentimes such methods are computationally inefficient, are restricted to some specific settings, or lack theoretical guarantees. In this paper, we revisit the problem of binary classification from crowdsourced data. Specifically we propose Streaming Bayesian Inference for Crowdsourcing (SBIC), a new algorithm that does not suffer from any of these limitations. First, SBIC has low complexity and can be used in a real-time online setting. Second, SBIC has the same accuracy as the best state-of-the-art algorithms in all settings. Third, SBIC has provable asymptotic guarantees both in the online and offline settings.\n",
      "----------- 106 ------------\n",
      "Maximum Inner Product Search (MIPS) is an important task in many machine learning applications such as the prediction phase of low-rank matrix factorization models and deep learning models. Recently, there has been substantial research on how to perform MIPS in sub-linear time, but most of the existing work does not have the flexibility to control the trade-off between search efficiency and search quality. In this paper, we study the important problem of MIPS with a computational budget. By carefully studying the problem structure of MIPS, we develop a novel Greedy-MIPS algorithm, which can handle budgeted MIPS by design. While simple and intuitive, Greedy-MIPS yields surprisingly superior performance compared to state-of-the-art approaches. As a specific example, on a candidate set containing half a million vectors of dimension 200, Greedy-MIPS runs 200x faster than the naive approach while yielding search results with the top-5 precision greater than 75%.\n",
      "----------- 107 ------------\n",
      "The use of Bayesian methods in large-scale data settings is attractive because of the rich hierarchical models, uncertainty quantification, and prior specification they provide. Standard Bayesian inference algorithms are computationally expensive, however, making their direct application to large datasets difficult or infeasible. Recent work on scaling Bayesian inference has focused on modifying the underlying algorithms to, for example, use only a random data subsample at each iteration. We leverage the insight that data is often redundant to instead obtain a weighted subset of the data (called a coreset) that is much smaller than the original dataset. We can then use this small coreset in any number of existing posterior inference algorithms without modification. In this paper, we develop an efficient coreset construction algorithm for Bayesian logistic regression models. We provide theoretical guarantees on the size and approximation quality of the coreset -- both for fixed, known datasets, and in expectation for a wide class of data generative models. Crucially, the proposed approach also permits efficient construction of the coreset in both streaming and parallel settings, with minimal additional effort. We demonstrate the efficacy of our approach on a number of synthetic and real-world datasets, and find that, in practice, the size of the coreset is independent of the original dataset size. Furthermore, constructing the coreset takes a negligible amount of time compared to that required to run MCMC on it.\n",
      "----------- 108 ------------\n",
      "We study the recovering bandits problem, a variant of the stochastic multi-armed bandit problem where the expected reward of each arm varies according to some unknown function of the time since the arm was last played. While being a natural extension of the classical bandit problem that arises in many real-world settings, this variation is accompanied by significant difficulties. In particular, methods need to plan ahead and estimate many more quantities than in the classical bandit setting. In this work, we explore the use of Gaussian processes to tackle the estimation and planing problem. We also discuss different regret definitions that let us quantify the performance of the methods. To improve computational efficiency of the methods, we provide an optimistic planning approximation. We complement these discussions with regret bounds and empirical studies.\n",
      "----------- 109 ------------\n",
      "Understanding why a model makes a certain prediction can be as crucial as the prediction's accuracy in many applications. However, the highest accuracy for large modern datasets is often achieved by complex models that even experts struggle to interpret, such as ensemble or deep learning models, creating a tension between accuracy and interpretability. In response, various methods have recently been proposed to help users interpret the predictions of complex models, but it is often unclear how these methods are related and when one method is preferable over another. To address this problem, we present a unified framework for interpreting predictions, SHAP (SHapley Additive exPlanations). SHAP assigns each feature an importance value for a particular prediction. Its novel components include: (1) the identification of a new class of additive feature importance measures, and (2) theoretical results showing there is a unique solution in this class with a set of desirable properties. The new class unifies six existing methods, notable because several recent methods in the class lack the proposed desirable properties. Based on insights from this unification, we present new methods that show improved computational performance and/or better consistency with human intuition than previous approaches.\n",
      "----------- 110 ------------\n",
      "The ability to inferring latent psychological traits from human behavior is key to developing personalized human-interacting machine learning systems. Approaches to infer such traits range from surveys to manually-constructed experiments and games. However, these traditional games are limited because they are typically designed based on heuristics. In this paper, we formulate the task of designing behavior diagnostic games that elicit distinguishable behavior as a mutual information maximization problem, which can be solved by optimizing a variational lower bound. Our framework is instantiated by using prospect theory to model varying player traits, and Markov Decision Processes to parameterize the games. We validate our approach empirically, showing that our designed games can successfully distinguish among players with different traits, outperforming manually-designed ones by a large margin.\n",
      "----------- 111 ------------\n",
      "A proper understanding of the striking generalization abilities of deep neural networks presents an enduring puzzle. Recently, there has been a growing body of numerically-grounded theoretical work that has contributed important insights to the theory of learning in deep neural nets. There has also been a recent interest in extending these analyses to understanding how multitask learning can further improve the generalization capacity of deep neural nets. These studies deal almost exclusively with regression tasks which are amenable to existing analytical techniques. We develop an analytic theory of the nonlinear dynamics of generalization of deep neural networks trained to solve classification tasks using softmax outputs and cross-entropy loss, addressing both single task and multitask settings. We do so by adapting techniques from the statistical physics of disordered systems, accounting for both finite size datasets and correlated outputs induced by the training dynamics. We discuss the validity of our theoretical results in comparison to a comprehensive suite of numerical experiments. Our analysis provides theoretical support for the intuition that the performance of multitask learning is determined by the noisiness of the tasks and how well their input features align with each other. Highly related, clean tasks benefit each other, whereas unrelated, clean tasks can be detrimental to individual task performance.\n",
      "----------- 112 ------------\n",
      "Recommendation and collaborative filtering systems are important in modern information and e-commerce applications. As these systems are becoming increasingly popular in industry, their outputs could affect business decision making, introducing incentives for an adversarial party to compromise the availability or integrity of such systems. We introduce a data poisoning attack on collaborative filtering systems. We demonstrate how a powerful attacker with full knowledge of the learner can generate malicious data so as to maximize his/her malicious objectives, while at the same time mimicking normal user behaviors to avoid being detected. While the complete knowledge assumption seems extreme, it enables a robust assessment of the vulnerability of collaborative filtering schemes to highly motivated attacks. We present efficient solutions for two popular factorization-based collaborative filtering algorithms: the alternative minimization formulation and the nuclear norm minimization method. Finally, we test the effectiveness of our proposed algorithms on real-world data and discuss potential defensive strategies.\n",
      "----------- 113 ------------\n",
      "Interpretability has become an important topic of research as more machine learning (ML) models are deployed and widely used to make important decisions. Most of the current explanation methods provide explanations through feature importance scores, which identify features that are important for each individual input. However, how to systematically summarize and interpret such per sample feature importance scores itself is challenging. In this work, we propose principles and desiderata for \\emph{concept} based explanation, which goes beyond per-sample features to identify higher level human-understandable concepts that apply across the entire dataset. We develop a new algorithm, ACE, to automatically extract visual concepts. Our systematic experiments demonstrate that \\alg discovers concepts that are human-meaningful, coherent and important for the neural network's predictions.\n",
      "----------- 114 ------------\n",
      "Dynamic neural networks toolkits such as PyTorch, DyNet, and Chainer offer more flexibility for implementing models that cope with data of varying dimensions and structure, relative to toolkits that operate on statically declared computations (e.g., TensorFlow, CNTK, and Theano). However, existing toolkits - both static and dynamic - require that the developer organize the computations into the batches necessary for exploiting high-performance data-parallel algorithms and hardware. This batching task is generally difficult, but it becomes a major hurdle as architectures become complex. In this paper, we present an algorithm, and its implementation in the DyNet toolkit, for automatically batching operations. Developers simply write minibatch computations as aggregations of single instance computations, and the batching algorithm seamlessly executes them, on the fly, in computationally efficient batches. On a variety of tasks, we obtain throughput similar to manual batches, as well as comparable speedups over single-instance learning on architectures that are impractical to batch manually.\n",
      "----------- 115 ------------\n",
      "Minimizing non-convex and high-dimensional objective functions is challenging, especially when training modern deep neural networks. In this paper, a novel approach is proposed which divides the training process into two consecutive phases to obtain better generalization performance: Bayesian sampling and stochastic optimization. The first phase is to explore the energy landscape and to capture the [Math Processing Error]`temperature dynamics''. These strategies can overcome the challenge of early trapping into bad local minima and have achieved remarkable improvements in various types of neural networks as shown in our theoretical analysis and empirical experiments.\n",
      "----------- 116 ------------\n",
      "Even though neural networks enjoy widespread use, they still struggle to learn the basic laws of physics. How might we endow them with better inductive biases? In this paper, we draw inspiration from Hamiltonian mechanics to train models that learn and respect exact conservation laws in an unsupervised manner. We evaluate our models on problems where conservation of energy is important, including the two-body problem and pixel observations of a pendulum. Our model trains faster and generalizes better than a regular neural network. An interesting side effect is that our model is perfectly reversible in time.\n",
      "----------- 117 ------------\n",
      "Evaluating a policy by deploying it in the real world can be risky and costly. Off-policy policy evaluation (OPE) algorithms use historical data collected from running a previous policy to evaluate a new policy, which provides a means for evaluating a policy without requiring it to ever be deployed. Importance sampling is a popular OPE method because it is robust to partial observability and works with continuous states and actions. However, the amount of historical data required by importance sampling can scale exponentially with the horizon of the problem: the number of sequential decisions that are made. We propose using policies over temporally extended actions, called options, and show that combining these policies with importance sampling can significantly improve performance for long-horizon problems. In addition, we can take advantage of special cases that arise due to options-based policies to further improve the performance of importance sampling. We further generalize these special cases to a general covariance testing rule that can be used to decide which weights to drop in an IS estimate, and derive a new IS algorithm called Incremental Importance Sampling that can provide significantly more accurate estimates for a broad class of domains.\n",
      "----------- 118 ------------\n",
      "The advent of deep learning algorithms for mobile devices and sensors has led to a dramatic expansion in the availability and number of systems trained on a wide range of machine learning tasks, creating a host of opportunities and challenges in the realm of transfer learning. Currently, most transfer learning methods require some kind of control over the systems learned, either by enforcing constraints during the source training, or through the use of a joint optimization objective between tasks that requires all data be co-located for training. However, for practical, privacy, or other reasons, in a variety of applications we may have no control over the individual source task training, nor access to source training samples. Instead we only have access to features pre-trained on such data as the output of \"black-boxes.'' For such scenarios, we consider the multi-source learning problem of training a classifier using an ensemble of pre-trained neural networks for a set of classes that have not been observed by any of the source networks, and for which we have very few training samples. We show that by using these distributed networks as feature extractors, we can train an effective classifier in a computationally-efficient manner using tools from (nonlinear) maximal correlation analysis. In particular, we develop a method we refer to as maximal correlation weighting (MCW) to build the required target classifier from an appropriate weighting of the feature functions from the source networks. We illustrate the effectiveness of the resulting classifier on datasets derived from the CIFAR-100, Stanford Dogs, and Tiny ImageNet datasets, and, in addition, use the methodology to characterize the relative value of different source tasks in learning a target task.\n",
      "----------- 119 ------------\n",
      "Bayesian Optimisation (BO) refers to a class of methods for global optimisation of a function f which is only accessible via point evaluations. It is typically used in settings where f is expensive to evaluate. A common use case for BO in machine learning is model selection, where it is not possible to analytically model the generalisation performance of a statistical model, and we resort to noisy and expensive training and validation procedures to choose the best model. Conventional BO methods have focused on Euclidean and categorical domains, which, in the context of model selection, only permits tuning scalar hyper-parameters of machine learning algorithms. However, with the surge of interest in deep learning, there is an increasing demand to tune neural network architectures. In this work, we develop NASBOT, a Gaussian process based BO framework for neural architecture search. To accomplish this, we develop a distance metric in the space of neural network architectures which can be computed efficiently via an optimal transport program. This distance might be of independent interest to the deep learning community as it may find applications outside of BO. We demonstrate that NASBOT outperforms other alternatives for architecture search in several cross validation based model selection tasks on multi-layer perceptrons and convolutional neural networks.\n",
      "----------- 120 ------------\n",
      "Social goods, such as healthcare, smart city, and information networks, often produce ordered event data in continuous time. The generative processes of these event data can be very complex, requiring flexible models to capture their dynamics. Temporal point processes offer an elegant framework for modeling event data without discretizing the time. However, the existing maximum-likelihood-estimation (MLE) learning paradigm requires hand-crafting the intensity function beforehand and cannot directly monitor the goodness-of-fit of the estimated model in the process of training. To alleviate the risk of model-misspecification in MLE, we propose to generate samples from the generative model and monitor the quality of the samples in the process of training until the samples and the real data are indistinguishable. We take inspiration from reinforcement learning (RL) and treat the generation of each event as the action taken by a stochastic policy. We parameterize the policy as a flexible recurrent neural network and gradually improve the policy to mimic the observed event distribution. Since the reward function is unknown in this setting, we uncover an analytic and nonparametric form of the reward function using an inverse reinforcement learning formulation. This new RL framework allows us to derive an efficient policy gradient algorithm for learning flexible point process models, and we show that it performs well in both synthetic and real data.\n",
      "----------- 121 ------------\n",
      "We reduce the computational cost of Neural AutoML with transfer learning. AutoML relieves human effort by automating the design of ML algorithms. Neural AutoML has become popular for the design of deep learning architectures, however, this method has a high computation cost. To address this we propose Transfer Neural AutoML that uses knowledge from prior tasks to speed up network design. We extend RL-based architecture search methods to support parallel training on multiple tasks and then transfer the search strategy to new tasks. On language and image classification data, Transfer Neural AutoML reduces convergence time over single-task training by over an order of magnitude on many tasks.\n",
      "----------- 122 ------------\n",
      "Neural networks are increasingly deployed in real-world safety-critical domains such as autonomous driving, aircraft collision avoidance, and malware detection. However, these networks have been shown to often mispredict on inputs with minor adversarial or even accidental perturbations. Consequences of such errors can be disastrous and even potentially fatal as shown by the recent Tesla autopilot crash. Thus, there is an urgent need for formal analysis systems that can rigorously check neural networks for violations of different safety properties such as robustness against adversarial perturbations within a certain L-norm of a given image. An effective safety analysis system for a neural network must be able to either ensure that a safety property is satisfied by the network or find a counterexample, i.e., an input for which the network will violate the property. Unfortunately, most existing techniques for performing such analysis struggle to scale beyond very small networks and the ones that can scale to larger networks suffer from high false positives and cannot produce concrete counterexamples in case of a property violation. In this paper, we present a new efficient approach for rigorously checking different safety properties of neural networks that significantly outperforms existing approaches by multiple orders of magnitude. Our approach can check different safety properties and find concrete counterexamples for networks that are 10x larger than the ones supported by existing analysis techniques. We believe that our approach to estimating tight output bounds of a network for a given input range can also help improve the explainability of neural networks and guide the training process of more robust neural networks.\n",
      "----------- 123 ------------\n",
      "Optimization is an integral part of most machine learning systems and most numerical optimization schemes rely on the computation of derivatives. Therefore, frameworks for computing derivatives are an active area of machine learning research. Surprisingly, as of yet, no existing framework is capable of computing higher order matrix and tensor derivatives directly. Here, we close this fundamental gap and present an algorithmic framework for computing matrix and tensor derivatives that extends seamlessly to higher order derivatives. The framework can be used for symbolic as well as for forward and reverse mode automatic differentiation. Experiments show a speedup between one and four orders of magnitude over state-of-the-art frameworks when evaluating higher order derivatives.\n",
      "----------- 124 ------------\n",
      "Increasing the batch size is a popular way to speed up neural network training, but beyond some critical batch size, larger batch sizes yield diminishing returns. In this work, we study how the critical batch size changes based on properties of the optimization algorithm, including acceleration and preconditioning, through two different lenses: large scale experiments and analysis using a simple noisy quadratic model (NQM). We experimentally demonstrate that optimization algorithms that employ preconditioning, specifically Adam and K-FAC, result in much larger critical batch sizes than stochastic gradient descent with momentum. We also demonstrate that the NQM captures many of the essential features of real neural network training, despite being drastically simpler to work with. The NQM predicts our results with preconditioned optimizers, previous results with accelerated gradient descent, and other results around optimal learning rates and large batch training, making it a useful tool to generate testable predictions about neural network optimization. We demonstrate empirically that the simple noisy quadratic model (NQM) displays many similarities to neural networks in terms of large-batch training. We prove analytical convergence results for the NQM model that predict such behavior and hence provide possible explanations and a better understanding for many large-batch training phenomena.\n",
      "----------- 125 ------------\n",
      "The design of neural network architectures is an important component for achieving state-of-the-art performance with machine learning systems across a broad array of tasks. Much work has endeavored to design and build architectures automatically through clever construction of a search space paired with simple learning algorithms. Recent progress has demonstrated that such meta-learning methods may exceed scalable human-invented architectures on image classification tasks. An open question is the degree to which such methods may generalize to new domains. In this work we explore the construction of meta-learning techniques for dense image prediction focused on the tasks of scene parsing, person-part segmentation, and semantic image segmentation. Constructing viable search spaces in this domain is challenging because of the multi-scale representation of visual information and the necessity to operate on high resolution imagery. Based on a survey of techniques in dense image prediction, we construct a recursive search space and demonstrate that even with efficient random search, we can identify architectures that outperform human-invented architectures and achieve state-of-the-art performance on three dense prediction tasks including 82.7% on Cityscapes (street scene parsing), 71.3% on PASCAL-Person-Part (person-part segmentation), and 87.9% on PASCAL VOC 2012 (semantic image segmentation). Additionally, the resulting architecture is more computationally efficient, requiring half the parameters and half the computational cost as previous state of the art systems.\n",
      "----------- 126 ------------\n",
      "We consider the problem of building continuous occupancy representations in dynamic environments for robotics applications. The problem has hardly been discussed previously due to the complexity of patterns in urban environments, which have both spatial and temporal dependencies. We address the problem as learning a kernel classifier on an efficient feature space. The key novelty of our approach is the incorporation of variations in the time domain into the spatial domain. We propose a method to propagate motion uncertainty into the kernel using a hierarchical model. The main benefit of this approach is that it can directly predict the occupancy state of the map in the future from past observations, being a valuable tool for robot trajectory planning under uncertainty. Our approach preserves the main computational benefits of static Hilbert maps — using stochastic gradient descent for fast optimization of model parameters and incremental updates as new data are captured. Experiments conducted in road intersections of an urban environment demonstrated that spatio-temporal Hilbert maps can accurately model changes in the map while outperforming other techniques on various aspects.\n",
      "----------- 127 ------------\n",
      "Efficient exploration remains a major challenge for reinforcement learning (RL). Common dithering strategies for exploration, such as epsilon-greedy, do not carry out temporally-extended (or deep) exploration; this can lead to exponentially larger data requirements. However, most algorithms for statistically efficient RL are not computationally tractable in complex environments. Randomized value functions offer a promising approach to efficient exploration with generalization, but existing algorithms are not compatible with nonlinearly parameterized value functions. As a first step towards addressing such contexts we develop bootstrapped DQN. We demonstrate that bootstrapped DQN can combine deep exploration with deep neural networks for exponentially faster learning than any dithering strategy. In the Arcade Learning Environment bootstrapped DQN substantially improves learning speed and cumulative performance across most games.\n",
      "----------- 128 ------------\n",
      "Inferring topological and geometrical information from data can offer an alternative perspective in machine learning problems. Methods from topological data analysis, e.g., persistent homology, enable us to obtain such information, typically in the form of summary representations of topological features. However, such topological signatures often come with an unusual structure (e.g., multisets of intervals) that is highly impractical for most machine learning techniques. While many strategies have been proposed to map these topological signatures into machine learning compatible representations, they suffer from being agnostic to the target learning task. In contrast, we propose a technique that enables us to input topological signatures to deep neural networks and learn a task-optimal representation during training. Our approach is realized as a novel input layer with favorable theoretical properties. Classification experiments on 2D object shapes and social network graphs demonstrate the versatility of the approach and, in case of the latter, we even outperform the state-of-the-art by a large margin.\n",
      "----------- 129 ------------\n",
      "Reinforcement learning (RL) algorithms have demonstrated promising results on complex tasks, yet often require impractical numbers of samples because they learn from scratch. Meta-RL aims to address this challenge by leveraging experience from previous tasks so as to more quickly solve new tasks. However, in practice, these algorithms generally also require large amounts of on-policy experience during the \\emph{meta-training} process, making them impractical for use in many problems. To this end, we propose to learn a reinforcement learning procedure in a federated way, where individual off-policy learners can solve the individual meta-training tasks, and then consolidate these solutions into a single meta-learner. Since the central meta-learner learns by imitating the solutions to the individual tasks, it can accommodate either the standard meta-RL problem setting, or a hybrid setting where some or all tasks are provided with example demonstrations. The former results in an approach that can leverage policies learned for previous tasks without significant amounts of on-policy data during meta-training, whereas the latter is particularly useful in cases where demonstrations are easy for a person to provide. Across a number of continuous control meta-RL problems, we demonstrate significant improvements in meta-RL sample efficiency in comparison to prior work as well as the ability to scale to domains with visual observations.\n",
      "----------- 130 ------------\n",
      "The vast majority of processors in the world are actually microcontroller units (MCUs), which find widespread use performing simple control tasks in applications ranging from automobiles to medical devices and office equipment. The Internet of Things (IoT) promises to inject machine learning into many of these every-day objects via tiny, cheap MCUs. However, these resource-impoverished hardware platforms severely limit the complexity of machine learning models that can be deployed. For example, although convolutional neural networks (CNNs) achieve state-of-the-art results on many visual recognition tasks, CNN inference on MCUs is challenging due to severe memory limitations. To circumvent the memory challenge associated with CNNs, various alternatives have been proposed that do fit within the memory budget of an MCU, albeit at the cost of prediction accuracy. This paper challenges the idea that CNNs are not suitable for deployment on MCUs. We demonstrate that it is possible to automatically design CNNs which generalize well, while also being small enough to fit onto memory-limited MCUs. Our Sparse Architecture Search method combines neural architecture search with pruning in a single, unified approach, which learns superior models on four popular IoT datasets. The CNNs we find are more accurate and up to 7.4× smaller than previous approaches, while meeting the strict MCU working memory constraint.\n",
      "----------- 131 ------------\n",
      "The success of machine learning in a broad range of applications has led to an ever-growing demand for machine learning systems that can be used off the shelf by non-experts. To be effective in practice, such systems need to automatically choose a good algorithm and feature preprocessing steps for a new dataset at hand, and also set their respective hyperparameters. Recent work has started to tackle this automated machine learning (AutoML) problem with the help of efficient Bayesian optimization methods. In this work we introduce a robust new AutoML system based on scikit-learn (using 15 classifiers, 14 feature preprocessing methods, and 4 data preprocessing methods, giving rise to a structured hypothesis space with 110 hyperparameters). This system, which we dub auto-sklearn, improves on existing AutoML methods by automatically taking into account past performance on similar datasets, and by constructing ensembles from the models evaluated during the optimization. Our system won the first phase of the ongoing ChaLearn AutoML challenge, and our comprehensive analysis on over 100 diverse datasets shows that it substantially outperforms the previous state of the art in AutoML. We also demonstrate the performance gains due to each of our contributions and derive insights into the effectiveness of the individual components of auto-sklearn.\n",
      "----------- 132 ------------\n",
      "Abstraction and realization are bilateral processes that are key in deriving intelligence and creativity. In many domains, the two processes are approached through \\emph{rules}: high-level principles that reveal invariances within similar yet diverse examples. Under a probabilistic setting for discrete input spaces, we focus on the rule realization problem which generates input sample distributions that follow the given rules. More ambitiously, we go beyond a mechanical realization that takes whatever is given, but instead ask for proactively selecting reasonable rules to realize. This goal is demanding in practice, since the initial rule set may not always be consistent and thus intelligent compromises are needed. We formulate both rule realization and selection as two strongly connected components within a single and symmetric bi-convex problem, and derive an efficient algorithm that works at large scale. Taking music compositional rules as the main example throughout the paper, we demonstrate our model's efficiency in not only music realization (composition) but also music interpretation and understanding (analysis).\n",
      "----------- 133 ------------\n",
      "Nowadays, the number of layers and of neurons in each layer of a deep network are typically set manually. While very deep and wide networks have proven effective in general, they come at a high memory and computation cost, thus making them impractical for constrained platforms. These networks, however, are known to have many redundant parameters, and could thus, in principle, be replaced by more compact architectures. In this paper, we introduce an approach to automatically determining the number of neurons in each layer of a deep network during learning. To this end, we propose to make use of a group sparsity regularizer on the parameters of the network, where each group is defined to act on a single neuron. Starting from an overcomplete network, we show that our approach can reduce the number of parameters by up to 80\\% while retaining or even improving the network accuracy.\n",
      "----------- 134 ------------\n",
      "Recent advances in bandit tools and techniques for sequential learning are steadily enabling new applications and are promising the resolution of a range of challenging related problems. We study the game tree search problem, where the goal is to quickly identify the optimal move in a given game tree by sequentially sampling its stochastic payoffs. We develop new algorithms for trees of arbitrary depth, that operate by summarizing all deeper levels of the tree into confidence intervals at depth one, and applying a best arm identification procedure at the root. We prove new sample complexity guarantees with a refined dependence on the problem instance. We show experimentally that our algorithms outperform existing elimination-based algorithms and match previous special-purpose methods for depth-two trees.\n",
      "----------- 135 ------------\n",
      "Beam search is widely used for approximate decoding in structured prediction problems. Models often use a beam at test time but ignore its existence at train time, and therefore do not explicitly learn how to use the beam. We develop an unifying meta-algorithm for learning beam search policies using imitation learning. In our setting, the beam is part of the model and not just an artifact of approximate decoding. Our meta-algorithm captures existing learning algorithms and suggests new ones. It also lets us show novel no-regret guarantees for learning beam search policies.\n",
      "----------- 136 ------------\n",
      "State representation learning, or the ability to capture latent generative factors of an environment is crucial for building intelligent agents that can perform a wide variety of tasks. Learning such representations in an unsupervised manner without supervision from rewards is an open problem. We introduce a method that tries to learn better state representations by maximizing mutual information across spatially and temporally distinct features of a neural encoder of the observations. We also introduce a new benchmark based on Atari 2600 games where we evaluate representations based on how well they capture the ground truth state. We believe this new framework for evaluating representation learning models will be crucial for future representation learning research. Finally, we compare our technique with other state-of-the-art generative and contrastive representation learning methods.\n",
      "----------- 137 ------------\n",
      "We present an end-to-end framework for solving the Vehicle Routing Problem (VRP) using reinforcement learning. In this approach, we train a single policy model that finds near-optimal solutions for a broad range of problem instances of similar size, only by observing the reward signals and following feasibility rules. We consider a parameterized stochastic policy, and by applying a policy gradient algorithm to optimize its parameters, the trained model produces the solution as a sequence of consecutive actions in real time, without the need to re-train for every new problem instance. On capacitated VRP, our approach outperforms classical heuristics and Google's OR-Tools on medium-sized instances in solution quality with comparable computation time (after training). We demonstrate how our approach can handle problems with split delivery and explore the effect of such deliveries on the solution quality. Our proposed framework can be applied to other variants of the VRP such as the stochastic VRP, and has the potential to be applied more generally to combinatorial optimization problems\n",
      "----------- 138 ------------\n",
      "The large majority of differentially private algorithms focus on the static setting, where queries are made on an unchanging database. This is unsuitable for the myriad applications involving databases that grow over time. To address this gap in the literature, we consider the dynamic setting, in which new data arrive over time. Previous results in this setting have been limited to answering a single non-adaptive query repeatedly as the database grows. In contrast, we provide tools for richer and more adaptive analysis of growing databases. Our first contribution is a novel modification of the private multiplicative weights algorithm, which provides accurate analysis of exponentially many adaptive linear queries (an expressive query class including all counting queries) for a static database. Our modification maintains the accuracy guarantee of the static setting even as the database grows without bound. Our second contribution is a set of general results which show that many other private and accurate algorithms can be immediately extended to the dynamic setting by rerunning them at appropriate points of data growth with minimal loss of accuracy, even when data growth is unbounded.\n",
      "----------- 139 ------------\n",
      "Efficient exploration is crucial to achieving good performance in reinforcement learning. Existing systematic exploration strategies (R-MAX, MBIE, UCRL, etc.), despite being promising theoretically, are essentially greedy strategies that follow some predefined heuristics. When the heuristics do not match the dynamics of Markov decision processes (MDPs) well, an excessive amount of time can be wasted in travelling through already-explored states, lowering the overall efficiency. We argue that explicit planning for exploration can help alleviate such a problem, and propose a Value Iteration for Exploration Cost (VIEC) algorithm which computes the optimal exploration scheme by solving an augmented MDP. We then present a detailed analysis of the exploration behaviour of some popular strategies, showing how these strategies can fail and spend O(n^2 md) or O(n^2 m + nmd) steps to collect sufficient data in some tower-shaped MDPs, while the optimal exploration scheme, which can be obtained by VIEC, only needs O(nmd), where n, m are the numbers of states and actions and d is the data demand. The analysis not only points out the weakness of existing heuristic-based strategies, but also suggests a remarkable potential in explicit planning for exploration.\n",
      "----------- 140 ------------\n",
      "Probability estimation is one of the fundamental tasks in statistics and machine learning. However, standard methods for probability estimation on discrete objects do not handle object structure in a satisfactory manner. In this paper, we derive a general Bayesian network formulation for probability estimation on leaf-labeled trees that enables flexible approximations which can generalize beyond observations. We show that efficient algorithms for learning Bayesian networks can be easily extended to probability estimation on this challenging structured space. Experiments on both synthetic and real data show that our methods greatly outperform the current practice of using the empirical distribution, as well as a previous effort for probability estimation on trees.\n",
      "----------- 141 ------------\n",
      "Modern discriminative predictors have been shown to match natural intelligences in specific perceptual tasks in image classification, object and part detection, boundary extraction, etc. However, a major advantage that natural intelligences still have is that they work well for all perceptual problems together, solving them efficiently and coherently in an integrated manner. In order to capture some of these advantages in machine perception, we ask two questions: whether deep neural networks can learn universal image representations, useful not only for a single task but for all of them, and how the solutions to the different tasks can be integrated in this framework. We answer by proposing a new architecture, which we call multinet, in which not only deep image features are shared between tasks, but where tasks can interact in a recurrent manner by encoding the results of their analysis in a common shared representation of the data. In this manner, we show that the performance of individual tasks in standard benchmarks can be improved first by sharing features between them and then, more significantly, by integrating their solutions in the common representation.\n",
      "----------- 142 ------------\n",
      "We develop a learning framework for building deformable templates, which play a fundamental role in many image analysis and computational anatomy tasks. Conventional methods for template creation and image alignment to the template have undergone decades of rich technical development. In these frameworks, templates are constructed using an iterative process of template estimation and alignment, which is often computationally very expensive. Due in part to this shortcoming, most methods compute a single template for the entire population of images, or a few templates for specific sub-groups of the data. In this work, we present a probabilistic model and efficient learning strategy that yields either universal or \\textit{conditional} templates, jointly with a neural network that provides efficient alignment of the images to these templates. We demonstrate the usefulness of this method on a variety of domains, with a special focus on neuroimaging. This is particularly useful for clinical applications where a pre-existing template does not exist, or creating a new one with traditional methods can be prohibitively expensive. Our code and atlases are available online as part of the VoxelMorph library at http://voxelmorph.csail.mit.edu.\n",
      "----------- 143 ------------\n",
      "Abstraction reasoning is a long-standing challenge in artificial intelligence. Recent studies suggest that many of the deep architectures that have triumphed over other domains failed to work well in abstract reasoning. In this paper, we first illustrate that one of the main challenges in such a reasoning task is the presence of distracting features, which requires the learning algorithm to leverage counter-evidence and to reject any of false hypothesis in order to learn the true patterns. We later show that carefully designed learning trajectory over different categories of training data can effectively boost learning performance by mitigating the impacts of distracting features. Inspired this fact, we propose feature robust abstract reasoning (FRAR) model, which consists of a reinforcement learning based teacher network to determine the sequence of training and a student network for predictions. Experimental results demonstrated strong improvements over baseline algorithms and we are able to beat the state-of-the-art models by 18.7\\% in RAVEN dataset and 13.3\\% in the PGM dataset.\n",
      "----------- 144 ------------\n",
      "Quantum information is a promising new paradigm for fast computations that can provide substantial speedups for many algorithms we use today. Among them, quantum machine learning is one of the most exciting applications of quantum computers. In this paper, we introduce q-means, a new quantum algorithm for clustering. It is a quantum version of a robust k-means algorithm, with similar convergence and precision guarantees. We also design a method to pick the initial centroids equivalent to the classical k-means++ method. Our algorithm provides currently an exponential speedup in the number of points of the dataset, compared to the classical k-means algorithm. We also detail the running time of q-means when applied to well-clusterable datasets. We provide a detailed runtime analysis and numerical simulations for specific datasets. Along with the algorithm, the theorems and tools introduced in this paper can be reused for various applications in quantum machine learning.\n",
      "----------- 145 ------------\n",
      "Stochastic search algorithms are general black-box optimizers. Due to their ease of use and their generality, they have recently also gained a lot of attention in operations research, machine learning and policy search. Yet, these algorithms require a lot of evaluations of the objective, scale poorly with the problem dimension, are affected by highly noisy objective functions and may converge prematurely. To alleviate these problems, we introduce a new surrogate-based stochastic search approach. We learn simple, quadratic surrogate models of the objective function. As the quality of such a quadratic approximation is limited, we do not greedily exploit the learned models. The algorithm can be misled by an inaccurate optimum introduced by the surrogate. Instead, we use information theoretic constraints to bound the `distance' between the new and old data distribution while maximizing the objective function. Additionally the new method is able to sustain the exploration of the search distribution to avoid premature convergence. We compare our method with state of art black-box optimization methods on standard uni-modal and multi-modal optimization functions, on simulated planar robot tasks and a complex robot ball throwing task.The proposed method considerably outperforms the existing approaches.\n",
      "----------- 146 ------------\n",
      "The design of flow control systems remains a challenge due to the nonlinear nature of the equations that govern fluid flow. However, recent advances in computational fluid dynamics (CFD) have enabled the simulation of complex fluid flows with high accuracy, opening the possibility of using learning-based approaches to facilitate controller design. We present a method for learning the forced and unforced dynamics of airflow over a cylinder directly from CFD data. The proposed approach, grounded in Koopman theory, is shown to produce stable dynamical models that can predict the time evolution of the cylinder system over extended time horizons. Finally, by performing model predictive control with the learned dynamical models, we are able to find a straightforward, interpretable control law for suppressing vortex shedding in the wake of the cylinder.\n",
      "----------- 147 ------------\n",
      "Neural architecture search methods are able to find high performance deep learning architectures with minimal effort from an expert. However, current systems focus on specific use-cases (e.g. convolutional image classifiers and recurrent language models), making them unsuitable for general use-cases that an expert might wish to write. Hyperparameter optimization systems are general-purpose but lack the constructs needed for easy application to architecture search. In this work, we propose a formal language for encoding search spaces over general computational graphs. The language constructs allow us to write modular, composable, and reusable search space encodings and to reason about search space design. We use our language to encode search spaces from the architecture search literature. The language allows us to decouple the implementations of the search space and the search algorithm, allowing us to expose search spaces to search algorithms through a consistent interface. Our experiments show the ease with which we can experiment with different combinations of search spaces and search algorithms without having to implement each combination from scratch. We release an implementation of our language with this paper.\n",
      "----------- 148 ------------\n",
      "How can we help a forgetful learner learn multiple concepts within a limited time frame? While there have been extensive studies in designing optimal schedules for teaching a single concept given a learner's memory model, existing approaches for teaching multiple concepts are typically based on heuristic scheduling techniques without theoretical guarantees. In this paper, we look at the problem from the perspective of discrete optimization and introduce a novel algorithmic framework for teaching multiple concepts with strong performance guarantees. Our framework is both generic, allowing the design of teaching schedules for different memory models, and also interactive, allowing the teacher to adapt the schedule to the underlying forgetting mechanisms of the learner. Furthermore, for a well-known memory model, we are able to identify a regime of model parameters where our framework is guaranteed to achieve high performance. We perform extensive evaluations using simulations along with real user studies in two concrete applications: (i) an educational app for online vocabulary teaching; and (ii) an app for teaching novices how to recognize animal species from images. Our results demonstrate the effectiveness of our algorithm compared to popular heuristic approaches.\n",
      "----------- 149 ------------\n",
      "We introduce a method which enables a recurrent dynamics model to be temporally abstract. Our approach, which we call Adaptive Skip Intervals (ASI), is based on the observation that in many sequential prediction tasks, the exact time at which events occur is irrelevant to the underlying objective. Moreover, in many situations, there exist prediction intervals which result in particularly easy-to-predict transitions. We show that there are prediction tasks for which we gain both computational efficiency and prediction accuracy by allowing the model to make predictions at a sampling rate which it can choose itself.\n",
      "----------- 150 ------------\n",
      "Deep Neural Networks (DNN) have achieved state-of-the-art results in a wide range of tasks, with the best results obtained with large training sets and large models. In the past, GPUs enabled these breakthroughs because of their greater computational speed. In the future, faster computation at both training and test time is likely to be crucial for further progress and for consumer applications on low-power devices. As a result, there is much interest in research and development of dedicated hardware for Deep Learning (DL). Binary weights, i.e., weights which are constrained to only two possible values (e.g. -1 or 1), would bring great benefits to specialized DL hardware by replacing many multiply-accumulate operations by simple accumulations, as multipliers are the most space and power-hungry components of the digital implementation of neural networks. We introduce BinaryConnect, a method which consists in training a DNN with binary weights during the forward and backward propagations, while retaining precision of the stored weights in which gradients are accumulated. Like other dropout schemes, we show that BinaryConnect acts as regularizer and we obtain near state-of-the-art results with BinaryConnect on the permutation-invariant MNIST, CIFAR-10 and SVHN.\n",
      "----------- 151 ------------\n",
      "Robotic motion-planning problems, such as a UAV flying fast in a partially-known environment or a robot arm moving around cluttered objects, require finding collision-free paths quickly. Typically, this is solved by constructing a graph, where vertices represent robot configurations and edges represent potentially valid movements of the robot between theses configurations. The main computational bottlenecks are expensive edge evaluations to check for collisions. State of the art planning methods do not reason about the optimal sequence of edges to evaluate in order to find a collision free path quickly. In this paper, we do so by drawing a novel equivalence between motion planning and the Bayesian active learning paradigm of decision region determination (DRD). Unfortunately, a straight application of ex- isting methods requires computation exponential in the number of edges in a graph. We present BISECT, an efficient and near-optimal algorithm to solve the DRD problem when edges are independent Bernoulli random variables. By leveraging this property, we are able to significantly reduce computational complexity from exponential to linear in the number of edges. We show that BISECT outperforms several state of the art algorithms on a spectrum of planning problems for mobile robots, manipulators, and real flight data collected from a full scale helicopter. Open-source code and details can be found here: https://github.com/sanjibac/matlab_learning_collision_checking\n",
      "----------- 152 ------------\n",
      "We introduce a learning-based framework to optimize tensor programs for deep learning workloads. Efficient implementations of tensor operators, such as matrix multiplication and high dimensional convolution are key enablers of effective deep learning systems. However, existing systems rely on manually optimized libraries such as cuDNN where only a narrow range of server class GPUs are well-supported. The reliance on hardware specific operator libraries limits the applicability of high-level graph optimizations and incurs significant engineering costs when deploying to new hardware targets. We use learning to remove this engineering burden. We learn domain specific statistical cost models to guide the search of tensor operator implementations over billions of possible program variants. We further accelerate the search by effective model transfer across workloads. Experimental results show that our framework delivers performance competitive with state-of-the-art hand-tuned libraries for low-power CPU, mobile GPU, and server-class GPU.\n",
      "----------- 153 ------------\n",
      "In addition to identifying the content within a single image, relating images and generating related images are critical tasks for image understanding. Recently, deep convolutional networks have yielded breakthroughs in producing image labels, annotations and captions, but have only just begun to be used for producing high-quality image outputs. In this paper we develop a novel deep network trained end-to-end to perform visual analogy making, which is the task of transforming a query image according to an example pair of related images. Solving this problem requires both accurately recognizing a visual relationship and generating a transformed query image accordingly. Inspired by recent advances in language modeling, we propose to solve visual analogies by learning to map images to a neural embedding in which analogical reasoning is simple, such as by vector subtraction and addition. In experiments, our model effectively models visual analogies on several datasets: 2D shapes, animated video game sprites, and 3D car models.\n",
      "----------- 154 ------------\n",
      "We build a theoretical framework for designing and understanding practical meta-learning methods that integrates sophisticated formalizations of task-similarity with the extensive literature on online convex optimization and sequential prediction algorithms. Our approach enables the task-similarity to be learned adaptively, provides sharper transfer-risk bounds in the setting of statistical learning-to-learn, and leads to straightforward derivations of average-case regret bounds for efficient algorithms in settings where the task-environment changes dynamically or the tasks share a certain geometric structure. We use our theory to modify several popular meta-learning algorithms and improve their training and meta-test-time performance on standard problems in few-shot and federated learning.\n",
      "----------- 155 ------------\n",
      "Despite the success of kernel-based nonparametric methods, kernel selection still requires considerable expertise, and is often described as a “black art.” We present a sophisticated method for automatically searching for an appropriate kernel from an infinite space of potential choices. Previous efforts in this direction have focused on traversing a kernel grammar, only examining the data via computation of marginal likelihood. Our proposed search method is based on Bayesian optimization in model space, where we reason about model evidence as a function to be maximized. We explicitly reason about the data distribution and how it induces similarity between potential model choices in terms of the explanations they can offer for observed data. In this light, we construct a novel kernel between models to explain a given dataset. Our method is capable of finding a model that explains a given dataset well without any human assistance, often with fewer computations of model evidence than previous approaches, a claim we demonstrate empirically.\n",
      "----------- 156 ------------\n",
      "The goal of minimizing misclassification error on a training set is often just one of several real-world goals that might be defined on different datasets. For example, one may require a classifier to also make positive predictions at some specified rate for some subpopulation (fairness), or to achieve a specified empirical recall. Other real-world goals include reducing churn with respect to a previously deployed model, or stabilizing online training. In this paper we propose handling multiple goals on multiple datasets by training with dataset constraints, using the ramp penalty to accurately quantify costs, and present an efficient algorithm to approximately optimize the resulting non-convex constrained optimization problem. Experiments on both benchmark and real-world industry datasets demonstrate the effectiveness of our approach.\n",
      "----------- 157 ------------\n",
      "Equipping machine learning models with ethical and legal constraints is a serious issue; without this, the future of machine learning is at risk. This paper takes a step forward in this direction and focuses on ensuring machine learning models deliver fair decisions. In legal scholarships, the notion of fairness itself is evolving and multi-faceted. We set an overarching goal to develop a unified machine learning framework that is able to handle any definitions of fairness, their combinations, and also new definitions that might be stipulated in the future. To achieve our goal, we recycle two well-established machine learning techniques, privileged learning and distribution matching, and harmonize them for satisfying multi-faceted fairness definitions. We consider protected characteristics such as race and gender as privileged information that is available at training but not at test time; this accelerates model training and delivers fairness through unawareness. Further, we cast demographic parity, equalized odds, and equality of opportunity as a classical two-sample problem of conditional distributions, which can be solved in a general form by using distance measures in Hilbert Space. We show several existing models are special cases of ours. Finally, we advocate returning the Pareto frontier of multi-objective minimization of error and unfairness in predictions. This will facilitate decision makers to select an operating point and to be accountable for it.\n",
      "----------- 158 ------------\n",
      "Many machine learning methods depend on human supervision to achieve optimal performance. However, in tasks such as DensePose, where the goal is to establish dense visual correspondences between images, the quality of manual annotations is intrinsically limited. We address this issue by augmenting neural network predictors with the ability to output a distribution over labels, thus explicitly and introspectively capturing the aleatoric uncertainty in the annotations. Compared to previous works, we show that correlated error fields arise naturally in applications such as DensePose and these fields can be modeled by deep networks, leading to a better understanding of the annotation errors. We show that these models, by understanding uncertainty better, can solve the original DensePose task more accurately, thus setting the new state-of-the-art accuracy in this benchmark. Finally, we demonstrate the utility of the uncertainty estimates in fusing the predictions of produced by multiple models, resulting in a better and more principled approach to model ensembling which can further improve accuracy.\n",
      "----------- 159 ------------\n",
      "Prior work has investigated variations of prediction markets that preserve participants' (differential) privacy, which formed the basis of useful mechanisms for purchasing data for machine learning objectives. Such markets required potentially unlimited financial subsidy, however, making them impractical. In this work, we design an adaptively-growing prediction market with a bounded financial subsidy, while achieving privacy, incentives to produce accurate predictions, and precision in the sense that market prices are not heavily impacted by the added privacy-preserving noise. We briefly discuss how our mechanism can extend to the data-purchasing setting, and its relationship to traditional learning algorithms.\n",
      "----------- 160 ------------\n",
      "Even in state-spaces of modest size, planning is plagued by the “curse of dimensionality”. This problem is particularly acute in human and animal cognition given the limited capacity of working memory, and the time pressures under which planning often occurs in the natural environment. Hierarchically organized modular representations have long been suggested to underlie the capacity of biological systems to efficiently and flexibly plan in complex environments. However, the principles underlying efficient modularization remain obscure, making it difficult to identify its behavioral and neural signatures. Here, we develop a normative theory of efficient state-space representations which partitions an environment into distinct modules by minimizing the average (information theoretic) description length of planning within the environment, thereby optimally trading off the complexity of planning across and within modules. We show that such optimal representations provide a unifying account for a diverse range of hitherto unrelated phenomena at multiple levels of behavior and neural representation.\n",
      "----------- 161 ------------\n",
      "This paper proposes an adaptive neural-compilation framework to address the problem of learning efficient program. Traditional code optimisation strategies used in compilers are based on applying pre-specified set of transformations that make the code faster to execute without changing its semantics. In contrast, our work involves adapting programs to make them more efficient while considering correctness only on a target input distribution. Our approach is inspired by the recent works on differentiable representations of programs. We show that it is possible to compile programs written in a low-level language to a differentiable representation. We also show how programs in this representation can be optimised to make them efficient on a target distribution of inputs. Experimental results demonstrate that our approach enables learning specifically-tuned algorithms for given data distributions with a high success rate.\n",
      "----------- 162 ------------\n",
      "The alternating direction method of multipliers (ADMM) is an important tool for solving complex optimization problems, but it involves minimization sub-steps that are often difficult to solve efficiently. The Primal-Dual Hybrid Gradient (PDHG) method is a powerful alternative that often has simpler substeps than ADMM, thus producing lower complexity solvers. Despite the flexibility of this method, PDHG is often impractical because it requires the careful choice of multiple stepsize parameters. There is often no intuitive way to choose these parameters to maximize efficiency, or even achieve convergence. We propose self-adaptive stepsize rules that automatically tune PDHG parameters for optimal convergence. We rigorously analyze our methods, and identify convergence rates. Numerical experiments show that adaptive PDHG has strong advantages over non-adaptive methods in terms of both efficiency and simplicity for the user.\n",
      "----------- 163 ------------\n",
      "The mutual information is a core statistical quantity that has applications in all areas of machine learning, whether this is in training of density models over multiple data modalities, in maximising the efficiency of noisy transmission channels, or when learning behaviour policies for exploration by artificial agents. Most learning algorithms that involve optimisation of the mutual information rely on the Blahut-Arimoto algorithm --- an enumerative algorithm with exponential complexity that is not suitable for modern machine learning applications. This paper provides a new approach for scalable optimisation of the mutual information by merging techniques from variational inference and deep learning. We develop our approach by focusing on the problem of intrinsically-motivated learning, where the mutual information forms the definition of a well-known internal drive known as empowerment. Using a variational lower bound on the mutual information, combined with convolutional networks for handling visual input streams, we develop a stochastic optimisation algorithm that allows for scalable information maximisation and empowerment-based reasoning directly from pixels to actions.\n",
      "----------- 164 ------------\n",
      "The ability to combine known skills to create new ones may be crucial in the solution of complex reinforcement learning problems that unfold over extended periods. We argue that a robust way of combining skills is to define and manipulate them in the space of pseudo-rewards (or \"cumulants\"). Based on this premise, we propose a framework for combining skills using the formalism of options. We show that every deterministic option can be unambiguously represented as a cumulant defined in an extended domain. Building on this insight and on previous results on transfer learning, we show how to approximate options whose cumulants are linear combinations of the cumulants of known options. This means that, once we have learned options associated with a set of cumulants, we can instantaneously synthesise options induced by any linear combination of them, without any learning involved. We describe how this framework provides a hierarchical interface to the environment whose abstract actions correspond to combinations of basic skills. We demonstrate the practical benefits of our approach in a resource management problem and a navigation task involving a quadrupedal simulated robot.\n",
      "----------- 165 ------------\n",
      "Solving complex, temporally-extended tasks is a long-standing problem in reinforcement learning (RL). We hypothesize that one critical element of solving such problems is the notion of compositionality. With the ability to learn sub-skills that can be composed to solve longer tasks, i.e. hierarchical RL, we can acquire temporally-extended behaviors. However, acquiring effective yet general abstractions for hierarchical RL is remarkably challenging. In this paper, we propose to use language as the abstraction, as it provides unique compositional structure, enabling fast learning and combinatorial generalization, while retaining tremendous flexibility, making it suitable for a variety of problems. Our approach learns an instruction-following low-level policy and a high-level policy that can reuse abstractions across tasks, in essence, permitting agents to reason using structured language. To study compositional task learning, we introduce an open-source object interaction environment built using the MuJoCo physics engine and the CLEVR engine. We find that, using our approach, agents can learn to solve to diverse, temporally-extended tasks such as object sorting and multi-object rearrangement, including from raw pixel observations. Our analysis find that the compositional nature of language is critical for learning and systematically generalizing sub-skills in comparison to non-compositional abstractions that use the same supervision.\n",
      "----------- 166 ------------\n",
      "Deep neural networks have yielded superior performance in many contemporary applications. However, the gradient computation in a deep model with millions of instances leads to a lengthy training process even with modern GPU/TPU hardware acceleration. In this paper, we propose AutoAssist, a simple framework to accelerate training of a deep neural network. Typically, as the training procedure evolves, the amount of improvement by a stochastic gradient update varies dynamically with the choice of instances in the mini-batch. In AutoAssist, we utilize this fact and design an instance shrinking operation that is used to filter out instances with relatively low marginal improvement to the current model; thus the computationally intensive gradient computations are performed on informative instances as much as possible. Specifically, we train a very lightweight Assistant model jointly with the original deep network, which we refer to as Boss. The Assistant model is designed to gauge the importance of a given instance with respect to the current Boss such that the shrinking operation can be applied in the batch generator. With careful design, we train the Boss and Assistant in a nonblocking and asynchronous fashion such that overhead is minimal. To demonstrate the effectiveness of AutoAssist, we conduct experiments on two contemporary applications: image classification using ResNets with varied number of layers, and neural machine translation using LSTMs, ConvS2S and Transformer models. For each application, we verify that AutoAssist leads to significant reduction in training time; in particular, 30% to 40% of the total operation count can be reduced which leads to faster convergence and a corresponding decrease in training time.\n",
      "----------- 167 ------------\n",
      "In this paper we consider the problem of how a reinforcement learning agent that is tasked with solving a sequence of reinforcement learning problems (a sequence of Markov decision processes) can use knowledge acquired early in its lifetime to improve its ability to solve new problems. We argue that previous experience with similar problems can provide an agent with information about how it should explore when facing a new but related problem. We show that the search for an optimal exploration strategy can be formulated as a reinforcement learning problem itself and demonstrate that such strategy can leverage patterns found in the structure of related problems. We conclude with experiments that show the benefits of optimizing an exploration strategy using our proposed framework.\n",
      "----------- 168 ------------\n",
      "A dynamic treatment regime (DTR) consists of a sequence of decision rules, one per stage of intervention, that dictates how to determine the treatment assignment to patients based on evolving treatments and covariates' history. These regimes are particularly effective for managing chronic disorders and is arguably one of the key aspects towards more personalized decision-making. In this paper, we investigate the online reinforcement learning (RL) problem for selecting optimal DTRs provided that observational data is available. We develop the first adaptive algorithm that achieves near-optimal regret in DTRs in online settings, without any access to historical data. We further derive informative bounds on the system dynamics of the underlying DTR from confounded, observational data. Finally, we combine these results and develop a novel RL algorithm that efficiently learns the optimal DTR while leveraging the abundant, yet imperfect confounded observations.\n",
      "----------- 169 ------------\n",
      "Until recently, research on artificial neural networks was largely restricted to systems with only two types of variable: Neural activities that represent the current or recent input and weights that learn to capture regularities among inputs, outputs and payoffs. There is no good reason for this restriction. Synapses have dynamics at many different time-scales and this suggests that artificial neural networks might benefit from variables that change slower than activities but much faster than the standard weights. These\n",
      "fast weights'' can be used to store temporary memories of the recent past and they provide a neurally plausible way of implementing the type of attention to the past that has recently proven helpful in sequence-to-sequence models. By using fast weights we can avoid the need to store copies of neural activity patterns.\n",
      "----------- 170 ------------\n",
      "Point processes are powerful tools to model user activities and have a plethora of applications in social sciences. Predicting user activities based on point processes is a central problem. However, existing works are mostly problem specific, use heuristics, or simplify the stochastic nature of point processes. In this paper, we propose a framework that provides an unbiased estimator of the probability mass function of point processes. In particular, we design a key reformulation of the prediction problem, and further derive a differential-difference equation to compute a conditional probability mass function. Our framework is applicable to general point processes and prediction tasks, and achieves superb predictive and efficiency performance in diverse real-world applications compared to state-of-arts.\n",
      "----------- 171 ------------\n",
      "Scaling model capacity has been vital in the success of deep learning. For a typical network, necessary compute resources and training time grow dramatically with model size. Conditional computation is a promising way to increase the number of parameters with a relatively small increase in resources. We propose a training algorithm that flexibly chooses neural modules based on the data to be processed. Both the decomposition and modules are learned end-to-end. In contrast to existing approaches, training does not rely on regularization to enforce diversity in module use. We apply modular networks both to image recognition and language modeling tasks, where we achieve superior performance compared to several baselines. Introspection reveals that modules specialize in interpretable contexts.\n",
      "----------- 172 ------------\n",
      "Designing effective architectures is one of the key factors behind the success of deep neural networks. Existing deep architectures are either manually designed or automatically searched by some Neural Architecture Search (NAS) methods. However, even a well-searched architecture may still contain many non-significant or redundant modules or operations (e.g., convolution or pooling), which may not only incur substantial memory consumption and computation cost but also deteriorate the performance. Thus, it is necessary to optimize the operations inside an architecture to improve the performance without introducing extra computation cost. Unfortunately, such a constrained optimization problem is NP-hard. To make the problem feasible, we cast the optimization problem into a Markov decision process (MDP) and seek to learn a Neural Architecture Transformer (NAT) to replace the redundant operations with the more computationally efficient ones (e.g., skip connection or directly removing the connection). Based on MDP, we learn NAT by exploiting reinforcement learning to obtain the optimization policies w.r.t. different architectures. To verify the effectiveness of the proposed strategies, we apply NAT on both hand-crafted architectures and NAS based architectures. Extensive experiments on two benchmark datasets, i.e., CIFAR-10 and ImageNet, demonstrate that the transformed architecture by NAT significantly outperforms both its original form and those architectures optimized by existing methods.\n",
      "----------- 173 ------------\n",
      "Learning to make decisions from observed data in dynamic environments remains a problem of fundamental importance in a numbers of fields, from artificial intelligence and robotics, to medicine and finance. This paper concerns the problem of learning control policies for unknown linear dynamical systems so as to maximize a quadratic reward function. We present a method to optimize the expected value of the reward over the posterior distribution of the unknown system parameters, given data. The algorithm involves sequential convex programing, and enjoys reliable local convergence and robust stability guarantees. Numerical simulations and stabilization of a real-world inverted pendulum are used to demonstrate the approach, with strong performance and robustness properties observed in both.\n",
      "----------- 174 ------------\n",
      "Sum-product networks have recently emerged as an attractive representation due to their dual view as a special type of deep neural network with clear semantics and a special type of probabilistic graphical model for which inference is always tractable. Those properties follow from some conditions (i.e., completeness and decomposability) that must be respected by the structure of the network. As a result, it is not easy to specify a valid sum-product network by hand and therefore structure learning techniques are typically used in practice. This paper describes a new online structure learning technique for feed-forward and recurrent SPNs. The algorithm is demonstrated on real-world datasets with continuous features for which it is not clear what network architecture might be best, including sequence datasets of varying length.\n",
      "----------- 175 ------------\n",
      "Neural networks are a powerful class of nonlinear functions that can be trained end-to-end on various applications. While the over-parametrization nature in many neural networks renders the ability to fit complex functions and the strong representation power to handle challenging tasks, it also leads to highly correlated neurons that can hurt the generalization ability and incur unnecessary computation cost. As a result, how to regularize the network to avoid undesired representation redundancy becomes an important issue. To this end, we draw inspiration from a well-known problem in physics -- Thomson problem, where one seeks to find a state that distributes N electrons on a unit sphere as evenly as possible with minimum potential energy. In light of this intuition, we reduce the redundancy regularization problem to generic energy minimization, and propose a minimum hyperspherical energy (MHE) objective as generic regularization for neural networks. We also propose a few novel variants of MHE, and provide some insights from a theoretical point of view. Finally, we apply neural networks with MHE regularization to several challenging tasks. Extensive experiments demonstrate the effectiveness of our intuition, by showing the superior performance with MHE regularization.\n",
      "----------- 176 ------------\n",
      "The success of Deep Learning and its potential use in many safety-critical applications has motivated research on formal verification of Neural Network (NN) models. Despite the reputation of learned NN models to behave as black boxes and the theoretical hardness of proving their properties, researchers have been successful in verifying some classes of models by exploiting their piecewise linear structure and taking insights from formal methods such as Satisifiability Modulo Theory. These methods are however still far from scaling to realistic neural networks. To facilitate progress on this crucial area, we make two key contributions. First, we present a unified framework that encompasses previous methods. This analysis results in the identification of new methods that combine the strengths of multiple existing approaches, accomplishing a speedup of two orders of magnitude compared to the previous state of the art. Second, we propose a new data set of benchmarks which includes a collection of previously released testcases. We use the benchmark to provide the first experimental comparison of existing algorithms and identify the factors impacting the hardness of verification problems.\n",
      "----------- 177 ------------\n",
      "Reasoning about objects, relations, and physics is central to human intelligence, and a key goal of artificial intelligence. Here we introduce the interaction network, a model which can reason about how objects in complex systems interact, supporting dynamical predictions, as well as inferences about the abstract properties of the system. Our model takes graphs as input, performs object- and relation-centric reasoning in a way that is analogous to a simulation, and is implemented using deep neural networks. We evaluate its ability to reason about several challenging physical domains: n-body problems, rigid-body collision, and non-rigid dynamics. Our results show it can be trained to accurately simulate the physical trajectories of dozens of objects over thousands of time steps, estimate abstract quantities such as energy, and generalize automatically to systems with different numbers and configurations of objects and relations. Our interaction network implementation is the first general-purpose, learnable physics engine, and a powerful general framework for reasoning about object and relations in a wide variety of complex real-world domains.\n",
      "----------- 178 ------------\n",
      "Continual learning, the setting where a learning agent is faced with a never-ending stream of data, continues to be a great challenge for modern machine learning systems. In particular the online or \"single-pass through the data\" setting has gained attention recently as a natural setting that is difficult to tackle. Methods based on replay, either generative or from a stored memory, have been shown to be effective approaches for continual learning, matching or exceeding the state of the art in a number of standard benchmarks. These approaches typically rely on randomly selecting samples from the replay memory or from a generative model, which is suboptimal. In this work, we consider a controlled sampling of memories for replay. We retrieve the samples which are most interfered, i.e. whose prediction will be most negatively impacted by the foreseen parameters update. We show a formulation for this sampling criterion in both the generative replay and the experience replay setting, producing consistent gains in performance and greatly reduced forgetting. We release an implementation of our method at https://github.com/optimass/Maximally_Interfered_Retrieval\n",
      "----------- 179 ------------\n",
      "Convolutional neural networks have achieved great success in various vision tasks; however, they incur heavy resource costs. By using deeper and wider networks, network accuracy can be improved rapidly. However, in an environment with limited resources (e.g., mobile applications), heavy networks may not be usable. This study shows that naive convolution can be deconstructed into a shift operation and pointwise convolution. To cope with various convolutions, we propose a new shift operation called active shift layer (ASL) that formulates the amount of shift as a learnable function with shift parameters. This new layer can be optimized end-to-end through backpropagation and it can provide optimal shift values. Finally, we apply this layer to a light and fast network that surpasses existing state-of-the-art networks.\n",
      "----------- 180 ------------\n",
      "Accurately answering a question about a given image requires combining observations with general knowledge. While this is effortless for humans, reasoning with general knowledge remains an algorithmic challenge. To advance research in this direction a novel fact-based' visual question answering (FVQA) task has been introduced recently along with a large set of curated facts which link two entities, i.e., two possible answers, via a relation. Given a question-image pair, deep network techniques have been employed to successively reduce the large set of facts until one of the two entities of the final remaining fact is predicted as the answer. We observe that a successive process which considers one fact at a time to form a local decision is sub-optimal. Instead, we develop an entity graph and use a graph convolutional network to\n",
      "reason' about the correct answer by jointly considering all entities. We show on the challenging FVQA dataset that this leads to an improvement in accuracy of around 7% compared to the state-of-the-art.\n",
      "----------- 181 ------------\n",
      "Bayesian optimization (BO) is a successful methodology to optimize black-box functions that are expensive to evaluate. While traditional methods optimize each black-box function in isolation, there has been recent interest in speeding up BO by transferring knowledge across multiple related black-box functions. In this work, we introduce a method to automatically design the BO search space by relying on evaluations of previous black-box functions. We depart from the common practice of defining a set of arbitrary search ranges a priori by considering search space geometries that are learnt from historical data. This simple, yet effective strategy can be used to endow many existing BO methods with transfer learning properties. Despite its simplicity, we show that our approach considerably boosts BO by reducing the size of the search space, thus accelerating the optimization of a variety of black-box optimization problems. In particular, the proposed approach combined with random search results in a parameter-free, easy-to-implement, robust hyperparameter optimization strategy. We hope it will constitute a natural baseline for further research attempting to warm-start BO.\n",
      "----------- 182 ------------\n",
      "We study the problem of reducing test-time acquisition costs in classification systems. Our goal is to learn decision rules that adaptively select sensors for each example as necessary to make a confident prediction. We model our system as a directed acyclic graph (DAG) where internal nodes correspond to sensor subsets and decision functions at each node choose whether to acquire a new sensor or classify using the available measurements. This problem can be naturally posed as an empirical risk minimization over training data. Rather than jointly optimizing such a highly coupled and non-convex problem over all decision nodes, we propose an efficient algorithm motivated by dynamic programming. We learn node policies in the DAG by reducing the global objective to a series of cost sensitive learning problems. Our approach is computationally efficient and has proven guarantees of convergence to the optimal system for a fixed architecture. In addition, we present an extension to map other budgeted learning problems with large number of sensors to our DAG architecture and demonstrate empirical performance exceeding state-of-the-art algorithms for data composed of both few and many sensors.\n",
      "----------- 183 ------------\n",
      "Causal structure learning from time series data is a major scientific challenge. Existing algorithms assume that measurements occur sufficiently quickly; more precisely, they assume that the system and measurement timescales are approximately equal. In many scientific domains, however, measurements occur at a significantly slower rate than the underlying system changes. Moreover, the size of the mismatch between timescales is often unknown. This paper provides three distinct causal structure learning algorithms, all of which discover all dynamic graphs that could explain the observed measurement data as arising from undersampling at some rate. That is, these algorithms all learn causal structure without assuming any particular relation between the measurement and system timescales; they are thus rate-agnostic. We apply these algorithms to data from simulations. The results provide insight into the challenge of undersampling.\n",
      "----------- 184 ------------\n",
      "In real-world machine learning applications, data subsets correspond to especially critical outcomes: vulnerable cyclist detections are safety-critical in an autonomous driving task, and \"question\" sentences might be important to a dialogue agent's language understanding for product purposes. While machine learning models can achieve quality performance on coarse-grained metrics like F1-score and overall accuracy, they may underperform on these critical subsets---we define these as slices, the key abstraction in our approach. To address slice-level performance, practitioners often train separate \"expert\" models on slice subsets or use multi-task hard parameter sharing. We propose Slice-based Learning, a new programming model in which the slicing function (SF), a programmer abstraction, is used to specify additional model capacity for each slice. Any model can leverage SFs to learn slice-specific representations, which are combined with an attention mechanism to make slice-aware predictions. We show that our approach improves over baselines in terms of computational complexity and slice-specific performance by up to 19.0 points, and overall performance by up to 4.6 F1 points on applications spanning natural language understanding and computer vision benchmarks as well as production-scale industrial systems.\n",
      "----------- 185 ------------\n",
      "The history of learning for control has been an exciting back and forth between two broad classes of algorithms: planning and reinforcement learning. Planning algorithms effectively reason over long horizons, but assume access to a local policy and distance metric over collision-free paths. Reinforcement learning excels at learning policies and relative values of states, but fails to plan over long horizons. Despite the successes of each method on various tasks, long horizon, sparse reward tasks with high-dimensional observations remain exceedingly challenging for both planning and reinforcement learning algorithms. Frustratingly, these sorts of tasks are potentially the most useful, as they are simple to design (a human only need to provide an example goal state) and avoid injecting bias through reward shaping. We introduce a general-purpose control algorithm that combines the strengths of planning and reinforcement learning to effectively solve these tasks. Our main idea is to decompose the task of reaching a distant goal state into a sequence of easier tasks, each of which corresponds to reaching a particular subgoal. We use goal-conditioned RL to learn a policy to reach each waypoint and to learn a distance metric for search. Using graph search over our replay buffer, we can automatically generate this sequence of subgoals, even in image-based environments. Our algorithm, search on the replay buffer (SoRB), enables agents to solve sparse reward tasks over hundreds of steps, and generalizes substantially better than standard RL algorithms.\n",
      "----------- 186 ------------\n",
      "Machine learning is now being used to make crucial decisions about people's lives. For nearly all of these decisions there is a risk that individuals of a certain race, gender, sexual orientation, or any other subpopulation are unfairly discriminated against. Our recent method has demonstrated how to use techniques from counterfactual inference to make predictions fair across different subpopulations. This method requires that one provides the causal model that generated the data at hand. In general, validating all causal implications of the model is not possible without further assumptions. Hence, it is desirable to integrate competing causal models to provide counterfactually fair decisions, regardless of which causal \"world\" is the correct one. In this paper, we show how it is possible to make predictions that are approximately fair with respect to multiple possible causal models at once, thus mitigating the problem of exact causal specification. We frame the goal of learning a fair classifier as an optimization problem with fairness constraints entailed by competing causal explanations. We show how this optimization problem can be efficiently solved using gradient-based methods. We demonstrate the flexibility of our model on two real-world fair classification problems. We show that our model can seamlessly balance fairness in multiple worlds with prediction accuracy.\n",
      "----------- 187 ------------\n",
      "We develop methods for rapidly identifying important components of a convex optimization problem for the purpose of achieving fast convergence times. By considering a novel problem formulation—the minimization of a sum of piecewise functions—we describe a principled and general mechanism for exploiting piecewise linear structure in convex optimization. This result leads to a theoretically justified working set algorithm and a novel screening test, which generalize and improve upon many prior results on exploiting structure in convex optimization. In empirical comparisons, we study the scalability of our methods. We find that screening scales surprisingly poorly with the size of the problem, while our working set algorithm convincingly outperforms alternative approaches.\n",
      "----------- 188 ------------\n",
      "There are now several large scale deployments of differential privacy used to collect statistical information about users. However, these deployments periodically recollect the data and recompute the statistics using algorithms designed for a single use. As a result, these systems do not provide meaningful privacy guarantees over long time scales. Moreover, existing techniques to mitigate this effect do not apply in the\n",
      "local model'' of differential privacy that these systems use. In this paper, we introduce a new technique for local differential privacy that makes it possible to maintain up-to-date statistics over time, with privacy guarantees that degrade only in the number of changes in the underlying distribution rather than the number of collection periods. We use our technique for tracking a changing statistic in the setting where users are partitioned into an unknown collection of groups, and at every time period each user draws a single bit from a common (but changing) group-specific distribution. We also provide an application to frequency and heavy-hitter estimation.\n",
      "----------- 189 ------------\n",
      "Humanity faces numerous problems of common-pool resource appropriation. This class of multi-agent social dilemma includes the problems of ensuring sustainable use of fresh water, common fisheries, grazing pastures, and irrigation systems. Abstract models of common-pool resource appropriation based on non-cooperative game theory predict that self-interested agents will generally fail to find socially positive equilibria---a phenomenon called the tragedy of the commons. However, in reality, human societies are sometimes able to discover and implement stable cooperative solutions. Decades of behavioral game theory research have sought to uncover aspects of human behavior that make this possible. Most of that work was based on laboratory experiments where participants only make a single choice: how much to appropriate. Recognizing the importance of spatial and temporal resource dynamics, a recent trend has been toward experiments in more complex real-time video game-like environments. However, standard methods of non-cooperative game theory can no longer be used to generate predictions for this case. Here we show that deep reinforcement learning can be used instead. To that end, we study the emergent behavior of groups of independently learning agents in a partially observed Markov game modeling common-pool resource appropriation. Our experiments highlight the importance of trial-and-error learning in common-pool resource appropriation and shed light on the relationship between exclusion, sustainability, and inequality.\n",
      "----------- 190 ------------\n",
      "Practical applications of machine learning often involve successive training iterations with changes to features and training examples. Ideally, changes in the output of any new model should only be improvements (wins) over the previous iteration, but in practice the predictions may change neutrally for many examples, resulting in extra net-zero wins and losses, referred to as unnecessary churn. These changes in the predictions are problematic for usability for some applications, and make it harder and more expensive to measure if a change is statistically significant positive. In this paper, we formulate the problem and present a stabilization operator to regularize a classifier towards a previous classifier. We use a Markov chain Monte Carlo stabilization operator to produce a model with more consistent predictions without adversely affecting accuracy. We investigate the properties of the proposal with theoretical analysis. Experiments on benchmark datasets for different classification algorithms demonstrate the method and the resulting reduction in churn.\n",
      "----------- 191 ------------\n",
      "We introduce the Adaptive Skills, Adaptive Partitions (ASAP) framework that (1) learns skills (i.e., temporally extended actions or options) as well as (2) where to apply them. We believe that both (1) and (2) are necessary for a truly general skill learning framework, which is a key building block needed to scale up to lifelong learning agents. The ASAP framework is also able to solve related new tasks simply by adapting where it applies its existing learned skills. We prove that ASAP converges to a local optimum under natural conditions. Finally, our experimental results, which include a RoboCup domain, demonstrate the ability of ASAP to learn where to reuse skills as well as solve multiple tasks with considerably less experience than solving each task from scratch.\n",
      "----------- 192 ------------\n",
      "Much of model-based reinforcement learning involves learning a model of an agent's world, and training an agent to leverage this model to perform a task more efficiently. While these models are demonstrably useful for agents, every naturally occurring model of the world of which we are aware---e.g., a brain---arose as the byproduct of competing evolutionary pressures for survival, not minimization of a supervised forward-predictive loss via gradient descent. That useful models can arise out of the messy and slow optimization process of evolution suggests that forward-predictive modeling can arise as a side-effect of optimization under the right circumstances. Crucially, this optimization process need not explicitly be a forward-predictive loss. In this work, we introduce a modification to traditional reinforcement learning which we call observational dropout, whereby we limit the agents ability to observe the real environment at each timestep. In doing so, we can coerce an agent into learning a world model to fill in the observation gaps during reinforcement learning. We show that the emerged world model, while not explicitly trained to predict the future, can help the agent learn key skills required to perform well in its environment. Videos of our results available at https://learningtopredict.github.io/\n",
      "----------- 193 ------------\n",
      "We consider the problem of multi-objective (MO) blackbox optimization using expensive function evaluations, where the goal is to approximate the true Pareto-set of solutions by minimizing the number of function evaluations. For example, in hardware design optimization, we need to find the designs that trade-off performance, energy, and area overhead using expensive simulations. We propose a novel approach referred to as Max-value Entropy Search for Multi-objective Optimization (MESMO) to solve this problem. MESMO employs an output-space entropy based acquisition function to efficiently select the sequence of inputs for evaluation for quickly uncovering high-quality solutions. We also provide theoretical analysis to characterize the efficacy of MESMO. Our experiments on several synthetic and real-world benchmark problems show that MESMO consistently outperforms state-of-the-art algorithms.\n",
      "----------- 194 ------------\n",
      "With the rapid growth of image and video data on the web, hashing has been extensively studied for image or video search in recent years. Benefiting from recent advances in deep learning, deep hashing methods have achieved promising results for image retrieval. However, there are some limitations of previous deep hashing methods (e.g., the semantic information is not fully exploited). In this paper, we develop a deep supervised discrete hashing algorithm based on the assumption that the learned binary codes should be ideal for classification. Both the pairwise label information and the classification information are used to learn the hash codes within one stream framework. We constrain the outputs of the last layer to be binary codes directly, which is rarely investigated in deep hashing algorithm. Because of the discrete nature of hash codes, an alternating minimization method is used to optimize the objective function. Experimental results have shown that our method outperforms current state-of-the-art methods on benchmark datasets.\n",
      "----------- 195 ------------\n",
      "There is significant recent interest to parallelize deep learning algorithms in order to handle the enormous growth in data and model sizes. While most advances focus on model parallelization and engaging multiple computing agents via using a central parameter server, aspect of data parallelization along with decentralized computation has not been explored sufficiently. In this context, this paper presents a new consensus-based distributed SGD (CDSGD) (and its momentum variant, CDMSGD) algorithm for collaborative deep learning over fixed topology networks that enables data parallelization as well as decentralized computation. Such a framework can be extremely useful for learning agents with access to only local/private data in a communication constrained environment. We analyze the convergence properties of the proposed algorithm with strongly convex and nonconvex objective functions with fixed and diminishing step sizes using concepts of Lyapunov function construction. We demonstrate the efficacy of our algorithms in comparison with the baseline centralized SGD and the recently proposed federated averaging algorithm (that also enables data parallelism) based on benchmark datasets such as MNIST, CIFAR-10 and CIFAR-100.\n",
      "----------- 196 ------------\n",
      "Digital crowdsourcing (CS) is a modern approach to perform certain large projects using small contributions of a large crowd. In CS, a taskmaster typically breaks down the project into small batches of tasks and assigns them to so-called workers with imperfect skill levels. The crowdsourcer then collects and analyzes the results for inference and serving the purpose of the project. In this work, the CS problem, as a human-in-the-loop computation problem, is modeled and analyzed in an information theoretic rate-distortion framework. The purpose is to identify the ultimate fidelity that one can achieve by any form of query from the crowd and any decoding (inference) algorithm with a given budget. The results are established by a joint source channel (de)coding scheme, which represent the query scheme and inference, over parallel noisy channels, which model workers with imperfect skill levels. We also present and analyze a query scheme dubbed k-ary incidence coding and study optimized query pricing in this setting.\n",
      "----------- 197 ------------\n",
      "We present Park, a platform for researchers to experiment with Reinforcement Learning (RL) for computer systems. Using RL for improving the performance of systems has a lot of potential, but is also in many ways very different from, for example, using RL for games. Thus, in this work we first discuss the unique challenges RL for systems has, and then propose Park an open extensible platform, which makes it easier for ML researchers to work on systems problems. Currently, Park consists of 12 real world system-centric optimization problems with one common easy to use interface. Finally, we present the performance of existing RL approaches over those 12 problems and outline potential areas of future work.\n",
      "----------- 198 ------------\n",
      "Real-world applications often combine learning and optimization problems on graphs. For instance, our objective may be to cluster the graph in order to detect meaningful communities (or solve other common graph optimization problems such as facility location, maxcut, and so on). However, graphs or related attributes are often only partially observed, introducing learning problems such as link prediction which must be solved prior to optimization. Standard approaches treat learning and optimization entirely separately, while recent machine learning work aims to predict the optimal solution directly from the inputs. Here, we propose an alternative decision-focused learning approach that integrates a differentiable proxy for common graph optimization problems as a layer in learned systems. The main idea is to learn a representation that maps the original optimization problem onto a simpler proxy problem that can be efficiently differentiated through. Experimental results show that our ClusterNet system outperforms both pure end-to-end approaches (that directly predict the optimal solution) and standard approaches that entirely separate learning and optimization. Code for our system is available at https://github.com/bwilder0/clusternet.\n",
      "----------- 199 ------------\n",
      "A core capability of intelligent systems is the ability to quickly learn new tasks by drawing on prior experience. Gradient (or optimization) based meta-learning has recently emerged as an effective approach for few-shot learning. In this formulation, meta-parameters are learned in the outer loop, while task-specific models are learned in the inner-loop, by using only a small amount of data from the current task. A key challenge in scaling these approaches is the need to differentiate through the inner loop learning process, which can impose considerable computational and memory burdens. By drawing upon implicit differentiation, we develop the implicit MAML algorithm, which depends only on the solution to the inner level optimization and not the path taken by the inner loop optimizer. This effectively decouples the meta-gradient computation from the choice of inner loop optimizer. As a result, our approach is agnostic to the choice of inner loop optimizer and can gracefully handle many gradient steps without vanishing gradients or memory constraints. Theoretically, we prove that implicit MAML can compute accurate meta-gradients with a memory footprint that is, up to small constant factors, no more than that which is required to compute a single inner loop gradient and at no overall increase in the total computational cost. Experimentally, we show that these benefits of implicit MAML translate into empirical gains on few-shot image recognition benchmarks.\n",
      "----------- 200 ------------\n",
      "In many real-world reinforcement learning (RL) problems, besides optimizing the main objective function, an agent must concurrently avoid violating a number of constraints. In particular, besides optimizing performance, it is crucial to guarantee the safety of an agent during training as well as deployment (e.g., a robot should avoid taking actions - exploratory or not - which irrevocably harm its hard- ware). To incorporate safety in RL, we derive algorithms under the framework of constrained Markov decision processes (CMDPs), an extension of the standard Markov decision processes (MDPs) augmented with constraints on expected cumulative costs. Our approach hinges on a novel Lyapunov method. We define and present a method for constructing Lyapunov functions, which provide an effective way to guarantee the global safety of a behavior policy during training via a set of local linear constraints. Leveraging these theoretical underpinnings, we show how to use the Lyapunov approach to systematically transform dynamic programming (DP) and RL algorithms into their safe counterparts. To illustrate their effectiveness, we evaluate these algorithms in several CMDP planning and decision-making tasks on a safety benchmark domain. Our results show that our proposed method significantly outperforms existing baselines in balancing constraint satisfaction and performance.\n",
      "----------- 201 ------------\n",
      "Neural networks augmented with external memory have the ability to learn algorithmic solutions to complex tasks. These models appear promising for applications such as language modeling and machine translation. However, they scale poorly in both space and time as the amount of memory grows --- limiting their applicability to real-world domains. Here, we present an end-to-end differentiable memory access scheme, which we call Sparse Access Memory (SAM), that retains the representational power of the original approaches whilst training efficiently with very large memories. We show that SAM achieves asymptotic lower bounds in space and time complexity, and find that an implementation runs 1,\\!000\\times\n",
      "faster and with 3,\\!000\\times\n",
      "less physical memory than non-sparse models. SAM learns with comparable data efficiency to existing models on a range of synthetic tasks and one-shot Omniglot character recognition, and can scale to tasks requiring 100,\\!000\n",
      "s of time steps and memories. As well, we show how our approach can be adapted for models that maintain temporal associations between memories, as with the recently introduced Differentiable Neural Computer.\n",
      "----------- 202 ------------\n",
      "Information diffusion in online social networks is affected by the underlying network topology, but it also has the power to change it. Online users are constantly creating new links when exposed to new information sources, and in turn these links are alternating the way information spreads. However, these two highly intertwined stochastic processes, information diffusion and network evolution, have been predominantly studied separately, ignoring their co-evolutionary dynamics.We propose a temporal point process model, COEVOLVE, for such joint dynamics, allowing the intensity of one process to be modulated by that of the other. This model allows us to efficiently simulate interleaved diffusion and network events, and generate traces obeying common diffusion and network patterns observed in real-world networks. Furthermore, we also develop a convex optimization framework to learn the parameters of the model from historical diffusion and network evolution traces. We experimented with both synthetic data and data gathered from Twitter, and show that our model provides a good fit to the data as well as more accurate predictions than alternatives.\n",
      "----------- 203 ------------\n",
      "Protein modeling is an increasingly popular area of machine learning research. Semi-supervised learning has emerged as an important paradigm in protein modeling due to the high cost of acquiring supervised protein labels, but the current literature is fragmented when it comes to datasets and standardized evaluation techniques. To facilitate progress in this field, we introduce the Tasks Assessing Protein Embeddings (TAPE), a set of five biologically relevant semi-supervised learning tasks spread across different domains of protein biology. We curate tasks into specific training, validation, and test splits to ensure that each task tests biologically relevant generalization that transfers to real-life scenarios. We benchmark a range of approaches to semi-supervised protein representation learning, which span recent work as well as canonical sequence learning techniques. We find that self-supervised pretraining is helpful for almost all models on all tasks, more than doubling performance in some cases. Despite this increase, in several cases features learned by self-supervised pretraining still lag behind features extracted by state-of-the-art non-neural techniques. This gap in performance suggests a huge opportunity for innovative architecture design and improved modeling paradigms that better capture the signal in biological sequences. TAPE will help the machine learning community focus effort on scientifically relevant problems. Toward this end, all data and code used to run these experiments is available at https://github.com/songlab-cal/tape\n",
      "----------- 204 ------------\n",
      "Compression and computational efficiency in deep learning have become a problem of great significance. In this work, we argue that the most principled and effective way to attack this problem is by adopting a Bayesian point of view, where through sparsity inducing priors we prune large parts of the network. We introduce two novelties in this paper: 1) we use hierarchical priors to prune nodes instead of individual weights, and 2) we use the posterior uncertainties to determine the optimal fixed point precision to encode the weights. Both factors significantly contribute to achieving the state of the art in terms of compression rates, while still staying competitive with methods designed to optimize for speed or energy efficiency.\n",
      "----------- 205 ------------\n",
      "Throughout the past five years, the susceptibility of neural networks to minimal adversarial perturbations has moved from a peculiar phenomenon to a core issue in Deep Learning. Despite much attention, however, progress towards more robust models is significantly impaired by the difficulty of evaluating the robustness of neural network models. Today's methods are either fast but brittle (gradient-based attacks), or they are fairly reliable but slow (score- and decision-based attacks). We here develop a new set of gradient-based adversarial attacks which (a) are more reliable in the face of gradient-masking than other gradient-based attacks, (b) perform better and are more query efficient than current state-of-the-art gradient-based attacks, (c) can be flexibly adapted to a wide range of adversarial criteria and (d) require virtually no hyperparameter tuning. These findings are carefully validated across a diverse set of six different models and hold for L0, L1, L2 and Linf in both targeted as well as untargeted scenarios. Implementations will soon be available in all major toolboxes (Foolbox, CleverHans and ART). We hope that this class of attacks will make robustness evaluations easier and more reliable, thus contributing to more signal in the search for more robust machine learning models.\n",
      "----------- 206 ------------\n",
      "A fundamental problem in program verification concerns inferring loop invariants. The problem is undecidable and even practical instances are challenging. Inspired by how human experts construct loop invariants, we propose a reasoning framework Code2Inv that constructs the solution by multi-step decision making and querying an external program graph memory block. By training with reinforcement learning, Code2Inv captures rich program features and avoids the need for ground truth solutions as supervision. Compared to previous learning tasks in domains with graph-structured data, it addresses unique challenges, such as a binary objective function and an extremely sparse reward that is given by an automated theorem prover only after the complete loop invariant is proposed. We evaluate Code2Inv on a suite of 133 benchmark problems and compare it to three state-of-the-art systems. It solves 106 problems compared to 73 by a stochastic search-based system, 77 by a heuristic search-based system, and 100 by a decision tree learning-based system. Moreover, the strategy learned can be generalized to new programs: compared to solving new instances from scratch, the pre-trained agent is more sample efficient in finding solutions.\n",
      "----------- 207 ------------\n",
      "In interactive machine learning (IML), we iteratively make decisions and obtain noisy observations of an unknown function. While IML methods, e.g., Bayesian optimization and active learning, have been successful in applications, on real-world systems they must provably avoid unsafe decisions. To this end, safe IML algorithms must carefully learn about a priori unknown constraints without making unsafe decisions. Existing algorithms for this problem learn about the safety of all decisions to ensure convergence. This is sample-inefficient, as it explores decisions that are not relevant for the original IML objective. In this paper, we introduce a novel framework that renders any existing unsafe IML algorithm safe. Our method works as an add-on that takes suggested decisions as input and exploits regularity assumptions in terms of a Gaussian process prior in order to efficiently learn about their safety. As a result, we only explore the safe set when necessary for the IML problem. We apply our framework to safe Bayesian optimization and to safe exploration in deterministic Markov Decision Processes (MDP), which have been analyzed separately before. Our method outperforms other algorithms empirically.\n",
      "----------- 208 ------------\n",
      "Differential privacy has emerged as the main definition for private data analysis and machine learning. The global model of differential privacy, which assumes that users trust the data collector, provides strong privacy guarantees and introduces small errors in the output. In contrast, applications of differential privacy in commercial systems by Apple, Google, and Microsoft, use the local model. Here, users do not trust the data collector, and hence randomize their data before sending it to the data collector. Unfortunately, local model is too strong for several important applications and hence is limited in its applicability. In this work, we propose a framework based on trusted processors and a new definition of differential privacy called Oblivious Differential Privacy, which combines the best of both local and global models. The algorithms we design in this framework show interesting interplay of ideas from the streaming algorithms, oblivious algorithms, and differential privacy.\n",
      "----------- 209 ------------\n",
      "We collect a large real-world test set, ObjectNet, for object recognition with controls where object backgrounds, rotations, and imaging viewpoints are random. Most scientific experiments have controls, confounds which are removed from the data, to ensure that subjects cannot perform a task by exploiting trivial correlations in the data. Historically, large machine learning and computer vision datasets have lacked such controls. This has resulted in models that must be fine-tuned for new datasets and perform better on datasets than in real-world applications. When tested on ObjectNet, object detectors show a 40-45% drop in performance, with respect to their performance on other benchmarks, due to the controls for biases. Controls make ObjectNet robust to fine-tuning showing only small performance increases. We develop a highly automated platform that enables gathering datasets with controls by crowdsourcing image capturing and annotation. ObjectNet is the same size as the ImageNet test set (50,000 images), and by design does not come paired with a training set in order to encourage generalization. The dataset is both easier than ImageNet (objects are largely centered and unoccluded) and harder (due to the controls). Although we focus on object recognition here, data with controls can be gathered at scale using automated tools throughout machine learning to generate datasets that exercise models in new ways thus providing valuable feedback to researchers. This work opens up new avenues for research in generalizable, robust, and more human-like computer vision and in creating datasets where results are predictive of real-world performance.\n",
      "----------- 210 ------------\n",
      "Most deep reinforcement learning algorithms are data inefficient in complex and rich environments, limiting their applicability to many scenarios. One direction for improving data efficiency is multitask learning with shared neural network parameters, where efficiency may be improved through transfer across related tasks. In practice, however, this is not usually observed, because gradients from different tasks can interfere negatively, making learning unstable and sometimes even less data efficient. Another issue is the different reward schemes between tasks, which can easily lead to one task dominating the learning of a shared model. We propose a new approach for joint training of multiple tasks, which we refer to as Distral (DIStill & TRAnsfer Learning). Instead of sharing parameters between the different workers, we propose to share a distilled policy that captures common behaviour across tasks. Each worker is trained to solve its own task while constrained to stay close to the shared policy, while the shared policy is trained by distillation to be the centroid of all task policies. Both aspects of the learning process are derived by optimizing a joint objective function. We show that our approach supports efficient transfer on complex 3D environments, outperforming several related methods. Moreover, the proposed learning process is more robust and more stable---attributes that are critical in deep reinforcement learning.\n",
      "----------- 211 ------------\n",
      "We study a novel problem lying at the intersection of two areas: multi-armed bandit and outlier detection. Multi-armed bandit is a useful tool to model the process of incrementally collecting data for multiple objects in a decision space. Outlier detection is a powerful method to narrow down the attention to a few objects after the data for them are collected. However, no one has studied how to detect outlier objects while incrementally collecting data for them, which is necessary when data collection is expensive. We formalize this problem as identifying outlier arms in a multi-armed bandit. We propose two sampling strategies with theoretical guarantee, and analyze their sampling efficiency. Our experimental results on both synthetic and real data show that our solution saves 70-99% of data collection cost from baseline while having nearly perfect accuracy.\n",
      "----------- 212 ------------\n",
      "Learning latent representations from long text sequences is an important first step in many natural language processing applications. Recurrent Neural Networks (RNNs) have become a cornerstone for this challenging task. However, the quality of sentences during RNN-based decoding (reconstruction) decreases with the length of the text. We propose a sequence-to-sequence, purely convolutional and deconvolutional autoencoding framework that is free of the above issue, while also being computationally efficient. The proposed method is simple, easy to implement and can be leveraged as a building block for many applications. We show empirically that compared to RNNs, our framework is better at reconstructing and correcting long paragraphs. Quantitative evaluation on semi-supervised text classification and summarization tasks demonstrate the potential for better utilization of long unlabeled text data.\n",
      "----------- 213 ------------\n",
      "Uplift modeling is aimed at estimating the incremental impact of an action on an individual's behavior, which is useful in various application domains such as targeted marketing (advertisement campaigns) and personalized medicine (medical treatments). Conventional methods of uplift modeling require every instance to be jointly equipped with two types of labels: the taken action and its outcome. However, obtaining two labels for each instance at the same time is difficult or expensive in many real-world problems. In this paper, we propose a novel method of uplift modeling that is applicable to a more practical setting where only one type of labels is available for each instance. We show a mean squared error bound for the proposed estimator and demonstrate its effectiveness through experiments.\n",
      "----------- 214 ------------\n",
      "Training deep neural networks requires an exorbitant amount of computation resources, including a heterogeneous mix of GPU and CPU devices. It is critical to place operations in a neural network on these devices in an optimal way, so that the training process can complete within the shortest amount of time. The state-of-the-art uses reinforcement learning to learn placement skills by repeatedly performing Monte-Carlo experiments. However, due to its equal treatment of placement samples, we argue that there remains ample room for significant improvements. In this paper, we propose a new joint learning algorithm, called Post, that integrates cross-entropy minimization and proximal policy optimization to achieve theoretically guaranteed optimal efficiency. In order to incorporate the cross-entropy method as a sampling technique, we propose to represent placements using discrete probability distributions, which allows us to estimate an optimal probability mass by maximal likelihood estimation, a powerful tool with the best possible efficiency. We have implemented Post in the Google Cloud platform, and our extensive experiments with several popular neural network training benchmarks have demonstrated clear evidence of superior performance: with the same amount of learning time, it leads to placements that have training times up to 63.7% shorter over the state-of-the-art.\n",
      "----------- 215 ------------\n",
      "Calcium imaging is a prominent technology in neuroscience research which allows for simultaneous recording of large numbers of neurons in awake animals. Automated extraction of neurons and their temporal activity from imaging datasets is an important step in the path to producing neuroscience results. However, nearly all imaging datasets contain gross contaminating sources which could originate from the technology used, or the underlying biological tissue. Although past work has considered the effects of contamination under limited circumstances, there has not been a general framework treating contamination and its effects on the statistical estimation of calcium signals. In this work, we proceed in a new direction and propose to extract cells and their activity using robust statistical estimation. Using the theory of M-estimation, we derive a minimax optimal robust loss, and also find a simple and practical optimization routine for this loss with provably fast convergence. We use our proposed robust loss in a matrix factorization framework to extract the neurons and their temporal activity in calcium imaging datasets. We demonstrate the superiority of our robust estimation approach over existing methods on both simulated and real datasets.\n",
      "----------- 216 ------------\n",
      "Analyzing the structure and function of proteins is a key part of understanding biology at the molecular and cellular level. In addition, a major engineering challenge is to design new proteins in a principled and methodical way. Current computational modeling methods for protein design are slow and often require human oversight and intervention. Here, we apply Generative Adversarial Networks (GANs) to the task of generating protein structures, toward application in fast de novo protein design. We encode protein structures in terms of pairwise distances between alpha-carbons on the protein backbone, which eliminates the need for the generative model to learn translational and rotational symmetries. We then introduce a convex formulation of corruption-robust 3D structure recovery to fold the protein structures from generated pairwise distance maps, and solve these problems using the Alternating Direction Method of Multipliers. We test the effectiveness of our models by predicting completions of corrupted protein structures and show that the method is capable of quickly producing structurally plausible solutions.\n",
      "----------- 217 ------------\n",
      "Recurrent neural networks (RNNs) are a widely used tool for modeling sequential data, yet they are often treated as inscrutable black boxes. Given a trained recurrent network, we would like to reverse engineer it--to obtain a quantitative, interpretable description of how it solves a particular task. Even for simple tasks, a detailed understanding of how recurrent networks work, or a prescription for how to develop such an understanding, remains elusive. In this work, we use tools from dynamical systems analysis to reverse engineer recurrent networks trained to perform sentiment classification, a foundational natural language processing task. Given a trained network, we find fixed points of the recurrent dynamics and linearize the nonlinear system around these fixed points. Despite their theoretical capacity to implement complex, high-dimensional computations, we find that trained networks converge to highly interpretable, low-dimensional representations. In particular, the topological structure of the fixed points and corresponding linearized dynamics reveal an approximate line attractor within the RNN, which we can use to quantitatively understand how the RNN solves the sentiment analysis task. Finally, we find this mechanism present across RNN architectures (including LSTMs, GRUs, and vanilla RNNs) trained on multiple datasets, suggesting that our findings are not unique to a particular architecture or dataset. Overall, these results demonstrate that surprisingly universal and human interpretable computations can arise across a range of recurrent networks.\n",
      "----------- 218 ------------\n",
      "Active search is a learning paradigm for actively identifying as many members of a given class as possible. A critical target scenario is high-throughput screening for scientific discovery, such as drug or materials discovery. In these settings, specialized instruments can often evaluate \\emph{multiple} points simultaneously; however, all existing work on active search focuses on sequential acquisition. We bridge this gap, addressing batch active search from both the theoretical and practical perspective. We first derive the Bayesian optimal policy for this problem, then prove a lower bound on the performance gap between sequential and batch optimal policies: the\n",
      "cost of parallelization.'' We also propose novel, efficient batch policies inspired by state-of-the-art sequential policies, and develop an aggressive pruning technique that can dramatically speed up computation. We conduct thorough experiments on data from three application domains: a citation network, material science, and drug discovery, testing all proposed policies (14 total) with a wide range of batch sizes. Our results demonstrate that the empirical performance gap matches our theoretical bound, that nonmyopic policies usually significantly outperform myopic alternatives, and that diversity is an important consideration for batch policy design.\n",
      "----------- 219 ------------\n",
      "A central problem in dynamical system modeling is state discovery—that is, finding a compact summary of the past that captures the information needed to predict the future. Predictive State Representations (PSRs) enable clever spectral methods for state discovery; however, while consistent in the limit of infinite data, these methods often suffer from poor performance in the low data regime. In this paper we develop a novel algorithm for incorporating domain knowledge, in the form of an imperfect state representation, as side information to speed spectral learning for PSRs. We prove theoretical results characterizing the relevance of a user-provided state representation, and design spectral algorithms that can take advantage of a relevant representation. Our algorithm utilizes principal angles to extract the relevant components of the representation, and is robust to misspecification. Empirical evaluation on synthetic HMMs, an aircraft identification domain, and a gene splice dataset shows that, even with weak domain knowledge, the algorithm can significantly outperform standard PSR learning.\n",
      "----------- 220 ------------\n",
      "In standard reinforcement learning (RL), a learning agent seeks to optimize the overall reward. However, many key aspects of a desired behavior are more naturally expressed as constraints. For instance, the designer may want to limit the use of unsafe actions, increase the diversity of trajectories to enable exploration, or approximate expert trajectories when rewards are sparse. In this paper, we propose an algorithmic scheme that can handle a wide class of constraints in RL tasks: specifically, any constraints that require expected values of some vector measurements (such as the use of an action) to lie in a convex set. This captures previously studied constraints (such as safety and proximity to an expert), but also enables new classes of constraints (such as diversity). Our approach comes with rigorous theoretical guarantees and only relies on the ability to approximately solve standard RL tasks. As a result, it can be easily adapted to work with any model-free or model-based RL. In our experiments, we show that it matches previous algorithms that enforce safety via constraints, but can also enforce new properties that these algorithms do not incorporate, such as diversity.\n",
      "----------- 221 ------------\n",
      "Neural codes are inevitably shaped by various kinds of biological constraints, \\emph{e.g.} noise and metabolic cost. Here we formulate a coding framework which explicitly deals with noise and the metabolic costs associated with the neural representation of information, and analytically derive the optimal neural code for monotonic response functions and arbitrary stimulus distributions. For a single neuron, the theory predicts a family of optimal response functions depending on the metabolic budget and noise characteristics. Interestingly, the well-known histogram equalization solution can be viewed as a special case when metabolic resources are unlimited. For a pair of neurons, our theory suggests that under more severe metabolic constraints, ON-OFF coding is an increasingly more efficient coding scheme compared to ON-ON or OFF-OFF. The advantage could be as large as one-fold, substantially larger than the previous estimation. Some of these predictions could be generalized to the case of large neural populations. In particular, these analytical results may provide a theoretical basis for the predominant segregation into ON- and OFF-cells in early visual processing areas. Overall, we provide a unified framework for optimal neural codes with monotonic tuning curves in the brain, and makes predictions that can be directly tested with physiology experiments.\n",
      "----------- 222 ------------\n",
      "The amount of data available in the world is growing faster than our ability to deal with it. However, if we take advantage of the internal structure, data may become much smaller for machine learning purposes. In this paper we focus on one of the fundamental machine learning tasks, empirical risk minimization (ERM), and provide faster algorithms with the help from the clustering structure of the data. We introduce a simple notion of raw clustering that can be efficiently computed from the data, and propose two algorithms based on clustering information. Our accelerated algorithm ClusterACDM is built on a novel Haar transformation applied to the dual space of the ERM problem, and our variance-reduction based algorithm ClusterSVRG introduces a new gradient estimator using clustering. Our algorithms outperform their classical counterparts ACDM and SVRG respectively.\n",
      "----------- 223 ------------\n",
      "We connect a broad class of generative models through their shared reliance on sequential decision making. Motivated by this view, we develop extensions to an existing model, and then explore the idea further in the context of data imputation -- perhaps the simplest setting in which to investigate the relation between unconditional and conditional generative modelling. We formulate data imputation as an MDP and develop models capable of representing effective policies for it. We construct the models using neural networks and train them using a form of guided policy search. Our models generate predictions through an iterative process of feedback and refinement. We show that this approach can learn effective policies for imputation problems of varying difficulty and across multiple datasets.\n",
      "----------- 224 ------------\n",
      "Spectral inference provides fast algorithms and provable optimality for latent topic analysis. But for real data these algorithms require additional ad-hoc heuristics, and even then often produce unusable results. We explain this poor performance by casting the problem of topic inference in the framework of Joint Stochastic Matrix Factorization (JSMF) and showing that previous methods violate the theoretical conditions necessary for a good solution to exist. We then propose a novel rectification method that learns high quality topics and their interactions even on small, noisy data. This method achieves results comparable to probabilistic techniques in several domains while maintaining scalability and provable optimality.\n",
      "----------- 225 ------------\n",
      "Large labeled training sets are the critical building blocks of supervised learning methods and are key enablers of deep learning techniques. For some applications, creating labeled training sets is the most time-consuming and expensive part of applying machine learning. We therefore propose a paradigm for the programmatic creation of training sets called data programming in which users provide a set of labeling functions, which are programs that heuristically label subsets of the data, but that are noisy and may conflict. By viewing these labeling functions as implicitly describing a generative model for this noise, we show that we can recover the parameters of this model to \"denoise\" the generated training set, and establish theoretically that we can recover the parameters of these generative models in a handful of settings. We then show how to modify a discriminative loss function to make it noise-aware, and demonstrate our method over a range of discriminative models including logistic regression and LSTMs. Experimentally, on the 2014 TAC-KBP Slot Filling challenge, we show that data programming would have led to a new winning score, and also show that applying data programming to an LSTM model leads to a TAC-KBP score almost 6 F1 points over a state-of-the-art LSTM baseline (and into second place in the competition). Additionally, in initial user studies we observed that data programming may be an easier way for non-experts to create machine learning models when training data is limited or unavailable.\n",
      "----------- 226 ------------\n",
      "In the recent years, a number of parameter-free algorithms have been developed for online linear optimization over Hilbert spaces and for learning with expert advice. These algorithms achieve optimal regret bounds that depend on the unknown competitors, without having to tune the learning rates with oracle choices. We present a new intuitive framework to design parameter-free algorithms for both online linear optimization over Hilbert spaces and for learning with expert advice, based on reductions to betting on outcomes of adversarial coins. We instantiate it using a betting algorithm based on the Krichevsky-Trofimov estimator. The resulting algorithms are simple, with no parameters to be tuned, and they improve or match previous results in terms of regret guarantee and per-round complexity.\n",
      "----------- 227 ------------\n",
      "Reinforcement learning is a promising approach for learning control policies for robot tasks. However, specifying complex tasks (e.g., with multiple objectives and safety constraints) can be challenging, since the user must design a reward function that encodes the entire task. Furthermore, the user often needs to manually shape the reward to ensure convergence of the learning algorithm. We propose a language for specifying complex control tasks, along with an algorithm that compiles specifications in our language into a reward function and automatically performs reward shaping. We implement our approach in a tool called SPECTRL, and show that it outperforms several state-of-the-art baselines.\n",
      "----------- 228 ------------\n",
      "Deep neural networks (DNNs) have demonstrated state-of-the-art results on many pattern recognition tasks, especially vision classification problems. Understanding the inner workings of such computational brains is both fascinating basic science that is interesting in its own right---similar to why we study the human brain---and will enable researchers to further improve DNNs. One path to understanding how a neural network functions internally is to study what each of its neurons has learned to detect. One such method is called activation maximization, which synthesizes an input (e.g. an image) that highly activates a neuron. Here we dramatically improve the qualitative state of the art of activation maximization by harnessing a powerful, learned prior: a deep generator network. The algorithm (1) generates qualitatively state-of-the-art synthetic images that look almost real, (2) reveals the features learned by each neuron in an interpretable way, (3) generalizes well to new datasets and somewhat well to different network architectures without requiring the prior to be relearned, and (4) can be considered as a high-quality generative method (in this case, by generating novel, creative, interesting, recognizable images).\n",
      "----------- 229 ------------\n",
      "We study the problem of modeling spatiotemporal trajectories over long time horizons using expert demonstrations. For instance, in sports, agents often choose action sequences with long-term goals in mind, such as achieving a certain strategic position. Conventional policy learning approaches, such as those based on Markov decision processes, generally fail at learning cohesive long-term behavior in such high-dimensional state spaces, and are only effective when fairly myopic decision-making yields the desired behavior. The key difficulty is that conventional models are\n",
      "single-scale'' and only learn a single state-action policy. We instead propose a hierarchical policy class that automatically reasons about both long-term and short-term goals, which we instantiate as a hierarchical neural network. We showcase our approach in a case study on learning to imitate demonstrated basketball trajectories, and show that it generates significantly more realistic trajectories compared to non-hierarchical baselines as judged by professional sports analysts.\n",
      "----------- 230 ------------\n",
      "By making personalized suggestions, a recommender system is playing a crucial role in improving the engagement of users in modern web-services. However, most recommendation algorithms do not explicitly take into account the temporal behavior and the recurrent activities of users. Two central but less explored questions are how to recommend the most desirable item \\emph{at the right moment}, and how to predict \\emph{the next returning time} of a user to a service. To address these questions, we propose a novel framework which connects self-exciting point processes and low-rank models to capture the recurrent temporal patterns in a large collection of user-item consumption pairs. We show that the parameters of the model can be estimated via a convex optimization, and furthermore, we develop an efficient algorithm that maintains O(1 / \\epsilon)\n",
      "convergence rate, scales up to problems with millions of user-item pairs and thousands of millions of temporal events. Compared to other state-of-the-arts in both synthetic and real datasets, our model achieves superb predictive performance in the two time-sensitive recommendation questions. Finally, we point out that our formulation can incorporate other extra context information of users, such as profile, textual and spatial features.\n",
      "----------- 231 ------------\n",
      "Stochastic gradient descent (SGD) is a ubiquitous algorithm for a variety of machine learning problems. Researchers and industry have developed several techniques to optimize SGD's runtime performance, including asynchronous execution and reduced precision. Our main result is a martingale-based analysis that enables us to capture the rich noise models that may arise from such techniques. Specifically, we useour new analysis in three ways: (1) we derive convergence rates for the convex case (Hogwild) with relaxed assumptions on the sparsity of the problem; (2) we analyze asynchronous SGD algorithms for non-convex matrix problems including matrix completion; and (3) we design and analyze an asynchronous SGD algorithm, called Buckwild, that uses lower-precision arithmetic. We show experimentally that our algorithms run efficiently for a variety of problems on modern hardware.\n",
      "----------- 232 ------------\n",
      "Computational models in fields such as computational neuroscience are often evaluated via stochastic simulation or numerical approximation. Fitting these models implies a difficult optimization problem over complex, possibly noisy parameter landscapes. Bayesian optimization (BO) has been successfully applied to solving expensive black-box problems in engineering and machine learning. Here we explore whether BO can be applied as a general tool for model fitting. First, we present a novel hybrid BO algorithm, Bayesian adaptive direct search (BADS), that achieves competitive performance with an affordable computational overhead for the running time of typical models. We then perform an extensive benchmark of BADS vs. many common and state-of-the-art nonconvex, derivative-free optimizers, on a set of model-fitting problems with real data and models from six studies in behavioral, cognitive, and computational neuroscience. With default settings, BADS consistently finds comparable or better solutions than other methods, including `vanilla' BO, showing great promise for advanced BO techniques, and BADS in particular, as a general model-fitting tool.\n",
      "----------- 233 ------------\n",
      "Convolutional neural networks are increasingly used outside the domain of image analysis, in particular in various areas of the natural sciences concerned with spatial data. Such networks often work out-of-the box, and in some cases entire model architectures from image analysis can be carried over to other problem domains almost unaltered. Unfortunately, this convenience does not trivially extend to data in non-euclidean spaces, such as spherical data. In this paper, we introduce two strategies for conducting convolutions on the sphere, using either a spherical-polar grid or a grid based on the cubed-sphere representation. We investigate the challenges that arise in this setting, and extend our discussion to include scenarios of spherical volumes, with several strategies for parameterizing the radial dimension. As a proof of concept, we conclude with an assessment of the performance of spherical convolutions in the context of molecular modelling, by considering structural environments within proteins. We show that the models are capable of learning non-trivial functions in these molecular environments, and that our spherical convolutions generally outperform standard 3D convolutions in this setting. In particular, despite the lack of any domain specific feature-engineering, we demonstrate performance comparable to state-of-the-art methods in the field, which build on decades of domain-specific knowledge.\n",
      "----------- 234 ------------\n",
      "Persistence diagrams (PDs) are now routinely used to summarize the underlying topology of complex data. Despite several appealing properties, incorporating PDs in learning pipelines can be challenging because their natural geometry is not Hilbertian. Indeed, this was recently exemplified in a string of papers which show that the simple task of averaging a few PDs can be computationally prohibitive. We propose in this article a tractable framework to carry out standard tasks on PDs at scale, notably evaluating distances, estimating barycenters and performing clustering. This framework builds upon a reformulation of PD metrics as optimal transport (OT) problems. Doing so, we can exploit recent computational advances: the OT problem on a planar grid, when regularized with entropy, is convex can be solved in linear time using the Sinkhorn algorithm and convolutions. This results in scalable computations that can stream on GPUs. We demonstrate the efficiency of our approach by carrying out clustering with diagrams metrics on several thousands of PDs, a scale never seen before in the literature.\n",
      "----------- 235 ------------\n",
      "Asynchronous event sequences are the basis of many applications throughout different industries. In this work, we tackle the task of predicting the next event (given a history), and how this prediction changes with the passage of time. Since at some time points (e.g. predictions far into the future) we might not be able to predict anything with confidence, capturing uncertainty in the predictions is crucial. We present two new architectures, WGP-LN and FD-Dir, modelling the evolution of the distribution on the probability simplex with time-dependent logistic normal and Dirichlet distributions. In both cases, the combination of RNNs with either Gaussian process or function decomposition allows to express rich temporal evolution of the distribution parameters, and naturally captures uncertainty. Experiments on class prediction, time prediction and anomaly detection demonstrate the high performances of our models on various datasets compared to other approaches.\n",
      "----------- 236 ------------\n",
      "An emerging problem in trustworthy machine learning is to train models that produce robust interpretations for their predictions. We take a step towards solving this problem through the lens of axiomatic attribution of neural networks. Our theory is grounded in the recent work, Integrated Gradients (IG) [STY17], in axiomatically attributing a neural network’s output change to its input change. We propose training objectives in classic robust optimization models to achieve robust IG attributions. Our objectives give principled generalizations of previous objectives designed for robust predictions, and they naturally degenerate to classic soft-margin training for one-layer neural networks. We also generalize previous theory and prove that the objectives for different robust optimization models are closely related. Experiments demonstrate the effectiveness of our method, and also point to intriguing problems which hint at the need for better optimization techniques or better neural network architectures for robust attribution training.\n",
      "----------- 237 ------------\n",
      "Stochastic Gradient Descent (SGD) is a workhorse in machine learning, yet it is also known to be slow relative to steepest descent. Recently, variance reduction techniques such as SVRG and SAGA have been proposed to overcome this weakness. With asymptotically vanishing variance, a constant step size can be maintained, resulting in geometric convergence rates. However, these methods are either based on occasional computations of full gradients at pivot points (SVRG), or on keeping per data point corrections in memory (SAGA). This has the disadvantage that one cannot employ these methods in a streaming setting and that speed-ups relative to SGD may need a certain number of epochs in order to materialize. This paper investigates a new class of algorithms that can exploit neighborhood structure in the training data to share and re-use information about past stochastic gradients across data points. While not meant to be offering advantages in an asymptotic setting, there are significant benefits in the transient optimization phase, in particular in a streaming or single-epoch setting. We investigate this family of algorithms in a thorough analysis and show supporting experimental results. As a side-product we provide a simple and unified proof technique for a broad class of variance reduction algorithms.\n",
      "----------- 238 ------------\n",
      "We introduce a new stochastic multi-armed bandit setting where arms are grouped inside\n",
      "ordered'' categories. The motivating example comes from e-commerce, where a customer typically has a greater appetence for items of a specific well-identified but unknown category than any other one. We introduce three concepts of ordering between categories, inspired by stochastic dominance between random variables, which are gradually weaker so that more and more bandit scenarios satisfy at least one of them. We first prove instance-dependent lower bounds on the cumulative regret for each of these models, indicating how the complexity of the bandit problems increases with the generality of the ordering concept considered. We also provide algorithms that fully leverage the structure of the model with their associated theoretical guarantees. Finally, we have conducted an analysis on real data to highlight that those ordered categories actually exist in practice.\n",
      "----------- 239 ------------\n",
      "Many applications of machine learning, for example in health care, would benefit from methods that can guarantee privacy of data subjects. Differential privacy (DP) has become established as a standard for protecting learning results. The standard DP algorithms require a single trusted party to have access to the entire data, which is a clear weakness, or add prohibitive amounts of noise. We consider DP Bayesian learning in a distributed setting, where each party only holds a single sample or a few samples of the data. We propose a learning strategy based on a secure multi-party sum function for aggregating summaries from data holders and the Gaussian mechanism for DP. Our method builds on an asymptotically optimal and practically efficient DP Bayesian inference with rapidly diminishing extra cost.\n",
      "----------- 240 ------------\n",
      "Compressive image recovery is a challenging problem that requires fast and accurate algorithms. Recently, neural networks have been applied to this problem with promising results. By exploiting massively parallel GPU processing architectures and oodles of training data, they can run orders of magnitude faster than existing techniques. However, these methods are largely unprincipled black boxes that are difficult to train and often-times specific to a single measurement matrix. It was recently demonstrated that iterative sparse-signal-recovery algorithms can be unrolled’' to form interpretable deep networks. Taking inspiration from this work, we develop a novel neural network architecture that mimics the behavior of the denoising-based approximate message passing (D-AMP) algorithm. We call this new network {\\em Learned} D-AMP (LDAMP). The LDAMP network is easy to train, can be applied to a variety of different measurement matrices, and comes with a state-evolution heuristic that accurately predicts its performance. Most importantly, it outperforms the state-of-the-art BM3D-AMP and NLR-CS algorithms in terms of both accuracy and run time. At high resolutions, and when used with sensing matrices that have fast implementations, LDAMP runs over 50\\times faster than BM3D-AMP and hundreds of times faster than NLR-CS.\n",
      "----------- 241 ------------\n",
      "We introduce a theorem proving algorithm that uses practically no domain heuristics for guiding its connection-style proof search. Instead, it runs many Monte-Carlo simulations guided by reinforcement learning from previous proof attempts. We produce several versions of the prover, parameterized by different learning and guiding algorithms. The strongest version of the system is trained on a large corpus of mathematical problems and evaluated on previously unseen problems. The trained system solves within the same number of inferences over 40% more problems than a baseline prover, which is an unusually high improvement in this hard AI domain. To our knowledge this is the first time reinforcement learning has been convincingly applied to solving general mathematical problems on a large scale.\n",
      "----------- 242 ------------\n",
      "Relational reasoning is a central component of generally intelligent behavior, but has proven difficult for neural networks to learn. In this paper we describe how to use Relation Networks (RNs) as a simple plug-and-play module to solve problems that fundamentally hinge on relational reasoning. We tested RN-augmented networks on three tasks: visual question answering using a challenging dataset called CLEVR, on which we achieve state-of-the-art, super-human performance; text-based question answering using the bAbI suite of tasks; and complex reasoning about dynamical physical systems. Then, using a curated dataset called Sort-of-CLEVR we show that powerful convolutional networks do not have a general capacity to solve relational questions, but can gain this capacity when augmented with RNs. Thus, by simply augmenting convolutions, LSTMs, and MLPs with RNs, we can remove computational burden from network components that are not well-suited to handle relational reasoning, reduce overall network complexity, and gain a general ability to reason about the relations between entities and their properties.\n",
      "----------- 243 ------------\n",
      "Building systems that autonomously create temporal abstractions from data is a key challenge in scaling learning and planning in reinforcement learning. One popular approach for addressing this challenge is the options framework (Sutton et al., 1999). However, only recently in (Bacon et al., 2017) was a policy gradient theorem derived for online learning of general purpose options in an end to end fashion. In this work, we extend previous work on this topic that only focuses on learning a two-level hierarchy including options and primitive actions to enable learning simultaneously at multiple resolutions in time. We achieve this by considering an arbitrarily deep hierarchy of options where high level temporally extended options are composed of lower level options with finer resolutions in time. We extend results from (Bacon et al., 2017) and derive policy gradient theorems for a deep hierarchy of options. Our proposed hierarchical option-critic architecture is capable of learning internal policies, termination conditions, and hierarchical compositions over options without the need for any intrinsic rewards or subgoals. Our empirical results in both discrete and continuous environments demonstrate the efficiency of our framework.\n",
      "----------- 244 ------------\n",
      "Point clouds provide a flexible and natural representation usable in countless applications such as robotics or self-driving cars. Recently, deep neural networks operating on raw point cloud data have shown promising results on supervised learning tasks such as object classification and semantic segmentation. While massive point cloud datasets can be captured using modern scanning technology, manually labelling such large 3D point clouds for supervised learning tasks is a cumbersome process. This necessitates methods that can learn from unlabelled data to significantly reduce the number of annotated samples needed in supervised learning. We propose a self-supervised learning task for deep learning on raw point cloud data in which a neural network is trained to reconstruct point clouds whose parts have been randomly rearranged. While solving this task, representations that capture semantic properties of the point cloud are learned. Our method is agnostic of network architecture and outperforms current unsupervised learning approaches in downstream object classification tasks. We show experimentally, that pre-training with our method before supervised training improves the performance of state-of-the-art models and significantly improves sample efficiency.\n",
      "----------- 245 ------------\n",
      "We develop a latent variable model and an efficient spectral algorithm motivated by the recent emergence of very large data sets of chromatin marks from multiple human cell types. A natural model for chromatin data in one cell type is a Hidden Markov Model (HMM); we model the relationship between multiple cell types by connecting their hidden states by a fixed tree of known structure. The main challenge with learning parameters of such models is that iterative methods such as EM are very slow, while naive spectral methods result in time and space complexity exponential in the number of cell types. We exploit properties of the tree structure of the hidden states to provide spectral algorithms that are more computationally efficient for current biological datasets. We provide sample complexity bounds for our algorithm and evaluate it experimentally on biological data from nine human cell types. Finally, we show that beyond our specific model, some of our algorithmic ideas can be applied to other graphical models.\n",
      "----------- 246 ------------\n",
      "Semi-supervised learning (SSL) provides a powerful framework for leveraging unlabeled data when labels are limited or expensive to obtain. SSL algorithms based on deep neural networks have recently proven successful on standard benchmark tasks. However, we argue that these benchmarks fail to address many issues that SSL algorithms would face in real-world applications. After creating a unified reimplementation of various widely-used SSL techniques, we test them in a suite of experiments designed to address these issues. We find that the performance of simple baselines which do not use unlabeled data is often underreported, SSL methods differ in sensitivity to the amount of labeled and unlabeled data, and performance can degrade substantially when the unlabeled dataset contains out-of-distribution examples. To help guide SSL research towards real-world applicability, we make our unified reimplemention and evaluation platform publicly available.\n",
      "----------- 247 ------------\n",
      "The design of revenue-maximizing combinatorial auctions, i.e. multi item auctions over bundles of goods, is one of the most fundamental problems in computational economics, unsolved even for two bidders and two items for sale. In the traditional economic models, it is assumed that the bidders' valuations are drawn from an underlying distribution and that the auction designer has perfect knowledge of this distribution. Despite this strong and oftentimes unrealistic assumption, it is remarkable that the revenue-maximizing combinatorial auction remains unknown. In recent years, automated mechanism design has emerged as one of the most practical and promising approaches to designing high-revenue combinatorial auctions. The most scalable automated mechanism design algorithms take as input samples from the bidders' valuation distribution and then search for a high-revenue auction in a rich auction class. In this work, we provide the first sample complexity analysis for the standard hierarchy of deterministic combinatorial auction classes used in automated mechanism design. In particular, we provide tight sample complexity bounds on the number of samples needed to guarantee that the empirical revenue of the designed mechanism on the samples is close to its expected revenue on the underlying, unknown distribution over bidder valuations, for each of the auction classes in the hierarchy. In addition to helping set automated mechanism design on firm foundations, our results also push the boundaries of learning theory. In particular, the hypothesis functions used in our contexts are defined through multi stage combinatorial optimization procedures, rather than simple decision boundaries, as are common in machine learning.\n",
      "----------- 248 ------------\n",
      "What policy should be employed in a Markov decision process with uncertain parameters? Robust optimization answer to this question is to use rectangular uncertainty sets, which independently reflect available knowledge about each state, and then obtains a decision policy that maximizes expected reward for the worst-case decision process parameters from these uncertainty sets. While this rectangularity is convenient computationally and leads to tractable solutions, it often produces policies that are too conservative in practice, and does not facilitate knowledge transfer between portions of the state space or across related decision processes. In this work, we propose non-rectangular uncertainty sets that bound marginal moments of state-action features defined over entire trajectories through a decision process. This enables generalization to different portions of the state space while retaining appropriate uncertainty of the decision process. We develop algorithms for solving the resulting robust decision problems, which reduce to finding an optimal policy for a mixture of decision processes, and demonstrate the benefits of our approach experimentally.\n",
      "----------- 249 ------------\n",
      "The Multi-Armed Bandit problem constitutes an archetypal setting for sequential decision-making, permeating multiple domains including engineering, business, and medicine. One of the hallmarks of a bandit setting is the agent's capacity to explore its environment through active intervention, which contrasts with the ability to collect passive data by estimating associational relationships between actions and payouts. The existence of unobserved confounders, namely unmeasured variables affecting both the action and the outcome variables, implies that these two data-collection modes will in general not coincide. In this paper, we show that formalizing this distinction has conceptual and algorithmic implications to the bandit setting. The current generation of bandit algorithms implicitly try to maximize rewards based on estimation of the experimental distribution, which we show is not always the best strategy to pursue. Indeed, to achieve low regret in certain realistic classes of bandit problems (namely, in the face of unobserved confounders), both experimental and observational quantities are required by the rational agent. After this realization, we propose an optimization metric (employing both experimental and observational distributions) that bandit agents should pursue, and illustrate its benefits over traditional algorithms.\n",
      "----------- 250 ------------\n",
      "We introduce the community exploration problem that has various real-world applications such as online advertising. In the problem, an explorer allocates limited budget to explore communities so as to maximize the number of members he could meet. We provide a systematic study of the community exploration problem, from offline optimization to online learning. For the offline setting where the sizes of communities are known, we prove that the greedy methods for both of non-adaptive exploration and adaptive exploration are optimal. For the online setting where the sizes of communities are not known and need to be learned from the multi-round explorations, we propose an\n",
      "upper confidence'' like algorithm that achieves the logarithmic regret bounds. By combining the feedback from different rounds, we can achieve a constant regret bound.\n",
      "----------- 251 ------------\n",
      "In this work, we consider the problem of model selection for deep reinforcement learning (RL) in real-world environments. Typically, the performance of deep RL algorithms is evaluated via on-policy interactions with the target environment. However, comparing models in a real-world environment for the purposes of early stopping or hyperparameter tuning is costly and often practically infeasible. This leads us to examine off-policy policy evaluation (OPE) in such settings. We focus on OPE of value-based methods, which are of particular interest in deep RL with applications like robotics, where off-policy algorithms based on Q-function estimation can often attain better sample complexity than direct policy optimization. Furthermore, existing OPE metrics either rely on a model of the environment, or the use of importance sampling (IS) to correct for the data being off-policy. However, for high-dimensional observations, such as images, models of the environment can be difficult to fit and value-based methods can make IS hard to use or even ill-conditioned, especially when dealing with continuous action spaces. In this paper, we focus on the specific case of MDPs with continuous action spaces and sparse binary rewards, which is representative of many important real-world applications. We propose an alternative metric that relies on neither models nor IS, by framing OPE as a positive-unlabeled (PU) classification problem. We experimentally show that this metric outperforms baselines on a number of tasks. Most importantly, it can reliably predict the relative performance of different policies in a number of generalization scenarios, including the transfer to the real-world of policies trained in simulation for an image-based robotic manipulation task.\n",
      "----------- 252 ------------\n",
      "We resolve the fundamental problem of online decoding with general nth order ergodic Markov chain models. Specifically, we provide deterministic and randomized algorithms whose performance is close to that of the optimal offline algorithm even when latency is small. Our algorithms admit efficient implementation via dynamic programs, and readily extend to (adversarial) non-stationary or time-varying settings. We also establish lower bounds for online methods under latency constraints in both deterministic and randomized settings, and show that no online algorithm can perform significantly better than our algorithms. To our knowledge, our work is the first to analyze general Markov chain decoding under hard constraints on latency. We provide strong empirical evidence to illustrate the potential impact of our work in applications such as gene sequencing.\n",
      "----------- 253 ------------\n",
      "The determinantal point process (DPP) is an elegant probabilistic model of repulsion with applications in various machine learning tasks including summarization and search. However, the maximum a posteriori (MAP) inference for DPP which plays an important role in many applications is NP-hard, and even the popular greedy algorithm can still be too computationally expensive to be used in large-scale real-time scenarios. To overcome the computational challenge, in this paper, we propose a novel algorithm to greatly accelerate the greedy MAP inference for DPP. In addition, our algorithm also adapts to scenarios where the repulsion is only required among nearby few items in the result sequence. We apply the proposed algorithm to generate relevant and diverse recommendations. Experimental results show that our proposed algorithm is significantly faster than state-of-the-art competitors, and provides a better relevance-diversity trade-off on several public datasets, which is also confirmed in an online A/B test.\n",
      "----------- 254 ------------\n",
      "Graph-structured data arise in wide applications, such as computer vision, bioinformatics, and social networks. Quantifying similarities among graphs is a fundamental problem. In this paper, we develop a framework for computing graph kernels, based on return probabilities of random walks. The advantages of our proposed kernels are that they can effectively exploit various node attributes, while being scalable to large datasets. We conduct extensive graph classification experiments to evaluate our graph kernels. The experimental results show that our graph kernels significantly outperform other state-of-the-art approaches in both accuracy and computational efficiency.\n",
      "----------- 255 ------------\n",
      "Reinforcement learning demands a reward function, which is often difficult to provide or design in real world applications. While inverse reinforcement learning (IRL) holds promise for automatically learning reward functions from demonstrations, several major challenges remain. First, existing IRL methods learn reward functions from scratch, requiring large numbers of demonstrations to correctly infer the reward for each task the agent may need to perform. Second, and more subtly, existing methods typically assume demonstrations for one, isolated behavior or task, while in practice, it is significantly more natural and scalable to provide datasets of heterogeneous behaviors. To this end, we propose a deep latent variable model that is capable of learning rewards from unstructured, multi-task demonstration data, and critically, use this experience to infer robust rewards for new, structurally-similar tasks from a single demonstration. Our experiments on multiple continuous control tasks demonstrate the effectiveness of our approach compared to state-of-the-art imitation and inverse reinforcement learning methods.\n",
      "----------- 256 ------------\n",
      "Graph clustering is a fundamental task in many data-mining and machine-learning pipelines. In particular, identifying a good hierarchical structure is at the same time a fundamental and challenging problem for several applications. The amount of data to analyze is increasing at an astonishing rate each day. Hence there is a need for new solutions to efficiently compute effective hierarchical clusterings on such huge data. The main focus of this paper is on minimum spanning tree (MST) based clusterings. In particular, we propose affinity, a novel hierarchical clustering based on Boruvka's MST algorithm. We prove certain theoretical guarantees for affinity (as well as some other classic algorithms) and show that in practice it is superior to several other state-of-the-art clustering algorithms. Furthermore, we present two MapReduce implementations for affinity. The first one works for the case where the input graph is dense and takes constant rounds. It is based on a Massively Parallel MST algorithm for dense graphs that improves upon the state-of-the-art algorithm of Lattanzi et al. (SPAA 2011). Our second algorithm has no assumption on the density of the input graph and finds the affinity clustering in O(\\log n)\n",
      "rounds using Distributed Hash Tables (DHTs). We show experimentally that our algorithms are scalable for huge data sets, e.g., for graphs with trillions of edges.\n",
      "----------- 257 ------------\n",
      "Reinforcement learning is a powerful paradigm for learning optimal policies from experimental data. However, to find optimal policies, most reinforcement learning algorithms explore all possible actions, which may be harmful for real-world systems. As a consequence, learning algorithms are rarely applied on safety-critical systems in the real world. In this paper, we present a learning algorithm that explicitly considers safety, defined in terms of stability guarantees. Specifically, we extend control-theoretic results on Lyapunov stability verification and show how to use statistical models of the dynamics to obtain high-performance control policies with provable stability certificates. Moreover, under additional regularity assumptions in terms of a Gaussian process prior, we prove that one can effectively and safely collect data in order to learn about the dynamics and thus both improve control performance and expand the safe region of the state space. In our experiments, we show how the resulting algorithm can safely optimize a neural network policy on a simulated inverted pendulum, without the pendulum ever falling down.\n",
      "----------- 258 ------------\n",
      "A central challenge faced by memory systems is the robust retrieval of a stored pattern in the presence of interference due to other stored patterns and noise. A theoretically well-founded solution to robust retrieval is given by attractor dynamics, which iteratively cleans up patterns during recall. However, incorporating attractor dynamics into modern deep learning systems poses difficulties: attractor basins are characterised by vanishing gradients, which are known to make training neural networks difficult. In this work, we exploit recent advances in variational inference and avoid the vanishing gradient problem by training a generative distributed memory with a variational lower-bound-based Lyapunov function. The model is minimalistic with surprisingly few parameters. Experiments shows it converges to correct patterns upon iterative retrieval and achieves competitive performance as both a memory model and a generative model.\n",
      "----------- 259 ------------\n",
      "Teaching is critical to human society: it is with teaching that prospective students are educated and human civilization can be inherited and advanced. A good teacher not only provides his/her students with qualified teaching materials (e.g., textbooks), but also sets up appropriate learning objectives (e.g., course projects and exams) considering different situations of a student. When it comes to artificial intelligence, treating machine learning models as students, the loss functions that are optimized act as perfect counterparts of the learning objective set by the teacher. In this work, we explore the possibility of imitating human teaching behaviors by dynamically and automatically outputting appropriate loss functions to train machine learning models. Different from typical learning settings in which the loss function of a machine learning model is predefined and fixed, in our framework, the loss function of a machine learning model (we call it student) is defined by another machine learning model (we call it teacher). The ultimate goal of teacher model is cultivating the student to have better performance measured on development dataset. Towards that end, similar to human teaching, the teacher, a parametric model, dynamically outputs different loss functions that will be used and optimized by its student model at different training stages. We develop an efficient learning method for the teacher model that makes gradient based optimization possible, exempt of the ineffective solutions such as policy optimization. We name our method as\n",
      "learning to teach with dynamic loss functions'' (L2T-DLF for short). Extensive experiments on real world tasks including image classification and neural machine translation demonstrate that our method significantly improves the quality of various student models.\n",
      "----------- 260 ------------\n",
      "Recently, there has been growing interest in lifting MAP inference algorithms for Markov logic networks (MLNs). A key advantage of these lifted algorithms is that they have much smaller computational complexity than propositional algorithms when symmetries are present in the MLN and these symmetries can be detected using lifted inference rules. Unfortunately, lifted inference rules are sound but not complete and can often miss many symmetries. This is problematic because when symmetries cannot be exploited, lifted inference algorithms ground the MLN, and search for solutions in the much larger propositional space. In this paper, we present a novel approach, which cleverly introduces new symmetries at the time of grounding. Our main idea is to partition the ground atoms and force the inference algorithm to treat all atoms in each part as indistinguishable. We show that by systematically and carefully refining (and growing) the partitions, we can build advanced any-time and any-space MAP inference algorithms. Our experiments on several real-world datasets clearly show that our new algorithm is superior to previous approaches and often finds useful symmetries in the search space that existing lifted inference rules are unable to detect.\n",
      "----------- 261 ------------\n",
      "In imperfect-information games, the optimal strategy in a subgame may depend on the strategy in other, unreached subgames. Thus a subgame cannot be solved in isolation and must instead consider the strategy for the entire game as a whole, unlike perfect-information games. Nevertheless, it is possible to first approximate a solution for the whole game and then improve it in individual subgames. This is referred to as subgame solving. We introduce subgame-solving techniques that outperform prior methods both in theory and practice. We also show how to adapt them, and past subgame-solving techniques, to respond to opponent actions that are outside the original action abstraction; this significantly outperforms the prior state-of-the-art approach, action translation. Finally, we show that subgame solving can be repeated as the game progresses down the game tree, leading to far lower exploitability. These techniques were a key component of Libratus, the first AI to defeat top humans in heads-up no-limit Texas hold'em poker.\n",
      "----------- 262 ------------\n",
      "Deep Reinforcement Learning (DRL) algorithms have been successfully applied to a range of challenging control tasks. However, these methods typically suffer from three core difficulties: temporal credit assignment with sparse rewards, lack of effective exploration, and brittle convergence properties that are extremely sensitive to hyperparameters. Collectively, these challenges severely limit the applicability of these approaches to real world problems. Evolutionary Algorithms (EAs), a class of black box optimization techniques inspired by natural evolution, are well suited to address each of these three challenges. However, EAs typically suffer from high sample complexity and struggle to solve problems that require optimization of a large number of parameters. In this paper, we introduce Evolutionary Reinforcement Learning (ERL), a hybrid algorithm that leverages the population of an EA to provide diversified data to train an RL agent, and reinserts the RL agent into the EA population periodically to inject gradient information into the EA. ERL inherits EA's ability of temporal credit assignment with a fitness metric, effective exploration with a diverse set of policies, and stability of a population-based approach and complements it with off-policy DRL's ability to leverage gradients for higher sample efficiency and faster learning. Experiments in a range of challenging continuous control benchmarks demonstrate that ERL significantly outperforms prior DRL and EA methods.\n",
      "----------- 263 ------------\n",
      "We study the classic k-means/median clustering, which are fundamental problems in unsupervised learning, in the setting where data are partitioned across multiple sites, and where we are allowed to discard a small portion of the data by labeling them as outliers. We propose a simple approach based on constructing small summary for the original dataset. The proposed method is time and communication efficient, has good approximation guarantees, and can identify the global outliers effectively. To the best of our knowledge, this is the first practical algorithm with theoretical guarantees for distributed clustering with outliers. Our experiments on both real and synthetic data have demonstrated the clear superiority of our algorithm against all the baseline algorithms in almost all metrics.\n",
      "----------- 264 ------------\n",
      "Online learning to rank is a sequential decision-making problem where in each round the learning agent chooses a list of items and receives feedback in the form of clicks from the user. Many sample-efficient algorithms have been proposed for this problem that assume a specific click model connecting rankings and user behavior. We propose a generalized click model that encompasses many existing models, including the position-based and cascade models. Our generalization motivates a novel online learning algorithm based on topological sort, which we call TopRank. TopRank is (a) more natural than existing algorithms, (b) has stronger regret guarantees than existing algorithms with comparable generality, (c) has a more insightful proof that leaves the door open to many generalizations, (d) outperforms existing algorithms empirically.\n",
      "----------- 265 ------------\n",
      "Instrumental variable analysis is a powerful tool for estimating causal effects when randomization or full control of confounders is not possible. The application of standard methods such as 2SLS, GMM, and more recent variants are significantly impeded when the causal effects are complex, the instruments are high-dimensional, and/or the treatment is high-dimensional. In this paper, we propose the DeepGMM algorithm to overcome this. Our algorithm is based on a new variational reformulation of GMM with optimal inverse-covariance weighting that allows us to efficiently control very many moment conditions. We further develop practical techniques for optimization and model selection that make it particularly successful in practice. Our algorithm is also computationally tractable and can handle large-scale datasets. Numerical results show our algorithm matches the performance of the best tuned methods in standard settings and continues to work in high-dimensional settings where even recent methods break.\n",
      "----------- 266 ------------\n",
      "Learning goal-directed behavior in environments with sparse feedback is a major challenge for reinforcement learning algorithms. One of the key difficulties is insufficient exploration, resulting in an agent being unable to learn robust policies. Intrinsically motivated agents can explore new behavior for their own sake rather than to directly solve external goals. Such intrinsic behaviors could eventually help the agent solve tasks posed by the environment. We present hierarchical-DQN (h-DQN), a framework to integrate hierarchical action-value functions, operating at different temporal scales, with goal-driven intrinsically motivated deep reinforcement learning. A top-level q-value function learns a policy over intrinsic goals, while a lower-level function learns a policy over atomic actions to satisfy the given goals. h-DQN allows for flexible goal specifications, such as functions over entities and relations. This provides an efficient space for exploration in complicated environments. We demonstrate the strength of our approach on two problems with very sparse and delayed feedback: (1) a complex discrete stochastic decision process with stochastic transitions, and (2) the classic ATARI game -- `Montezuma's Revenge'.\n",
      "----------- 267 ------------\n",
      "Neural networks are becoming more and more popular for the analysis of physiological time-series. The most successful deep learning systems in this domain combine convolutional and recurrent layers to extract useful features to model temporal relations. Unfortunately, these recurrent models are difficult to tune and optimize. In our experience, they often require task-specific modifications, which makes them challenging to use for non-experts. We propose U-Time, a fully feed-forward deep learning approach to physiological time series segmentation developed for the analysis of sleep data. U-Time is a temporal fully convolutional network based on the U-Net architecture that was originally proposed for image segmentation. U-Time maps sequential inputs of arbitrary length to sequences of class labels on a freely chosen temporal scale. This is done by implicitly classifying every individual time-point of the input signal and aggregating these classifications over fixed intervals to form the final predictions. We evaluated U-Time for sleep stage classification on a large collection of sleep electroencephalography (EEG) datasets. In all cases, we found that U-Time reaches or outperforms current state-of-the-art deep learning models while being much more robust in the training process and without requiring architecture or hyperparameter adaptation across tasks.\n",
      "----------- 268 ------------\n",
      "The need to efficiently calculate first- and higher-order derivatives of increasingly complex models expressed in Python has stressed or exceeded the capabilities of available tools. In this work, we explore techniques from the field of automatic differentiation (AD) that can give researchers expressive power, performance and strong usability. These include source-code transformation (SCT), flexible gradient surgery, efficient in-place array operations, and higher-order derivatives. We implement and demonstrate these ideas in the Tangent software library for Python, the first AD framework for a dynamic language that uses SCT.\n",
      "----------- 269 ------------\n",
      "From traditional Web search engines to virtual assistants and Web accelerators, services that rely on online information need to continually keep track of remote content changes by explicitly requesting content updates from remote sources (e.g., web pages). We propose a novel optimization objective for this setting that has several practically desirable properties, and efficient algorithms for it with optimality guarantees even in the face of mixed content change observability and initially unknown change model parameters. Experiments on 18.5M URLs crawled daily for 14 weeks show significant advantages of this approach over prior art.\n",
      "----------- 270 ------------\n",
      "One of the main challenges in reinforcement learning (RL) is generalisation. In typical deep RL methods this is achieved by approximating the optimal value function with a low-dimensional representation using a deep network. While this approach works well in many domains, in domains where the optimal value function cannot easily be reduced to a low-dimensional representation, learning can be very slow and unstable. This paper contributes towards tackling such challenging domains, by proposing a new method, called Hybrid Reward Architecture (HRA). HRA takes as input a decomposed reward function and learns a separate value function for each component reward function. Because each component typically only depends on a subset of all features, the corresponding value function can be approximated more easily by a low-dimensional representation, enabling more effective learning. We demonstrate HRA on a toy-problem and the Atari game Ms. Pac-Man, where HRA achieves above-human performance.\n",
      "----------- 271 ------------\n",
      "Continual learning aims to improve the ability of modern learning systems to deal with non-stationary distributions, typically by attempting to learn a series of tasks sequentially. Prior art in the field has largely considered supervised or reinforcement learning tasks, and often assumes full knowledge of task labels and boundaries. In this work, we propose an approach (CURL) to tackle a more general problem that we will refer to as unsupervised continual learning. The focus is on learning representations without any knowledge about task identity, and we explore scenarios when there are abrupt changes between tasks, smooth transitions from one task to another, or even when the data is shuffled. The proposed approach performs task inference directly within the model, is able to dynamically expand to capture new concepts over its lifetime, and incorporates additional rehearsal-based techniques to deal with catastrophic forgetting. We demonstrate the efficacy of CURL in an unsupervised learning setting with MNIST and Omniglot, where the lack of labels ensures no information is leaked about the task. Further, we demonstrate strong performance compared to prior art in an i.i.d setting, or when adapting the technique to supervised tasks such as incremental class learning.\n",
      "----------- 272 ------------\n",
      "We investigate the problem of assigning trip requests to available vehicles in on-demand ridesourcing. Much of the literature has focused on maximizing the total value of served requests, achieving efficiency on the passengers’ side. However, such solutions may result in some drivers being assigned to insufficient or undesired trips, therefore losing fairness from the drivers’ perspective. In this paper, we focus on both the system efficiency and the fairness among drivers and quantitatively analyze the trade-offs between these two objectives. In particular, we give an explicit answer to the question of whether there always exists an assignment that achieves any target efficiency and fairness. We also propose a simple reassignment algorithm that can achieve any selected trade-off. Finally, we demonstrate the effectiveness of the algorithms through extensive experiments on real-world datasets.\n",
      "----------- 273 ------------\n",
      "Dropout is used as a practical tool to obtain uncertainty estimates in large vision models and reinforcement learning (RL) tasks. But to obtain well-calibrated uncertainty estimates, a grid-search over the dropout probabilities is necessary—a prohibitive operation with large models, and an impossible one with RL. We propose a new dropout variant which gives improved performance and better calibrated uncertainties. Relying on recent developments in Bayesian deep learning, we use a continuous relaxation of dropout’s discrete masks. Together with a principled optimisation objective, this allows for automatic tuning of the dropout probability in large models, and as a result faster experimentation cycles. In RL this allows the agent to adapt its uncertainty dynamically as more data is observed. We analyse the proposed variant extensively on a range of tasks, and give insights into common practice in the field where larger dropout probabilities are often used in deeper model layers.\n",
      "----------- 274 ------------\n",
      "Communication could potentially be an effective way for multi-agent cooperation. However, information sharing among all agents or in predefined communication architectures that existing methods adopt can be problematic. When there is a large number of agents, agents cannot differentiate valuable information that helps cooperative decision making from globally shared information. Therefore, communication barely helps, and could even impair the learning of multi-agent cooperation. Predefined communication architectures, on the other hand, restrict communication among agents and thus restrain potential cooperation. To tackle these difficulties, in this paper, we propose an attentional communication model that learns when communication is needed and how to integrate shared information for cooperative decision making. Our model leads to efficient and effective communication for large-scale multi-agent cooperation. Empirically, we show the strength of our model in a variety of cooperative scenarios, where agents are able to develop more coordinated and sophisticated strategies than existing methods.\n",
      "----------- 275 ------------\n",
      "The design of codes for communicating reliably over a statistically well defined channel is an important endeavor involving deep mathematical research and wide- ranging practical applications. In this work, we present the first family of codes obtained via deep learning, which significantly beats state-of-the-art codes designed over several decades of research. The communication channel under consideration is the Gaussian noise channel with feedback, whose study was initiated by Shannon; feedback is known theoretically to improve reliability of communication, but no practical codes that do so have ever been successfully constructed. We break this logjam by integrating information theoretic insights harmoniously with recurrent-neural-network based encoders and decoders to create novel codes that outperform known codes by 3 orders of magnitude in reliability. We also demonstrate several desirable properties in the codes: (a) generalization to larger block lengths; (b) composability with known codes; (c) adaptation to practical constraints. This result also presents broader ramifications to coding theory: even when the channel has a clear mathematical model, deep learning methodologies, when combined with channel specific information-theoretic insights, can potentially beat state-of-the-art codes, constructed over decades of mathematical research.\n",
      "----------- 276 ------------\n",
      "The existence of adversarial data examples has drawn significant attention in the deep-learning community; such data are seemingly minimally perturbed relative to the original data, but lead to very different outputs from a deep-learning algorithm. Although a significant body of work on developing defense models has been developed, most such models are heuristic and are often vulnerable to adaptive attacks. Defensive methods that provide theoretical robustness guarantees have been studied intensively, yet most fail to obtain non-trivial robustness when a large-scale model and data are present. To address these limitations, we introduce a framework that is scalable and provides certified bounds on the norm of the input manipulation for constructing adversarial examples. We establish a connection between robustness against adversarial perturbation and additive random noise, and propose a training strategy that can significantly improve the certified bounds. Our evaluation on MNIST, CIFAR-10 and ImageNet suggests that our method is scalable to complicated models and large data sets, while providing competitive robustness to state-of-the-art provable defense methods.\n",
      "----------- 277 ------------\n",
      "We consider a large-scale matrix multiplication problem where the computation is carried out using a distributed system with a master node and multiple worker nodes, where each worker can store parts of the input matrices. We propose a computation strategy that leverages ideas from coding theory to design intermediate computations at the worker nodes, in order to optimally deal with straggling workers. The proposed strategy, named as \\emph{polynomial codes}, achieves the optimum recovery threshold, defined as the minimum number of workers that the master needs to wait for in order to compute the output. This is the first code that achieves the optimal utilization of redundancy for tolerating stragglers or failures in distributed matrix multiplication. Furthermore, by leveraging the algebraic structure of polynomial codes, we can map the reconstruction problem of the final output to a polynomial interpolation problem, which can be solved efficiently. Polynomial codes provide order-wise improvement over the state of the art in terms of recovery threshold, and are also optimal in terms of several other metrics including computation latency and communication load. Moreover, we extend this code to distributed convolution and show its order-wise optimality.\n",
      "----------- 278 ------------\n",
      "Crowdsourcing has gained immense popularity in machine learning applications for obtaining large amounts of labeled data. Crowdsourcing is cheap and fast, but suffers from the problem of low-quality data. To address this fundamental challenge in crowdsourcing, we propose a simple payment mechanism to incentivize workers to answer only the questions that they are sure of and skip the rest. We show that surprisingly, under a mild and natural no-free-lunch requirement, this mechanism is the one and only incentive-compatible payment mechanism possible. We also show that among all possible incentive-compatible mechanisms (that may or may not satisfy no-free-lunch), our mechanism makes the smallest possible payment to spammers. Interestingly, this unique mechanism takes a multiplicative form. The simplicity of the mechanism is an added benefit. In preliminary experiments involving over several hundred workers, we observe a significant reduction in the error rates under our unique mechanism for the same or lower monetary expenditure.\n",
      "----------- 279 ------------\n",
      "Autonomous agents optimize the reward function we give them. What they don't know is how hard it is for us to design a reward function that actually captures what we want. When designing the reward, we might think of some specific training scenarios, and make sure that the reward will lead to the right behavior in those scenarios. Inevitably, agents encounter new scenarios (e.g., new types of terrain) where optimizing that same reward may lead to undesired behavior. Our insight is that reward functions are merely observations about what the designer actually wants, and that they should be interpreted in the context in which they were designed. We introduce inverse reward design (IRD) as the problem of inferring the true objective based on the designed reward and the training MDP. We introduce approximate methods for solving IRD problems, and use their solution to plan risk-averse behavior in test MDPs. Empirical results suggest that this approach can help alleviate negative side effects of misspecified reward functions and mitigate reward hacking.\n",
      "----------- 280 ------------\n",
      "Learning to make decisions in an uncertain and dynamic environment is a task of fundamental performance in a number of domains. This paper concerns the problem of learning control policies for an unknown linear dynamical system so as to minimize a quadratic cost function. We present a method, based on convex optimization, that accomplishes this task ‘robustly’, i.e., the worst-case cost, accounting for system uncertainty given the observed data, is minimized. The method balances exploitation and exploration, exciting the system in such a way so as to reduce uncertainty in the model parameters to which the worst-case cost is most sensitive. Numerical simulations and application to a hardware-in-the-loop servo-mechanism are used to demonstrate the approach, with appreciable performance and robustness gains over alternative methods observed in both.\n",
      "----------- 281 ------------\n",
      "From just a glance, humans can make rich predictions about the future of a wide range of physical systems. On the other hand, modern approaches from engineering, robotics, and graphics are often restricted to narrow domains or require information about the underlying state. We introduce the Visual Interaction Network, a general-purpose model for learning the dynamics of a physical system from raw visual observations. Our model consists of a perceptual front-end based on convolutional neural networks and a dynamics predictor based on interaction networks. Through joint training, the perceptual front-end learns to parse a dynamic visual scene into a set of factored latent object representations. The dynamics predictor learns to roll these states forward in time by computing their interactions, producing a predicted physical trajectory of arbitrary length. We found that from just six input video frames the Visual Interaction Network can generate accurate future trajectories of hundreds of time steps on a wide range of physical systems. Our model can also be applied to scenes with invisible objects, inferring their future states from their effects on the visible objects, and can implicitly infer the unknown mass of objects. This work opens new opportunities for model-based decision-making and planning from raw sensory observations in complex physical environments.\n",
      "----------- 282 ------------\n",
      "The F-measure is an important and commonly used performance metric for binary prediction tasks. By combining precision and recall into a single score, it avoids disadvantages of simple metrics like the error rate, especially in cases of imbalanced class distributions. The problem of optimizing the F-measure, that is, of developing learning algorithms that perform optimally in the sense of this measure, has recently been tackled by several authors. In this paper, we study the problem of F-measure maximization in the setting of online learning. We propose an efficient online algorithm and provide a formal analysis of its convergence properties. Moreover, first experimental results are presented, showing that our method performs well in practice.\n",
      "----------- 283 ------------\n",
      "Reducing the numerical precision of data and computation is extremely effective in accelerating deep learning training workloads. Towards this end, 8-bit floating point representations (FP8) were recently proposed for DNN training. However, its applicability was demonstrated on a few selected models only and significant degradation is observed when popular networks such as MobileNet and Transformer are trained using FP8. This degradation is due to the inherent precision requirement difference in the forward and backward passes of DNN training. Using theoretical insights, we propose a hybrid FP8 (HFP8) format and DNN end-to-end distributed training procedure. We demonstrate, using HFP8, the successful training of deep learning models across a whole spectrum of applications including Image Classification, Object Detection, Language and Speech without accuracy degradation. Finally, we demonstrate that, by using the new 8 bit format, we can directly quantize a pre-trained model down to 8-bits without losing accuracy by simply fine-tuning batch normalization statistics. These novel techniques enable a new generations of 8-bit hardware that are robust for building and deploying neural network models.\n",
      "----------- 284 ------------\n",
      "Subset selection, which is the task of finding a small subset of representative items from a large ground set, finds numerous applications in different areas. Sequential data, including time-series and ordered data, contain important structural relationships among items, imposed by underlying dynamic models of data, that should play a vital role in the selection of representatives. However, nearly all existing subset selection techniques ignore underlying dynamics of data and treat items independently, leading to incompatible sets of representatives. In this paper, we develop a new framework for sequential subset selection that finds a set of representatives compatible with the dynamic models of data. To do so, we equip items with transition dynamic models and pose the problem as an integer binary optimization over assignments of sequential items to representatives, that leads to high encoding, diversity and transition potentials. Our formulation generalizes the well-known facility location objective to deal with sequential data, incorporating transition dynamics among facilities. As the proposed formulation is non-convex, we derive a max-sum message passing algorithm to solve the problem efficiently. Experiments on synthetic and real data, including instructional video summarization, show that our sequential subset selection framework not only achieves better encoding and diversity than the state of the art, but also successfully incorporates dynamics of data, leading to compatible representatives.\n",
      "----------- 285 ------------\n",
      "The Boolean Satisfiability (SAT) problem is the canonical NP-complete problem and is fundamental to computer science, with a wide array of applications in planning, verification, and theorem proving. Developing and evaluating practical SAT solvers relies on extensive empirical testing on a set of real-world benchmark formulas. However, the availability of such real-world SAT formulas is limited. While these benchmark formulas can be augmented with synthetically generated ones, existing approaches for doing so are heavily hand-crafted and fail to simultaneously capture a wide range of characteristics exhibited by real-world SAT instances. In this work, we present G2SAT, the first deep generative framework that learns to generate SAT formulas from a given set of input formulas. Our key insight is that SAT formulas can be transformed into latent bipartite graph representations which we model using a specialized deep generative neural network. We show that G2SAT can generate SAT formulas that closely resemble given real-world SAT instances, as measured by both graph metrics and SAT solver behavior. Further, we show that our synthetic SAT formulas could be used to improve SAT solver performance on real-world benchmarks, which opens up new opportunities for the continued development of SAT solvers and a deeper understanding of their performance.\n",
      "----------- 286 ------------\n",
      "Continual lifelong learning is essential to many applications. In this paper, we propose a simple but effective approach to continual deep learning. Our approach leverages the principles of deep model compression, critical weights selection, and progressive networks expansion. By enforcing their integration in an iterative manner, we introduce an incremental learning method that is scalable to the number of sequential tasks in a continual learning process. Our approach is easy to implement and owns several favorable characteristics. First, it can avoid forgetting (i.e., learn new tasks while remembering all previous tasks). Second, it allows model expansion but can maintain the model compactness when handling sequential tasks. Besides, through our compaction and selection/expansion mechanism, we show that the knowledge accumulated through learning previous tasks is helpful to build a better model for the new tasks compared to training the models independently with tasks. Experimental results show that our approach can incrementally learn a deep model tackling multiple tasks without forgetting, while the model compactness is maintained with the performance more satisfiable than individual task training.\n",
      "----------- 287 ------------\n",
      "Sequence classification algorithms, such as SVM, require a definition of distance (similarity) measure between two sequences. A commonly used notion of similarity is the number of matches between k-mers (k-length subsequences) in the two sequences. Extending this definition, by considering two k-mers to match if their distance is at most m, yields better classification performance. This, however, makes the problem computationally much more complex. Known algorithms to compute this similarity have computational complexity that render them applicable only for small values of k and m. In this work, we develop novel techniques to efficiently and accurately estimate the pairwise similarity score, which enables us to use much larger values of k and m, and get higher predictive accuracy. This opens up a broad avenue of applying this classification approach to audio, images, and text sequences. Our algorithm achieves excellent approximation performance with theoretical guarantees. In the process we solve an open combinatorial problem, which was posed as a major hindrance to the scalability of existing solutions. We give analytical bounds on quality and runtime of our algorithm and report its empirical performance on real world biological and music sequences datasets.\n",
      "----------- 288 ------------\n",
      "Community detection, which focuses on clustering nodes or detecting communities in (mostly) a single network, is a problem of considerable practical interest and has received a great deal of attention in the research community. While being able to cluster within a network is important, there are emerging needs to be able to \\emph{cluster multiple networks}. This is largely motivated by the routine collection of network data that are generated from potentially different populations. These networks may or may not have node correspondence. When node correspondence is present, we cluster networks by summarizing a network by its graphon estimate, whereas when node correspondence is not present, we propose a novel solution for clustering such networks by associating a computationally feasible feature vector to each network based on trace of powers of the adjacency matrix. We illustrate our methods using both simulated and real data sets, and theoretical justifications are provided in terms of consistency.\n",
      "----------- 289 ------------\n",
      "Stochastic gradient methods for machine learning and optimization problems are usually analyzed assuming data points are sampled *with* replacement. In contrast, sampling *without* replacement is far less understood, yet in practice it is very common, often easier to implement, and usually performs better. In this paper, we provide competitive convergence guarantees for without-replacement sampling under several scenarios, focusing on the natural regime of few passes over the data. Moreover, we describe a useful application of these results in the context of distributed optimization with randomly-partitioned data, yielding a nearly-optimal algorithm for regularized least squares (in terms of both communication complexity and runtime complexity) under broad parameter regimes. Our proof techniques combine ideas from stochastic optimization, adversarial online learning and transductive learning theory, and can potentially be applied to other stochastic optimization and learning problems.\n",
      "----------- 290 ------------\n",
      "The quest for algorithms that enable cognitive abilities is an important part of machine learning. A common trait in many recently investigated cognitive-like tasks is that they take into account different data modalities, such as visual and textual input. In this paper we propose a novel and generally applicable form of attention mechanism that learns high-order correlations between various data modalities. We show that high-order correlations effectively direct the appropriate attention to the relevant elements in the different data modalities that are required to solve the joint task. We demonstrate the effectiveness of our high-order attention mechanism on the task of visual question answering (VQA), where we achieve state-of-the-art performance on the standard VQA dataset.\n",
      "----------- 291 ------------\n",
      "The loss functions of deep neural networks are complex and their geometric properties are not well understood. We show that the optima of these complex loss functions are in fact connected by simple curves, over which training and test accuracy are nearly constant. We introduce a training procedure to discover these high-accuracy pathways between modes. Inspired by this new geometric insight, we also propose a new ensembling method entitled Fast Geometric Ensembling (FGE). Using FGE we can train high-performing ensembles in the time required to train a single model. We achieve improved performance compared to the recent state-of-the-art Snapshot Ensembles, on CIFAR-10, CIFAR-100, and ImageNet.\n",
      "----------- 292 ------------\n",
      "Machine understanding of complex images is a key goal of artificial intelligence. One challenge underlying this task is that visual scenes contain multiple inter-related objects, and that global context plays an important role in interpreting the scene. A natural modeling framework for capturing such effects is structured prediction, which optimizes over complex labels, while modeling within-label interactions. However, it is unclear what principles should guide the design of a structured prediction model that utilizes the power of deep learning components. Here we propose a design principle for such architectures that follows from a natural requirement of permutation invariance. We prove a necessary and sufficient characterization for architectures that follow this invariance, and discuss its implication on model design. Finally, we show that the resulting model achieves new state of the art results on the Visual Genome scene graph labeling benchmark, outperforming all recent approaches.\n",
      "----------- 293 ------------\n",
      "Effective coordination is crucial to solve multi-agent collaborative (MAC) problems. While centralized reinforcement learning methods can optimally solve small MAC instances, they do not scale to large problems and they fail to generalize to scenarios different from those seen during training. In this paper, we consider MAC problems with some intrinsic notion of locality (e.g., geographic proximity) such that interactions between agents and tasks are locally limited. By leveraging this property, we introduce a novel structured prediction approach to assign agents to tasks. At each step, the assignment is obtained by solving a centralized optimization problem (the inference procedure) whose objective function is parameterized by a learned scoring model. We propose different combinations of inference procedures and scoring models able to represent coordination patterns of increasing complexity. The resulting assignment policy can be efficiently learned on small problem instances and readily reused in problems with more agents and tasks (i.e., zero-shot generalization). We report experimental results on a toy search and rescue problem and on several target selection scenarios in StarCraft: Brood War, in which our model significantly outperforms strong rule-based baselines on instances with 5 times more agents and tasks than those seen during training.\n",
      "----------- 294 ------------\n",
      "Overfitting is the bane of data analysts, even when data are plentiful. Formal approaches to understanding this problem focus on statistical inference and generalization of individual analysis procedures. Yet the practice of data analysis is an inherently interactive and adaptive process: new analyses and hypotheses are proposed after seeing the results of previous ones, parameters are tuned on the basis of obtained results, and datasets are shared and reused. An investigation of this gap has recently been initiated by the authors in (Dwork et al., 2014), where we focused on the problem of estimating expectations of adaptively chosen functions.In this paper, we give a simple and practical method for reusing a holdout (or testing) set to validate the accuracy of hypotheses produced by a learning algorithm operating on a training set. Reusing a holdout set adaptively multiple times can easily lead to overfitting to the holdout set itself. We give an algorithm that enables the validation of a large number of adaptively chosen hypotheses, while provably avoiding overfitting. We illustrate the advantages of our algorithm over the standard use of the holdout set via a simple synthetic experiment.We also formalize and address the general problem of data reuse in adaptive data analysis. We show how the differential-privacy based approach in (Dwork et al., 2014) is applicable much more broadly to adaptive data analysis. We then show that a simple approach based on description length can also be used to give guarantees of statistical validity in adaptive settings. Finally, we demonstrate that these incomparable approaches can be unified via the notion of approximate max-information that we introduce. This, in particular, allows the preservation of statistical validity guarantees even when an analyst adaptively composes algorithms which have guarantees based on either of the two approaches.\n",
      "----------- 295 ------------\n",
      "A key challenge in sequential decision problems is to determine how many samples are needed for an agent to make reliable decisions with good probabilistic guarantees. We introduce Hoeffding-like concentration inequalities that hold for a random, adaptively chosen number of samples. Our inequalities are tight under natural assumptions and can greatly simplify the analysis of common sequential decision problems. In particular, we apply them to sequential hypothesis testing, best arm identification, and sorting. The resulting algorithms rival or exceed the state of the art both theoretically and empirically.\n",
      "----------- 296 ------------\n",
      "Recurrent networks of spiking neurons (RSNNs) underlie the astounding computing and learning capabilities of the brain. But computing and learning capabilities of RSNN models have remained poor, at least in comparison with ANNs. We address two possible reasons for that. One is that RSNNs in the brain are not randomly connected or designed according to simple rules, and they do not start learning as a tabula rasa network. Rather, RSNNs in the brain were optimized for their tasks through evolution, development, and prior experience. Details of these optimization processes are largely unknown. But their functional contribution can be approximated through powerful optimization methods, such as backpropagation through time (BPTT). A second major mismatch between RSNNs in the brain and models is that the latter only show a small fraction of the dynamics of neurons and synapses in the brain. We include neurons in our RSNN model that reproduce one prominent dynamical process of biological neurons that takes place at the behaviourally relevant time scale of seconds: neuronal adaptation. We denote these networks as LSNNs because of their Long short-term memory. The inclusion of adapting neurons drastically increases the computing and learning capability of RSNNs if they are trained and configured by deep learning (BPTT combined with a rewiring algorithm that optimizes the network architecture). In fact, the computational performance of these RSNNs approaches for the first time that of LSTM networks. In addition RSNNs with adapting neurons can acquire abstract knowledge from prior learning in a Learning-to-Learn (L2L) scheme, and transfer that knowledge in order to learn new but related tasks from very few examples. We demonstrate this for supervised learning and reinforcement learning.\n",
      "----------- 297 ------------\n",
      "Many measurement modalities arise from well-understood physical processes and result in information-rich but difficult-to-interpret data. Much of this data still requires laborious human interpretation. This is the case in nuclear magnetic resonance (NMR) spectroscopy, where the observed spectrum of a molecule provides a distinguishing fingerprint of its bond structure. Here we solve the resulting inverse problem: given a molecular formula and a spectrum, can we infer the chemical structure? We show for a wide variety of molecules we can quickly compute the correct molecular structure, and can detect with reasonable certainty when our method cannot. We treat this as a problem of graph-structured prediction, where armed with per-vertex information on a subset of the vertices, we infer the edges and edge types. We frame the problem as a Markov decision process (MDP) and incrementally construct molecules one bond at a time, training a deep neural network via imitation learning, where we learn to imitate a subisomorphic oracle which knows which remaining bonds are correct. Our method is fast, accurate, and is the first among recent chemical-graph generation approaches to exploit per-vertex information and generate graphs with vertex constraints. Our method points the way towards automation of molecular structure identification and potentially active learning for spectroscopy.\n",
      "----------- 298 ------------\n",
      "Deep learning has the potential to revolutionize quantum chemistry as it is ideally suited to learn representations for structured data and speed up the exploration of chemical space. While convolutional neural networks have proven to be the first choice for images, audio and video data, the atoms in molecules are not restricted to a grid. Instead, their precise locations contain essential physical information, that would get lost if discretized. Thus, we propose to use continuous-filter convolutional layers to be able to model local correlations without requiring the data to lie on a grid. We apply those layers in SchNet: a novel deep learning architecture modeling quantum interactions in molecules. We obtain a joint model for the total energy and interatomic forces that follows fundamental quantum-chemical principles. Our architecture achieves state-of-the-art performance for benchmarks of equilibrium molecules and molecular dynamics trajectories. Finally, we introduce a more challenging benchmark with chemical and structural variations that suggests the path for further work.\n",
      "----------- 299 ------------\n",
      "Despite the exploding interest in graph neural networks there has been little effort to verify and improve their robustness. This is even more alarming given recent findings showing that they are extremely vulnerable to adversarial attacks on both the graph structure and the node attributes. We propose the first method for verifying certifiable (non-)robustness to graph perturbations for a general class of models that includes graph neural networks and label/feature propagation. By exploiting connections to PageRank and Markov decision processes our certificates can be efficiently (and under many threat models exactly) computed. Furthermore, we investigate robust training procedures that increase the number of certifiably robust nodes while maintaining or improving the clean predictive accuracy.\n"
     ]
    }
   ],
   "source": [
    "for i, a in enumerate(np.array(allAbs)[np.argsort(D[:,-1])][:300]):\n",
    "    print(\"----------- %s ------------\" % str(i))\n",
    "    print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "ix = list(np.where(L==0)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "np.random.shuffle(ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['2016_9', '2019_451', '2019_645', '2016_285', '2018_430',\n",
       "       '2019_138', '2019_922', '2017_6', '2019_274', '2017_377'],\n",
       "      dtype='<U9')"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(didList)[ix][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------0------------\n",
      "We study the problem of 3D object generation. We propose a novel framework, namely 3D Generative Adversarial Network (3D-GAN), which generates 3D objects from a probabilistic space by leveraging recent advances in volumetric convolutional networks and generative adversarial nets. The benefits of our model are three-fold: first, the use of an adversarial criterion, instead of traditional heuristic criteria, enables the generator to capture object structure implicitly and to synthesize high-quality 3D objects; second, the generator establishes a mapping from a low-dimensional probabilistic space to the space of 3D objects, so that we can sample objects without a reference image or CAD models, and explore the 3D object manifold; third, the adversarial discriminator provides a powerful 3D shape descriptor which, learned without supervision, has wide applications in 3D object recognition. Experiments demonstrate that our method generates high-quality 3D objects, and our unsupervisedly learned features achieve impressive performance on 3D object recognition, comparable with those of supervised learning methods.\n",
      "----------1------------\n",
      "Inner product-based convolution has been the founding stone of convolutional neural networks (CNNs), enabling end-to-end learning of visual representation. By generalizing inner product with a bilinear matrix, we propose the neural similarity which serves as a learnable parametric similarity measure for CNNs. Neural similarity naturally generalizes the convolution and enhances flexibility. Further, we consider the neural similarity learning (NSL) in order to learn the neural similarity adaptively from training data. Specifically, we propose two different ways of learning the neural similarity: static NSL and dynamic NSL. Interestingly, dynamic neural similarity makes the CNN become a dynamic inference network. By regularizing the bilinear matrix, NSL can be viewed as learning the shape of kernel and the similarity measure simultaneously. We further justify the effectiveness of NSL with a theoretical viewpoint. Most importantly, NSL shows promising performance in visual recognition and few-shot learning, validating the superiority of NSL over the inner product-based convolution counterparts.\n",
      "----------2------------\n",
      "Domain Adaptation (DA) approaches achieved significant improvements in a wide range of machine learning and computer vision tasks (i.e., classification, detection, and segmentation). However, as far as we are aware, there are few methods yet to achieve domain adaptation directly on 3D point cloud data. The unique challenge of point cloud data lies in its abundant spatial geometric information, and the semantics of the whole object is contributed by including regional geometric structures. Specifically, most general-purpose DA methods that struggle for global feature alignment and ignore local geometric information are not suitable for 3D domain alignment. In this paper, we propose a novel 3D Domain Adaptation Network for point cloud data (PointDAN). PointDAN jointly aligns the global and local features in multi-level. For local alignment, we propose Self-Adaptive (SA) node module with an adjusted receptive field to model the discriminative local structures for aligning domains. To represent hierarchically scaled features, node-attention module is further introduced to weight the relationship of SA nodes across objects and domains. For global alignment, an adversarial-training strategy is employed to learn and align global features across domains. Since there is no common evaluation benchmark for 3D point cloud DA scenario, we build a general benchmark (i.e., PointDA-10) extracted from three popular 3D object/scene datasets (i.e., ModelNet, ShapeNet and ScanNet) for cross-domain 3D objects classification fashion. Extensive experiments on PointDA-10 illustrate the superiority of our model over the state-of-the-art general-purpose DA methods.\n",
      "----------3------------\n",
      "We develop a probabilistic framework for deep learning based on the Deep Rendering Mixture Model (DRMM), a new generative probabilistic model that explicitly capture variations in data due to latent task nuisance variables. We demonstrate that max-sum inference in the DRMM yields an algorithm that exactly reproduces the operations in deep convolutional neural networks (DCNs), providing a first principles derivation. Our framework provides new insights into the successes and shortcomings of DCNs as well as a principled route to their improvement. DRMM training via the Expectation-Maximization (EM) algorithm is a powerful alternative to DCN back-propagation, and initial training results are promising. Classification based on the DRMM and other variants outperforms DCNs in supervised digit classification, training 2-3x faster while achieving similar accuracy. Moreover, the DRMM is applicable to semi-supervised and unsupervised learning tasks, achieving results that are state-of-the-art in several categories on the MNIST benchmark and comparable to state of the art on the CIFAR10 benchmark.\n",
      "----------4------------\n",
      "Generative adversarial networks (GANs) have achieved significant success in generating real-valued data. However, the discrete nature of text hinders the application of GAN to text-generation tasks. Instead of using the standard GAN objective, we propose to improve text-generation GAN via a novel approach inspired by optimal transport. Specifically, we consider matching the latent feature distributions of real and synthetic sentences using a novel metric, termed the feature-mover's distance (FMD). This formulation leads to a highly discriminative critic and easy-to-optimize objective, overcoming the mode-collapsing and brittle-training problems in existing methods. Extensive experiments are conducted on a variety of tasks to evaluate the proposed model empirically, including unconditional text generation, style transfer from non-parallel text, and unsupervised cipher cracking. The proposed model yields superior performance, demonstrating wide applicability and effectiveness.\n",
      "----------5------------\n",
      "Despite recent progress in computer vision, image retrieval remains a challenging open problem. Numerous variations such as view angle, lighting and occlusion make it difficult to design models that are both robust and efficient. Many leading methods traverse the nearest neighbor graph to exploit higher order neighbor information and uncover the highly complex underlying manifold. In this work we propose a different approach where we leverage graph convolutional networks to directly encode neighbor information into image descriptors. We further leverage ideas from clustering and manifold learning, and introduce an unsupervised loss based on pairwise separation of image similarities. Empirically, we demonstrate that our model is able to successfully learn a new descriptor space that significantly improves retrieval accuracy, while still allowing efficient inner product inference. Experiments on five public benchmarks show highly competitive performance with up to 24\\% relative improvement in mAP over leading baselines. Full code for this work is available here: https://github.com/layer6ai-labs/GSS.\n",
      "----------6------------\n",
      "Learning representations that accurately capture long-range dependencies in sequential inputs --- including text, audio, and genomic data --- is a key problem in deep learning. Feed-forward convolutional models capture only feature interactions within finite receptive fields while recurrent architectures can be slow and difficult to train due to vanishing gradients. Here, we propose Temporal Feature-Wise Linear Modulation (TFiLM) --- a novel architectural component inspired by adaptive batch normalization and its extensions --- that uses a recurrent neural network to alter the activations of a convolutional model. This approach expands the receptive field of convolutional sequence models with minimal computational overhead. Empirically, we find that TFiLM significantly improves the learning speed and accuracy of feed-forward neural networks on a range of generative and discriminative learning tasks, including text classification and audio super-resolution.\n",
      "----------7------------\n",
      "Synthesizing realistic profile faces is promising for more efficiently training deep pose-invariant models for large-scale unconstrained face recognition, by populating samples with extreme poses and avoiding tedious annotations. However, learning from synthetic faces may not achieve the desired performance due to the discrepancy between distributions of the synthetic and real face images. To narrow this gap, we propose a Dual-Agent Generative Adversarial Network (DA-GAN) model, which can improve the realism of a face simulator's output using unlabeled real faces, while preserving the identity information during the realism refinement. The dual agents are specifically designed for distinguishing real v.s. fake and identities simultaneously. In particular, we employ an off-the-shelf 3D face model as a simulator to generate profile face images with varying poses. DA-GAN leverages a fully convolutional network as the generator to generate high-resolution images and an auto-encoder as the discriminator with the dual agents. Besides the novel architecture, we make several key modifications to the standard GAN to preserve pose and texture, preserve identity and stabilize training process: (i) a pose perception loss; (ii) an identity perception loss; (iii) an adversarial loss with a boundary equilibrium regularization term. Experimental results show that DA-GAN not only presents compelling perceptual results but also significantly outperforms state-of-the-arts on the large-scale and challenging NIST IJB-A unconstrained face recognition benchmark. In addition, the proposed DA-GAN is also promising as a new approach for solving generic transfer learning problems more effectively.\n",
      "----------8------------\n",
      "This paper studies the task of one-shot fine-grained recognition, which suffers from the problem of data scarcity of novel fine-grained classes. To alleviate this problem, a off-the-shelf image generator can be applied to synthesize additional images to help one-shot learning. However, such synthesized images may not be helpful in one-shot fine-grained recognition, due to a large domain discrepancy between synthesized and original images. To this end, this paper proposes a meta-learning framework to reinforce the generated images by original images so that these images can facilitate one-shot learning. Specifically, the generic image generator is updated by few training instances of novel classes; and a Meta Image Reinforcing Network (MetaIRNet) is proposed to conduct one-shot fine-grained recognition as well as image reinforcement. The model is trained in an end-to-end manner, and our experiments demonstrate consistent improvement over baseline on one-shot fine-grained image classification benchmarks.\n",
      "----------9------------\n",
      "Convolution as inner product has been the founding basis of convolutional neural networks (CNNs) and the key to end-to-end visual representation learning. Benefiting from deeper architectures, recent CNNs have demonstrated increasingly strong representation abilities. Despite such improvement, the increased depth and larger parameter space have also led to challenges in properly training a network. In light of such challenges, we propose hyperspherical convolution (SphereConv), a novel learning framework that gives angular representations on hyperspheres. We introduce SphereNet, deep hyperspherical convolution networks that are distinct from conventional inner product based convolutional networks. In particular, SphereNet adopts SphereConv as its basic convolution operator and is supervised by generalized angular softmax loss - a natural loss formulation under SphereConv. We show that SphereNet can effectively encode discriminative representation and alleviate training difficulty, leading to easier optimization, faster convergence and comparable (even better) classification accuracy over convolutional counterparts. We also provide some theoretical insights for the advantages of learning on hyperspheres. In addition, we introduce the learnable SphereConv, i.e., a natural improvement over prefixed SphereConv, and SphereNorm, i.e., hyperspherical learning as a normalization method. Experiments have verified our conclusions.\n"
     ]
    }
   ],
   "source": [
    "for i, a in enumerate(np.array(abstractList)[ix][:10]):\n",
    "    print(\"----------{}------------\".format(i))\n",
    "    print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = KMeans(30).fit_predict(emb)\n",
    "#L = GaussianMixture(15).fit_predict(emb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 30 artists>"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.bar(list(Counter(L).keys()), list(Counter(L).values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "bool0, bool1, bool2, bool3, bool4, bool5 = L==0, L==1, L==2, L==3, L==4, L==5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Embeddings = [emb[bool0]] + [emb[bool1]] + [emb[bool2]] + [emb[bool3]] + [emb[bool4]] + [emb[bool5]]\n",
    "\n",
    "color = [u\"green\"] * sum(bool0) + [u\"yellow\"] * sum(bool1) + [u\"brown\"] * sum(bool2) + [u\"red\"] * sum(bool3) + [u\"blue\"] * sum(bool4) + [u\"cyan\"] * sum(bool5)\n",
    "\n",
    "label = [u\"0\"] * sum(bool0) + [u\"1\"] * sum(bool1) + [u\"2\"] * sum(bool2) + [u\"3\"] * sum(bool3) + [u\"4\"] * sum(bool4) + [u\"5\"] * sum(bool5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.asarray(Embeddings)\n",
    "\n",
    "C = np.asarray(color)\n",
    "L = np.asarray(label)\n",
    "T = np.asarray(titleList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4088,)"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.concatenate(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5min 45s, sys: 15.7 s, total: 6min\n",
      "Wall time: 49.9 s\n"
     ]
    }
   ],
   "source": [
    "tsne_input = X\n",
    "tsne_input = pd.DataFrame(tsne_input, index=L)\n",
    "tsne_input = tsne_input\n",
    "tsne = TSNE(perplexity=1200)\n",
    "%time tsne_vectors = tsne.fit_transform(tsne_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f829f0624e0>"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#df = sns.load_dataset('iris')\n",
    " \n",
    "# Basic 2D density plot\n",
    "sns.set_style(\"white\")\n",
    "sns.kdeplot(tsne_vectors[:,0], tsne_vectors[:,1])\n",
    "#sns.plt.show()\n",
    " \n",
    "# Custom it with the same argument as 1D density plot\n",
    "sns.kdeplot(tsne_vectors[:,0], tsne_vectors[:,1], cmap=\"Reds\", shade=True, bw=.15)\n",
    " \n",
    "# Some features are characteristic of 2D: color palette and wether or not color the lowest range\n",
    "sns.kdeplot(tsne_vectors[:,0], tsne_vectors[:,1], cmap=\"Blues\", shade=True, shade_lowest=True, )\n",
    "#sns.plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_c = tsne_vectors\n",
    "\n",
    "tsne_df = pd.DataFrame(X_c,\n",
    "                            index=pd.Index(tsne_input.index),\n",
    "                            columns=[u'x_coord', u'y_coord'])\n",
    "tsne_df[u'did'] = tsne_df.index\n",
    "tsne_df[u'C'] = np.atleast_2d(C).T\n",
    "tsne_df[u'label'] = np.atleast_2d(L).T\n",
    "tsne_df[u'title'] = np.atleast_2d(T).T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "color = [u\"green\", u\"yellow\", u\"brown\", u\"red\", u\"blue\", u\"cyan\"]\n",
    "#color = [u\"green\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bk-root\">\n",
       "        <a href=\"https://bokeh.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span id=\"4693\">Loading BokehJS ...</span>\n",
       "    </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "(function(root) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  var force = true;\n",
       "\n",
       "  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n",
       "    root._bokeh_onload_callbacks = [];\n",
       "    root._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "  var JS_MIME_TYPE = 'application/javascript';\n",
       "  var HTML_MIME_TYPE = 'text/html';\n",
       "  var EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n",
       "  var CLASS_NAME = 'output_bokeh rendered_html';\n",
       "\n",
       "  /**\n",
       "   * Render data to the DOM node\n",
       "   */\n",
       "  function render(props, node) {\n",
       "    var script = document.createElement(\"script\");\n",
       "    node.appendChild(script);\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when an output is cleared or removed\n",
       "   */\n",
       "  function handleClearOutput(event, handle) {\n",
       "    var cell = handle.cell;\n",
       "\n",
       "    var id = cell.output_area._bokeh_element_id;\n",
       "    var server_id = cell.output_area._bokeh_server_id;\n",
       "    // Clean up Bokeh references\n",
       "    if (id != null && id in Bokeh.index) {\n",
       "      Bokeh.index[id].model.document.clear();\n",
       "      delete Bokeh.index[id];\n",
       "    }\n",
       "\n",
       "    if (server_id !== undefined) {\n",
       "      // Clean up Bokeh references\n",
       "      var cmd = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n",
       "      cell.notebook.kernel.execute(cmd, {\n",
       "        iopub: {\n",
       "          output: function(msg) {\n",
       "            var id = msg.content.text.trim();\n",
       "            if (id in Bokeh.index) {\n",
       "              Bokeh.index[id].model.document.clear();\n",
       "              delete Bokeh.index[id];\n",
       "            }\n",
       "          }\n",
       "        }\n",
       "      });\n",
       "      // Destroy server and session\n",
       "      var cmd = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n",
       "      cell.notebook.kernel.execute(cmd);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when a new output is added\n",
       "   */\n",
       "  function handleAddOutput(event, handle) {\n",
       "    var output_area = handle.output_area;\n",
       "    var output = handle.output;\n",
       "\n",
       "    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n",
       "    if ((output.output_type != \"display_data\") || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n",
       "      return\n",
       "    }\n",
       "\n",
       "    var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n",
       "\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n",
       "      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n",
       "      // store reference to embed id on output_area\n",
       "      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
       "    }\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
       "      var bk_div = document.createElement(\"div\");\n",
       "      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "      var script_attrs = bk_div.children[0].attributes;\n",
       "      for (var i = 0; i < script_attrs.length; i++) {\n",
       "        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
       "      }\n",
       "      // store reference to server id on output_area\n",
       "      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function register_renderer(events, OutputArea) {\n",
       "\n",
       "    function append_mime(data, metadata, element) {\n",
       "      // create a DOM node to render to\n",
       "      var toinsert = this.create_output_subarea(\n",
       "        metadata,\n",
       "        CLASS_NAME,\n",
       "        EXEC_MIME_TYPE\n",
       "      );\n",
       "      this.keyboard_manager.register_events(toinsert);\n",
       "      // Render to node\n",
       "      var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
       "      render(props, toinsert[toinsert.length - 1]);\n",
       "      element.append(toinsert);\n",
       "      return toinsert\n",
       "    }\n",
       "\n",
       "    /* Handle when an output is cleared or removed */\n",
       "    events.on('clear_output.CodeCell', handleClearOutput);\n",
       "    events.on('delete.Cell', handleClearOutput);\n",
       "\n",
       "    /* Handle when a new output is added */\n",
       "    events.on('output_added.OutputArea', handleAddOutput);\n",
       "\n",
       "    /**\n",
       "     * Register the mime type and append_mime function with output_area\n",
       "     */\n",
       "    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
       "      /* Is output safe? */\n",
       "      safe: true,\n",
       "      /* Index of renderer in `output_area.display_order` */\n",
       "      index: 0\n",
       "    });\n",
       "  }\n",
       "\n",
       "  // register the mime type if in Jupyter Notebook environment and previously unregistered\n",
       "  if (root.Jupyter !== undefined) {\n",
       "    var events = require('base/js/events');\n",
       "    var OutputArea = require('notebook/js/outputarea').OutputArea;\n",
       "\n",
       "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
       "      register_renderer(events, OutputArea);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  \n",
       "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    root._bokeh_timeout = Date.now() + 5000;\n",
       "    root._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  var NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded() {\n",
       "    var el = document.getElementById(\"4693\");\n",
       "    if (el != null) {\n",
       "      el.textContent = \"BokehJS is loading...\";\n",
       "    }\n",
       "    if (root.Bokeh !== undefined) {\n",
       "      if (el != null) {\n",
       "        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n",
       "      }\n",
       "    } else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(display_loaded, 100)\n",
       "    }\n",
       "  }\n",
       "\n",
       "\n",
       "  function run_callbacks() {\n",
       "    try {\n",
       "      root._bokeh_onload_callbacks.forEach(function(callback) {\n",
       "        if (callback != null)\n",
       "          callback();\n",
       "      });\n",
       "    } finally {\n",
       "      delete root._bokeh_onload_callbacks\n",
       "    }\n",
       "    console.debug(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(css_urls, js_urls, callback) {\n",
       "    if (css_urls == null) css_urls = [];\n",
       "    if (js_urls == null) js_urls = [];\n",
       "\n",
       "    root._bokeh_onload_callbacks.push(callback);\n",
       "    if (root._bokeh_is_loading > 0) {\n",
       "      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    root._bokeh_is_loading = css_urls.length + js_urls.length;\n",
       "\n",
       "    function on_load() {\n",
       "      root._bokeh_is_loading--;\n",
       "      if (root._bokeh_is_loading === 0) {\n",
       "        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n",
       "        run_callbacks()\n",
       "      }\n",
       "    }\n",
       "\n",
       "    function on_error() {\n",
       "      console.error(\"failed to load \" + url);\n",
       "    }\n",
       "\n",
       "    for (var i = 0; i < css_urls.length; i++) {\n",
       "      var url = css_urls[i];\n",
       "      const element = document.createElement(\"link\");\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error;\n",
       "      element.rel = \"stylesheet\";\n",
       "      element.type = \"text/css\";\n",
       "      element.href = url;\n",
       "      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n",
       "      document.body.appendChild(element);\n",
       "    }\n",
       "\n",
       "    for (var i = 0; i < js_urls.length; i++) {\n",
       "      var url = js_urls[i];\n",
       "      var element = document.createElement('script');\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error;\n",
       "      element.async = false;\n",
       "      element.src = url;\n",
       "      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.head.appendChild(element);\n",
       "    }\n",
       "  };var element = document.getElementById(\"4693\");\n",
       "  if (element == null) {\n",
       "    console.error(\"Bokeh: ERROR: autoload.js configured with elementid '4693' but no matching script tag was found. \")\n",
       "    return false;\n",
       "  }\n",
       "\n",
       "  function inject_raw_css(css) {\n",
       "    const element = document.createElement(\"style\");\n",
       "    element.appendChild(document.createTextNode(css));\n",
       "    document.body.appendChild(element);\n",
       "  }\n",
       "\n",
       "  \n",
       "  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-1.4.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.4.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.4.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-1.4.0.min.js\"];\n",
       "  var css_urls = [];\n",
       "  \n",
       "\n",
       "  var inline_js = [\n",
       "    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "    function(Bokeh) {\n",
       "    \n",
       "    \n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    \n",
       "    if (root.Bokeh !== undefined || force === true) {\n",
       "      \n",
       "    for (var i = 0; i < inline_js.length; i++) {\n",
       "      inline_js[i].call(root, root.Bokeh);\n",
       "    }\n",
       "    if (force === true) {\n",
       "        display_loaded();\n",
       "      }} else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!root._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      root._bokeh_failed_load = true;\n",
       "    } else if (force !== true) {\n",
       "      var cell = $(document.getElementById(\"4693\")).parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "\n",
       "  }\n",
       "\n",
       "  if (root._bokeh_is_loading === 0) {\n",
       "    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(css_urls, js_urls, function() {\n",
       "      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(window));"
      ],
      "application/vnd.bokehjs_load.v0+json": "\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  var force = true;\n\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n  \n\n  \n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  var NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded() {\n    var el = document.getElementById(\"4693\");\n    if (el != null) {\n      el.textContent = \"BokehJS is loading...\";\n    }\n    if (root.Bokeh !== undefined) {\n      if (el != null) {\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(display_loaded, 100)\n    }\n  }\n\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n\n    function on_error() {\n      console.error(\"failed to load \" + url);\n    }\n\n    for (var i = 0; i < css_urls.length; i++) {\n      var url = css_urls[i];\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }\n\n    for (var i = 0; i < js_urls.length; i++) {\n      var url = js_urls[i];\n      var element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n  };var element = document.getElementById(\"4693\");\n  if (element == null) {\n    console.error(\"Bokeh: ERROR: autoload.js configured with elementid '4693' but no matching script tag was found. \")\n    return false;\n  }\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  \n  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-1.4.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.4.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.4.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-1.4.0.min.js\"];\n  var css_urls = [];\n  \n\n  var inline_js = [\n    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\n    function(Bokeh) {\n    \n    \n    }\n  ];\n\n  function run_inline_js() {\n    \n    if (root.Bokeh !== undefined || force === true) {\n      \n    for (var i = 0; i < inline_js.length; i++) {\n      inline_js[i].call(root, root.Bokeh);\n    }\n    if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      var cell = $(document.getElementById(\"4693\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(css_urls, js_urls, function() {\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BokehDeprecationWarning: 'legend' keyword is deprecated, use explicit 'legend_label', 'legend_field', or 'legend_group' keywords instead\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "  <div class=\"bk-root\" id=\"7364e8b3-44f5-42a7-a0e3-1aa7a9a6cbeb\" data-root-id=\"4695\"></div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "(function(root) {\n",
       "  function embed_document(root) {\n",
       "    \n",
       "  var docs_json = {\"90aa50bb-6d4b-4ecf-b5d8-9ad0392a96b7\":{\"roots\":{\"references\":[{\"attributes\":{\"below\":[{\"id\":\"4706\",\"type\":\"LinearAxis\"}],\"center\":[{\"id\":\"4710\",\"type\":\"Grid\"},{\"id\":\"4715\",\"type\":\"Grid\"},{\"id\":\"4743\",\"type\":\"Legend\"}],\"left\":[{\"id\":\"4711\",\"type\":\"LinearAxis\"}],\"outline_line_color\":{\"value\":null},\"plot_height\":800,\"plot_width\":800,\"renderers\":[{\"id\":\"4734\",\"type\":\"GlyphRenderer\"}],\"title\":{\"id\":\"4696\",\"type\":\"Title\"},\"toolbar\":{\"id\":\"4721\",\"type\":\"Toolbar\"},\"x_range\":{\"id\":\"4698\",\"type\":\"DataRange1d\"},\"x_scale\":{\"id\":\"4702\",\"type\":\"LinearScale\"},\"y_range\":{\"id\":\"4700\",\"type\":\"DataRange1d\"},\"y_scale\":{\"id\":\"4704\",\"type\":\"LinearScale\"}},\"id\":\"4695\",\"subtype\":\"Figure\",\"type\":\"Plot\"},{\"attributes\":{\"label\":{\"field\":\"label\"},\"renderers\":[{\"id\":\"4734\",\"type\":\"GlyphRenderer\"}]},\"id\":\"4744\",\"type\":\"LegendItem\"},{\"attributes\":{\"factors\":[\"green\",\"yellow\",\"brown\",\"red\",\"blue\",\"cyan\"],\"palette\":[\"#3288bd\",\"#99d594\",\"#e6f598\",\"#fee08b\",\"#fc8d59\",\"#d53e4f\"]},\"id\":\"4729\",\"type\":\"CategoricalColorMapper\"},{\"attributes\":{\"active_drag\":\"auto\",\"active_inspect\":\"auto\",\"active_multi\":null,\"active_scroll\":{\"id\":\"4717\",\"type\":\"WheelZoomTool\"},\"active_tap\":\"auto\",\"tools\":[{\"id\":\"4716\",\"type\":\"PanTool\"},{\"id\":\"4717\",\"type\":\"WheelZoomTool\"},{\"id\":\"4718\",\"type\":\"BoxZoomTool\"},{\"id\":\"4719\",\"type\":\"BoxSelectTool\"},{\"id\":\"4720\",\"type\":\"ResetTool\"},{\"id\":\"4727\",\"type\":\"HoverTool\"}]},\"id\":\"4721\",\"type\":\"Toolbar\"},{\"attributes\":{},\"id\":\"4716\",\"type\":\"PanTool\"},{\"attributes\":{},\"id\":\"4920\",\"type\":\"UnionRenderers\"},{\"attributes\":{},\"id\":\"4717\",\"type\":\"WheelZoomTool\"},{\"attributes\":{\"callback\":null,\"data\":{\"C\":[\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"brown\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\",\"cyan\"],\"did\":[\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\"],\"index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label\":[\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"2\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\"],\"title\":[\"Multimodal Model-Agnostic Meta-Learning via Task-Aware Modulation\",\"ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks\",\"Stochastic Shared Embeddings: Data-driven Regularization of Embedding Layers\",\"Unsupervised Scale-consistent Depth and Ego-motion Learning from Monocular Video\",\"Zero-shot Learning via Simultaneous Generating and Learning\",\"Ask not what AI can do, but what AI should do: Towards a framework of task delegability\",\"Stand-Alone Self-Attention in Vision Models\",\"High Fidelity Video Prediction with Large Stochastic Recurrent Neural Networks\",\"Unsupervised learning of object structure and dynamics from videos\",\"GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism\",\"Meta-Learning with Implicit Gradients\",\"Adversarial Examples Are Not Bugs, They Are Features\",\"Social-BiGAT: Multimodal Trajectory Forecasting using Bicycle-GAN and Graph Attention Networks\",\"FreeAnchor: Learning to Match Anchors for Visual Object Detection\",\"Private Hypothesis Selection\",\"Differentially Private Algorithms for Learning Mixtures of Separated Gaussians\",\"Average-Case Averages: Private Algorithms for Smooth Sensitivity and Mean Estimation\",\"Multi-Resolution Weak Supervision for Sequential Data\",\"DeepUSPS: Deep Robust Unsupervised Saliency Prediction via Self-supervision\",\"The Point Where Reality Meets Fantasy: Mixed Adversarial Generators for Image Splice Detection\",\"You Only Propagate Once: Accelerating Adversarial Training via Maximal Principle\",\"Imitation Learning from Observations by Minimizing Inverse Dynamics Disagreement\",\"Asymptotic Guarantees for Learning Generative Models with the Sliced-Wasserstein Distance\",\"Generalized Sliced Wasserstein Distances\",\"First Exit Time Analysis of Stochastic Gradient Descent Under Heavy-Tailed Gradient Noise\",\"Blind Super-Resolution Kernel Estimation using an Internal-GAN\",\"Noise-tolerant fair classification\",\"Generalization in Generative Adversarial Networks: A Novel Perspective from Privacy Protection\",\"Joint-task Self-supervised Learning for Temporal Correspondence\",\"Provable Gradient Variance Guarantees for Black-Box Variational Inference\",\"Divide and Couple: Using Monte Carlo Variational Objectives for Posterior Approximation\",\"Experience Replay for Continual Learning\",\"Deep ReLU Networks Have Surprisingly Few Activation Patterns\",\"Chasing Ghosts: Instruction Following as Bayesian State Tracking\",\"Block Coordinate Regularization by Denoising\",\"Reducing Noise in GAN Training with Variance Reduced Extragradient\",\"Learning Erdos-Renyi Random Graphs via Edge Detecting Queries\",\"A Primal-Dual link between GANs and Autoencoders\",\"muSSP: Efficient Min-cost Flow Algorithm for Multi-object Tracking\",\"Category Anchor-Guided Unsupervised Domain Adaptation for Semantic Segmentation\",\"Invert to Learn to Invert\",\"Equitable Stable Matchings in Quadratic Time\",\"Zero-Shot Semantic Segmentation\",\"Metric Learning for Adversarial Robustness\",\"DISN: Deep Implicit Surface Network for High-quality Single-view 3D Reconstruction\",\"Batched Multi-armed Bandits Problem\",\"vGraph: A Generative Model for Joint Community Detection and Node Representation Learning\",\"Differentially Private Bayesian Linear Regression\",\"Semantic Conditioned Dynamic Modulation for Temporal Sentence Grounding in Videos\",\"AGEM: Solving Linear Inverse Problems via Deep Priors and Sampling\",\"CPM-Nets: Cross Partial Multi-View Networks\",\"Learning to Predict Layout-to-image Conditional Convolutions for Semantic Image Synthesis\",\"Staying up to Date with Online Content Changes Using Reinforcement Learning for Scheduling\",\"SySCD: A System-Aware Parallel Coordinate Descent Algorithm\",\"Importance Weighted Hierarchical Variational Inference\",\"RSN: Randomized Subspace Newton\",\"Trust Region-Guided Proximal Policy Optimization\",\"Adversarial Self-Defense for Cycle-Consistent GANs\",\"Towards closing the gap between the theory and practice of SVRG\",\"Uniform Error Bounds for Gaussian Process Regression with Application to Safe Control\",\"ETNet: Error Transition Network for Arbitrary Style Transfer\",\"No Pressure! Addressing the Problem of Local Minima in Manifold Learning Algorithms\",\"Deep Equilibrium Models\",\"Saccader: Improving Accuracy of Hard Attention Models for Vision\",\"Multiway clustering via tensor block models\",\"Regret Minimization for Reinforcement Learning with Vectorial Feedback and Complex Objectives\",\"NAT: Neural Architecture Transformer for Accurate and Compact Architectures\",\"Selecting Optimal Decisions via Distributionally Robust Nearest-Neighbor Regression\",\"Network Pruning via Transformable Architecture Search\",\"Differentiable Cloth Simulation for Inverse Problems\",\"Poisson-Randomized Gamma Dynamical Systems\",\"Volumetric Correspondence Networks for Optical Flow\",\"Learning Conditional Deformable Templates with Convolutional Networks\",\"Fast Low-rank Metric Learning for Large-scale and High-dimensional Data\",\"Efficient Symmetric Norm Regression via Linear Sketching\",\"RUBi: Reducing Unimodal Biases for Visual Question Answering\",\"Why Can't I Dance in the Mall? Learning to Mitigate Scene Bias in Action Recognition\",\"NeurVPS: Neural Vanishing Point Scanning via Conic Convolution\",\"DATA: Differentiable ArchiTecture Approximation\",\"Learn, Imagine and Create: Text-to-Image Generation from Prior Knowledge\",\"Memory-oriented Decoder for Light Field Salient Object Detection\",\"Multi-label Co-regularization for Semi-supervised Facial Action Unit Recognition\",\"Correlated Uncertainty for Learning Dense Correspondences from Noisy Labels\",\"Powerset Convolutional Neural Networks\",\"Optimal Pricing in Repeated Posted-Price Auctions with Different Patience of the Seller and the Buyer\",\"An Accelerated Decentralized Stochastic Proximal Algorithm for Finite Sums\",\"Point-Voxel CNN for Efficient 3D Deep Learning\",\"Deep Learning without Weight Transport\",\"Combinatorial Bandits with Relative Feedback\",\"General Proximal Incremental Aggregated Gradient Algorithms: Better and Novel Results under General Scheme\",\"A Condition Number for Joint Optimization of Cycle-Consistent Networks\",\"Explicit Disentanglement of Appearance and Perspective in Generative Models\",\"Polynomial Cost of Adaptation for X-Armed Bandits\",\"Learning to Propagate for Graph Meta-Learning\",\"Secretary Ranking with Minimal Inversions\",\"Nonparametric Regressive Point Processes Based on Conditional Gaussian Processes\",\"Learning Perceptual Inference by Contrasting\",\"Selecting the independent coordinates of manifolds with large aspect ratios\",\"Region-specific Diffeomorphic Metric Mapping\",\"Deep Supervised Summarization: Algorithm and Application to Learning Instructions\",\"Scene Representation Networks: Continuous 3D-Structure-Aware Neural Scene Representations\",\"Reconciling \\u03bb-Returns with Experience Replay\",\"Control Batch Size and Learning Rate to Generalize Well: Theoretical and Empirical Evidence\",\"Non-Asymptotic Gap-Dependent Regret Bounds for Tabular MDPs\",\"A Graph Theoretic Framework of Recomputation Algorithms for Memory-Efficient Backpropagation\",\"Combinatorial Inference against Label Noise\",\"Value Propagation for Decentralized Networked Deep Multi-agent Reinforcement Learning\",\"Convolution with even-sized kernels and symmetric padding\",\"On The Classification-Distortion-Perception Tradeoff\",\"Optimal Statistical Rates for Decentralised Non-Parametric Regression with Linear Speed-Up\",\"Online sampling from log-concave distributions\",\"Envy-Free Classification\",\"Finding Friend and Foe in Multi-Agent Games\",\"Image Synthesis with a Single (Robust) Classifier\",\"Model Compression with Adversarial Robustness: A Unified Optimization Framework\",\"Cross-channel Communication Networks\",\"CondConv: Conditionally Parameterized Convolutions for Efficient Inference\",\"Regression Planning Networks\",\"Twin Auxilary Classifiers GAN\",\"Conditional Structure Generation through Graph Variational Generative Adversarial Nets\",\"Distributional Policy Optimization: An Alternative Approach for Continuous Control\",\"Sampling Sketches for Concave Sublinear Functions of Frequencies\",\"Deliberative Explanations: visualizing network insecurities\",\"Computing Full Conformal Prediction Set with Approximate Homotopy\",\"Failing Loudly: An Empirical Study of Methods for Detecting Dataset Shift\",\"Hierarchical Reinforcement Learning with Advantage-Based Auxiliary Rewards\",\"Multi-View Reinforcement Learning\",\"Cascade RPN: Delving into High-Quality Region Proposal Network with Adaptive Convolution\",\"Neural Diffusion Distance for Image Segmentation\",\"Fine-grained Optimization of Deep Neural Networks\",\"Extending Stein's unbiased risk estimator to train deep denoisers with correlated pairs of noisy images\",\"Fixing Implicit Derivatives: Trust-Region Based Learning of Continuous Energy Functions\",\"Hyperspherical Prototype Networks\",\"Expressive power of tensor-network factorizations for probabilistic modeling\",\"HyperGCN: A New Method For Training Graph Convolutional Networks on Hypergraphs\",\"SSRGD: Simple Stochastic Recursive Gradient Descent for Escaping Saddle Points\",\"Efficient Meta Learning via Minibatch Proximal Update\",\"Unconstrained Monotonic Neural Networks\",\"Guided Similarity Separation for Image Retrieval\",\"Learning Imbalanced Datasets with Label-Distribution-Aware Margin Loss\",\"Strategizing against No-regret Learners\",\"D-VAE: A Variational Autoencoder for Directed Acyclic Graphs\",\"Hierarchical Optimal Transport for Document Representation\",\"Multivariate Sparse Coding of Nonstationary Covariances with Gaussian Processes\",\"Positional Normalization\",\"A New Defense Against Adversarial Images: Turning a Weakness into a Strength\",\"Quadratic Video Interpolation\",\"ResNets Ensemble via the Feynman-Kac Formalism to Improve Natural and Robust Accuracies\",\"Incremental Scene Synthesis\",\"Self-Supervised Generalisation with Meta Auxiliary Learning\",\"Variational Denoising Network: Toward Blind Noise Modeling and Removal\",\"Fast Sparse Group Lasso\",\"Learnable Tree Filter for Structure-preserving Feature Transform\",\"Data-Dependence of Plateau Phenomenon in Learning with Neural Network --- Statistical Mechanical Analysis\",\"Coordinated hippocampal-entorhinal replay as structural inference\",\"Cascaded Dilated Dense Network with Two-step Data Consistency for MRI Reconstruction\",\"On the Ineffectiveness of Variance Reduced Optimization for Deep Learning\",\"On the Curved Geometry of Accelerated Optimization\",\"Multi-marginal Wasserstein GAN\",\"Better Exploration with Optimistic Actor Critic\",\"Importance Resampling for Off-policy Prediction\",\"The Label Complexity of Active Learning from Observational Data\",\"Meta-Learning Representations for Continual Learning\",\"Defense Against Adversarial Attacks Using Feature Scattering-based Adversarial Training\",\"Visualizing the PHATE of Neural Networks\",\"The Cells Out of Sample (COOS) dataset and benchmarks for measuring out-of-sample generalization of image classifiers\",\"Nonconvex Low-Rank Tensor Completion from Noisy Data\",\"Beyond Online Balanced Descent: An Optimal Algorithm for Smoothed Online Optimization\",\"Channel Gating Neural Networks\",\"Neural networks grown and self-organized by noise\",\"Catastrophic Forgetting Meets Negative Transfer: Batch Spectral Shrinkage for Safe Transfer Learning\",\"Meta-Weight-Net: Learning an Explicit Mapping For Sample Weighting\",\"Variational Structured Semantic Inference for Diverse Image Captioning\",\"Mapping State Space using Landmarks for Universal Goal Reaching\",\"Transferable Normalization: Towards Improving Transferability of Deep Neural Networks\",\"Random deep neural networks are biased towards simple functions\",\"XNAS: Neural Architecture Search with Expert Advice\",\"CNN^{2}: Viewpoint Generalization via a Binocular Vision\",\"Generalized Off-Policy Actor-Critic\",\"DAC: The Double Actor-Critic Architecture for Learning Options\",\"Numerically Accurate Hyperbolic Embeddings Using Tiling-Based Models\",\"Controlling Neural Level Sets\",\"Blended Matching Pursuit\",\"An Improved Analysis of Training Over-parameterized Deep Neural Networks\",\"Controllable Text-to-Image Generation\",\"Improving Textual Network Learning with Variational Homophilic Embeddings\",\"Rethinking Generative Mode Coverage: A Pointwise Guaranteed Approach\",\"The Randomized Midpoint Method for Log-Concave Sampling\",\"Sample-Efficient Deep Reinforcement Learning via Episodic Backward Update\",\"Fully Neural Network based Model for General Temporal Point Processes\",\"Gate Decorator: Global Filter Pruning Method for Accelerating Deep Convolutional Neural Networks\",\"Discrimination in Online Markets: Effects of Social Bias on Learning from Reviews and Policy Design\",\"Provably Powerful Graph Networks\",\"Order Optimal One-Shot Distributed Learning\",\"Information Competing Process for Learning Diversified Representations\",\"GENO -- GENeric Optimization for Classical Machine Learning\",\"Conditional Independence Testing using Generative Adversarial Networks\",\"Online Stochastic Shortest Path with Bandit Feedback and Unknown Transition Function\",\"Partitioning Structure Learning for Segmented Linear Regression Trees\",\"A Tensorized Transformer for Language Modeling\",\"Kernel Stein Tests for Multiple Model Comparison\",\"Disentangled behavioural representations\",\"More Is Less: Learning Efficient Video Representations by Big-Little Network and Depthwise Temporal Aggregation\",\"Rethinking the CSC Model for Natural Images\",\"Integrating Bayesian and Discriminative Sparse Kernel Machines for Multi-class Active Learning\",\"Learning to Control Self-Assembling Morphologies: A Study of Generalization via Modularity\",\"Perceiving the arrow of time in autoregressive motion\",\"DualDICE: Behavior-Agnostic Estimation of Discounted Stationary Distribution Corrections\",\"Hyper-Graph-Network Decoders for Block Codes\",\"Large Scale Markov Decision Processes with Changing Rewards\",\"Multiview Aggregation for Learning Category-Specific Shape Reconstruction\",\"Semi-Parametric Dynamic Contextual Pricing\",\"Interlaced Greedy Algorithm for Maximization of Submodular Functions in Nearly Linear Time\",\"Initialization of ReLUs for Dynamical Isometry\",\"Gradient Information for Representation and Modeling\",\"SpiderBoost and Momentum: Faster Variance Reduction Algorithms\",\"Minimax Optimal Estimation of Approximate Differential Privacy on Neighboring Databases\",\"Backprop with Approximate Activations for Memory-efficient Network Training\",\"Training Image Estimators without Image Ground Truth\",\"Deep Structured Prediction for Facial Landmark Detection\",\"Information-Theoretic Confidence Bounds for Reinforcement Learning\",\"Transfer Anomaly Detection by Inferring Latent Domain Representations\",\"Total Least Squares Regression in Input Sparsity Time\",\"Park: An Open Platform for Learning-Augmented Computer Systems\",\"Adapting Neural Networks for the Estimation of Treatment Effects\",\"Learning Transferable Graph Exploration\",\"Conformal Prediction Under Covariate Shift\",\"Optimal Analysis of Subset-Selection Based L_p Low-Rank Approximation\",\"Asymmetric Valleys: Beyond Sharp and Flat Local Minima\",\"Positive-Unlabeled Compression on the Cloud\",\"Direct Estimation of Differential Functional Graphical Models\",\"On the Calibration of Multiclass Classification with Rejection\",\"Third-Person Visual Imitation Learning via Decoupled Hierarchical Controller\",\"Stagewise Training Accelerates Convergence of Testing Error Over SGD\",\"Learning Robust Options by Conditional Value at Risk Optimization\",\"Non-asymptotic Analysis of Stochastic Methods for Non-Smooth Non-Convex Regularized Problems\",\"On Learning Over-parameterized Neural Networks: A Functional Approximation Perspective\",\"Drill-down: Interactive Retrieval of Complex Scenes using Natural Language Queries\",\"Visual Sequence Learning in Hierarchical Prediction Networks and Primate Visual Cortex\",\"Dual Variational Generation for Low Shot Heterogeneous Face Recognition\",\"Discovering Neural Wirings\",\"On the Optimality of Perturbations in Stochastic and Adversarial Multi-armed Bandit Problems\",\"Knowledge Extraction with No Observable Data\",\"PAC-Bayes under potentially heavy tails\",\"One-Shot Object Detection with Co-Attention and Co-Excitation\",\"Quaternion Knowledge Graph Embeddings\",\"Glyce: Glyph-vectors for Chinese Character Representations\",\"Turbo Autoencoder: Deep learning based channel codes for point-to-point communication channels\",\"Heterogeneous Graph Learning for Visual Commonsense Reasoning\",\"Probabilistic Watershed: Sampling all spanning forests for seeded segmentation and semi-supervised learning\",\"Classification-by-Components: Probabilistic Modeling of Reasoning over a Set of Components\",\"Identifying Causal Effects via Context-specific Independence Relations\",\"Bridging Machine Learning and Logical Reasoning by Abductive Learning\",\"Regret Minimization for Reinforcement Learning by Evaluating the Optimal Bias Function\",\"On the Global Convergence of (Fast) Incremental Expectation Maximization Methods\",\"A Linearly Convergent Proximal Gradient Algorithm for Decentralized Optimization\",\"Regularizing Trajectory Optimization with Denoising Autoencoders\",\"Learning Hierarchical Priors in VAEs\",\"Epsilon-Best-Arm Identification in Pay-Per-Reward Multi-Armed Bandits\",\"Safe Exploration for Interactive Machine Learning\",\"Addressing Failure Prediction by Learning Model Confidence\",\"Combinatorial Bayesian Optimization using the Graph Cartesian Product\",\"Fooling Neural Network Interpretations via Adversarial Model Manipulation\",\"On Lazy Training in Differentiable Programming\",\"Quality Aware Generative Adversarial Networks\",\"Copula-like Variational Inference\",\"Implicit Regularization for Optimal Sparse Recovery\",\"Locally Private Gaussian Estimation\",\"Multi-mapping Image-to-Image Translation via Learning Disentanglement\",\"Spatially Aggregated Gaussian Processes with Multivariate Areal Outputs\",\"Fast Structured Decoding for Sequence Models\",\"Learning Temporal Pose Estimation from Sparsely-Labeled Videos\",\"Putting An End to End-to-End: Gradient-Isolated Learning of Representations\",\"Scalable Gromov-Wasserstein Learning for Graph Partitioning and Matching\",\"Meta-Reinforced Synthetic Data for One-Shot Fine-Grained Visual Recognition\",\"Real-Time Reinforcement Learning\",\"Robust Multi-agent Counterfactual Prediction\",\"Approximate Inference Turns Deep Networks into Gaussian Processes\",\"Deep Signature Transforms\",\"Individual Regret in Cooperative Nonstochastic Multi-Armed Bandits\",\"Convergent Policy Optimization for Safe Reinforcement Learning\",\"Augmented Neural ODEs\",\"Thompson Sampling for Multinomial Logit Contextual Bandits\",\"Backpropagation-Friendly Eigendecomposition\",\"FastSpeech: Fast, Robust and Controllable Text to Speech\",\"Ultrametric Fitting by Gradient Descent\",\"Distinguishing Distributions When Samples Are Strategically Transformed\",\"Implicit Regularization of Discrete Gradient Dynamics in Linear Neural Networks\",\"Deep Set Prediction Networks\",\"DppNet: Approximating Determinantal Point Processes with Deep Networks\",\"Efficient Communication in Multi-Agent Reinforcement Learning via Variance Based Control\",\"Neural Lyapunov Control\",\"Fully Dynamic Consistent Facility Location\",\"SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems\",\"A Flexible Generative Framework for Graph-based Semi-supervised Learning\",\"Inherent Weight Normalization in Stochastic Neural Networks\",\"Optimal Decision Tree with Noisy Outcomes\",\"Meta-Curvature\",\"Intrinsically Efficient, Stable, and Bounded Off-Policy Evaluation for Reinforcement Learning\",\"KerGM: Kernelized Graph Matching\",\"Transfusion: Understanding Transfer Learning for Medical Imaging\",\"Adversarial training for free!\",\"Communication-Efficient Distributed Learning via Lazily Aggregated Quantized Gradients\",\"Implicitly learning to reason in first-order logic\",\"Kernel-Based Approaches for Sequence Modeling: Connections to Neural Methods\",\"PC-Fairness: A Unified Framework for Measuring Causality-based Fairness\",\"Arbicon-Net: Arbitrary Continuous Geometric Transformation Networks for Image Registration\",\"Assessing Disparate Impact of Personalized Interventions: Identifiability and Bounds\",\"The Fairness of Risk Scores Beyond Classification: Bipartite Ranking and the XAUC Metric\",\"HYPE: A Benchmark for Human eYe Perceptual Evaluation of Generative Models\",\"First order expansion of convex regularized estimators\",\"Capacity Bounded Differential Privacy\",\"Universal Boosting Variational Inference\",\"SGD on Neural Networks Learns Functions of Increasing Complexity\",\"The Landscape of Non-convex Empirical Risk with Degenerate Population Risk\",\"Making AI Forget You: Data Deletion in Machine Learning\",\"Practical Differentially Private Top-k Selection with Pay-what-you-get Composition\",\"Conformalized Quantile Regression\",\"Thompson Sampling with Information Relaxation Penalties\",\"Deep Generalized Method of Moments for Instrumental Variable Analysis\",\"Learning Sample-Specific Models with Low-Rank Personalized Regression\",\"Dancing to Music\",\"Deconstructing Lottery Tickets: Zeros, Signs, and the Supermask\",\"Implicit Generation and Modeling with Energy Based Models\",\"LCA: Loss Change Allocation for Neural Network Training\",\"Predicting the Politics of an Image Using Webly Supervised Data\",\"Adaptive GNN for Image Analysis and Editing\",\"Ultra Fast Medoid Identification via Correlated Sequential Halving\",\"Tight Dimension Independent Lower Bound on the Expected Convergence Rate for Diminishing Step Sizes in SGD\",\"Asymptotics for Sketching in Least Squares Regression\",\"MCP: Learning Composable Hierarchical Control with Multiplicative Compositional Policies\",\"Exact inference in structured prediction\",\"Coda: An End-to-End Neural Program Decompiler\",\"Bat-G net: Bat-inspired High-Resolution 3D Image Reconstruction using Ultrasonic Echoes\",\"Painless Stochastic Gradient: Interpolation, Line-Search, and Convergence Rates\",\"Scalable Structure Learning of Continuous-Time Bayesian Networks from Incomplete Data\",\"Privacy-Preserving Classification of Personal Text Messages with Secure Multi-Party Computation\",\"Efficiently Estimating Erdos-Renyi Graphs with Node Differential Privacy\",\"Learning Representations for Time Series Clustering\",\"Verified Uncertainty Calibration\",\"A Normative Theory for Causal Inference and Bayes Factor Computation in Neural Circuits\",\"Unsupervised Keypoint Learning for Guiding Class-Conditional Video Prediction\",\"Subspace Attack: Exploiting Promising Subspaces for Query-Efficient Black-box Attacks\",\"Stochastic Gradient Hamiltonian Monte Carlo Methods with Recursive Variance Reduction\",\"Learning Latent Process from High-Dimensional Event Sequences via Efficient Sampling\",\"Cross-sectional Learning of Extremal Dependence among Financial Assets\",\"Principal Component Projection and Regression in Nearly Linear Time through Asymmetric SVRG\",\"Compression with Flows via Local Bits-Back Coding\",\"Exact Rate-Distortion in Autoencoders via Echo Noise\",\"iSplit LBI: Individualized Partial Ranking with Ties via Split LBI\",\"Domes to Drones: Self-Supervised Active Triangulation for 3D Human Pose Reconstruction\",\"MetaQuant: Learning to Quantize by Learning to Penetrate Non-differentiable Quantization\",\"Improved Precision and Recall Metric for Assessing Generative Models\",\"A First-Order Algorithmic Framework for Distributionally Robust Logistic Regression\",\"PasteGAN: A Semi-Parametric Method to Generate Image from Scene Graph\",\"Handling correlated and repeated measurements with the smoothed multivariate square-root Lasso\",\"Joint Optimization of Tree-based Index and Deep Model for Recommender Systems\",\"Learning Generalizable Device Placement Algorithms for Distributed Machine Learning\",\"Uncoupled Regression from Pairwise Comparison Data\",\"Cross Attention Network for Few-shot Classification\",\"A Nonconvex Approach for Exact and Efficient Multichannel Sparse Blind Deconvolution\",\"SCAN: A Scalable Neural Networks Framework Towards Compact and Efficient Models\",\"Revisiting the Bethe-Hessian: Improved Community Detection in Sparse Heterogeneous Graphs\",\"Teaching Multiple Concepts to a Forgetful Learner\",\"Regularized Weighted Low Rank Approximation\",\"Practical and Consistent Estimation of f-Divergences\",\"Approximation Ratios of Graph Neural Networks for Combinatorial Problems\",\"Thinning for Accelerating the Learning of Point Processes\",\"A Prior of a Googol Gaussians: a Tensor Ring Induced Prior for Generative Models\",\"Differentially Private Markov Chain Monte Carlo\",\"Full-Gradient Representation for Neural Network Visualization\",\"q-means: A quantum algorithm for unsupervised machine learning\",\"Learner-aware Teaching: Inverse Reinforcement Learning with Preferences and Constraints\",\"Limitations of the empirical Fisher approximation for natural gradient descent\",\"Flow-based Image-to-Image Translation with Feature Disentanglement\",\"Learning dynamic polynomial proofs\",\"Shape and Time Distortion Loss for Training Deep Time Series Forecasting Models\",\"Understanding Attention and Generalization in Graph Neural Networks\",\"Data Cleansing for Models Trained with SGD\",\"Curvilinear Distance Metric Learning\",\"Embedding Symbolic Knowledge into Deep Networks\",\"Modeling Uncertainty by Learning a Hierarchy of Deep Neural Connections\",\"Efficient Graph Generation with Graph Recurrent Attention Networks\",\"Beyond Alternating Updates for Matrix Factorization with Inertial Bregman Proximal Gradient Algorithms\",\"Learning Deep Bilinear Transformation for Fine-grained Image Representation\",\"Practical Deep Learning with Bayesian Principles\",\"Training Language GANs from Scratch\",\"Pseudo-Extended Markov chain Monte Carlo\",\"Differentially Private Bagging: Improved utility and cheaper privacy than subsample-and-aggregate\",\"Propagating Uncertainty in Reinforcement Learning via Wasserstein Barycenters\",\"On Adversarial Mixup Resynthesis\",\"A Geometric Perspective on Optimal Representations for Reinforcement Learning\",\"Learning New Tricks From Old Dogs: Multi-Source Transfer Learning From Pre-Trained Networks\",\"Understanding and Improving Layer Normalization\",\"Uncertainty-based Continual Learning with Adaptive Regularization\",\"LIIR: Learning Individual Intrinsic Reward in Multi-Agent Reinforcement Learning\",\"U-Time: A Fully Convolutional Network for Time Series Segmentation Applied to Sleep Staging\",\"Massively scalable Sinkhorn distances via the Nystr\\u00f6m method\",\"Double Quantization for Communication-Efficient Distributed Optimization\",\"Globally optimal score-based learning of directed acyclic graphs in high-dimensions\",\"Multi-relational Poincar\\u00e9 Graph Embeddings\",\"No-Press Diplomacy: Modeling Multi-Agent Gameplay\",\"State Aggregation Learning from Markov Transition Data\",\"Disentangling Influence: Using disentangled representations to audit model predictions\",\"Successor Uncertainties: Exploration and Uncertainty in Temporal Difference Learning\",\"Partially Encrypted Deep Learning using Functional Encryption\",\"Decentralized Cooperative Stochastic Bandits\",\"Statistical bounds for entropic optimal transport: sample complexity and the central limit theorem\",\"Efficient Deep Approximation of GMMs\",\"Learning low-dimensional state embeddings and metastable clusters from time series data\",\"Exploiting Local and Global Structure for Point Cloud Semantic Segmentation with Contextual Point Representations\",\"Scalable Bayesian dynamic covariance modeling with variational Wishart and inverse Wishart processes\",\"Kernel Instrumental Variable Regression\",\"Symmetry-Based Disentangled Representation Learning requires Interaction with Environments\",\"Fast Efficient Hyperparameter Tuning for Policy Gradient Methods\",\"Offline Contextual Bayesian Optimization\",\"Making the Cut: A Bandit-based Approach to Tiered Interviewing\",\"Unsupervised Scalable Representation Learning for Multivariate Time Series\",\"A state-space model for inferring effective connectivity of latent neural dynamics from simultaneous EEG/fMRI\",\"End to end learning and optimization on graphs\",\"Game Design for Eliciting Distinguishable Behavior\",\"When does label smoothing help?\",\"Finite-Time Performance Bounds and Adaptive Learning Rate Selection for Two Time-Scale Reinforcement Learning\",\"Rethinking Deep Neural Network Ownership Verification: Embedding Passports to Defeat Ambiguity Attacks\",\"Scalable Spike Source Localization in Extracellular Recordings using Amortized Variational Inference\",\"Optimal Sketching for Kronecker Product Regression and Low Rank Approximation\",\"Distribution-Independent PAC Learning of Halfspaces with Massart Noise\",\"The Convergence Rate of Neural Networks for Learned Functions of Different Frequencies\",\"Adaptive Auxiliary Task Weighting for Reinforcement Learning\",\"Blocking Bandits\",\"Global Convergence of Least Squares EM for Demixing Two Log-Concave Densities\",\"Prior-Free Dynamic Auctions with Low Regret Buyers\",\"On Single Source Robustness in Deep Fusion Models\",\"Policy Evaluation with Latent Confounders via Optimal Balance\",\"Think Globally, Act Locally: A Deep Neural Network Approach to High-Dimensional Time Series Forecasting\",\"Adaptive Cross-Modal Few-shot Learning\",\"Spectral Modification of Graphs for Improved Spectral Clustering\",\"Hyperbolic Graph Convolutional Neural Networks\",\"Cost Effective Active Search\",\"Exploration Bonus for Regret Minimization in Discrete and Continuous Average Reward MDPs\",\"Hybrid 8-bit Floating Point (HFP8) Training and Inference for Deep Neural Networks\",\"Tight Certificates of Adversarial Robustness for Randomly Smoothed Classifiers\",\"Poisson-Minibatching for Gibbs Sampling with Convergence Rate Guarantees\",\"One ticket to win them all: generalizing lottery ticket initializations across datasets and optimizers\",\"Breaking the Glass Ceiling for Embedding-Based Classifiers for Large Output Spaces\",\"Fair Algorithms for Clustering\",\"Learning Mean-Field Games\",\"SpArSe: Sparse Architecture Search for CNNs on Resource-Constrained Microcontrollers\",\"Deep imitation learning for molecular inverse problems\",\"Visual Concept-Metaconcept Learning\",\"Few-shot Video-to-Video Synthesis\",\"Neural Similarity Learning\",\"Ordered Memory\",\"MixMatch: A Holistic Approach to Semi-Supervised Learning\",\"Multivariate Triangular Quantile Maps for Novelty Detection\",\"Fast Parallel Algorithms for Statistical Subset Selection Problems\",\"PHYRE: A New Benchmark for Physical Reasoning\",\"On the number of variables to use in principal component regression\",\"Factor Group-Sparse Regularization for Efficient Low-Rank Matrix Recovery\",\"Mutually Regressive Point Processes\",\"Data-driven Estimation of Sinusoid Frequencies\",\"E2-Train: Training State-of-the-art CNNs with Over 80% Energy Savings\",\"ANODEV2: A Coupled Neural ODE Framework\",\"Estimating Entropy of Distributions in Constant Space\",\"On the Utility of Learning about Humans for Human-AI Coordination\",\"Efficient Regret Minimization Algorithm for Extensive-Form Correlated Equilibrium\",\"Learning in Generalized Linear Contextual Bandits with Stochastic Delays\",\"Empirically Measuring Concentration: Fundamental Limits on Intrinsic Robustness\",\"Optimistic Regret Minimization for Extensive-Form Games via Dilated Distance-Generating Functions\",\"Learning Non-Convergent Non-Persistent Short-Run MCMC Toward Energy-Based Model\",\"Enhancing the Locality and Breaking the Memory Bottleneck of Transformer on Time Series Forecasting\",\"On the Accuracy of Influence Functions for Measuring Group Effects\",\"Face Reconstruction from Voice using Generative Adversarial Networks\",\"Incremental Few-Shot Learning with Attention Attractor Networks\",\"On Testing for Biases in Peer Review\",\"Learning Disentangled Representation for Robust Person Re-identification\",\"Balancing Efficiency and Fairness in On-Demand Ridesourcing\",\"Latent Ordinary Differential Equations for Irregularly-Sampled Time Series\",\"Deep RGB-D Canonical Correlation Analysis For Sparse Depth Completion\",\"Input Similarity from the Neural Network Perspective\",\"Adaptive Sequence Submodularity\",\"Weight Agnostic Neural Networks\",\"Learning to Predict Without Looking Ahead: World Models Without Forward Prediction\",\"Reducing the variance in online optimization by transporting past gradients\",\"Characterizing Bias in Classifiers using Generative Models\",\"Optimal Stochastic and Online Learning with Individual Iterates\",\"Policy Learning for Fairness in Ranking\",\"Off-Policy Evaluation via Off-Policy Classification\",\"Regularized Gradient Boosting\",\"Efficient Probabilistic Inference in the Quest for Physics Beyond the Standard Model\",\"Markov Random Fields for Collaborative Filtering\",\"A Step Toward Quantifying Independently Reproducible Machine Learning Research\",\"Scalable Global Optimization via Local Bayesian Optimization\",\"Time-series Generative Adversarial Networks\",\"Ouroboros: On Accelerating Training of Transformer-Based Language Models\",\"A Refined Margin Distribution Analysis for Forest Representation Learning\",\"Robustness to Adversarial Perturbations in Learning from Incomplete Data\",\"Exploring Unexplored Tensor Network Decompositions for Convolutional Neural Networks\",\"An Adaptive Empirical Bayesian Method for Sparse Deep Learning\",\"Adaptive Influence Maximization with Myopic Feedback\",\"Focused Quantization for Sparse CNNs\",\"Quantum Embedding of Knowledge for Reasoning\",\"Optimal Best Markovian Arm Identification with Fixed Confidence\",\"Limiting Extrapolation in Linear Approximate Value Iteration\",\"Almost Horizon-Free Structure-Aware Best Policy Identification with a Generative Model\",\"Invertible Convolutional Flow\",\"A Latent Variational Framework for Stochastic Optimization\",\"Topology-Preserving Deep Image Segmentation\",\"Connective Cognition Network for Directional Visual Commonsense Reasoning\",\"Online Markov Decoding: Lower Bounds and Near-Optimal Approximation Algorithms\",\"A Meta-MDP Approach to Exploration for Lifelong Reinforcement Learning\",\"Push-pull Feedback Implements Hierarchical Information Retrieval Efficiently\",\"Learning Disentangled Representations for Recommendation\",\"Graph Neural Tangent Kernel: Fusing Graph Neural Networks with Graph Kernels\",\"In-Place Zero-Space Memory Protection for CNN\",\"Acceleration via Symplectic Discretization of High-Resolution Differential Equations\",\"XLNet: Generalized Autoregressive Pretraining for Language Understanding\",\"Comparison Against Task Driven Artificial Neural Networks Reveals Functional Properties in Mouse Visual Cortex\",\"Mixtape: Breaking the Softmax Bottleneck Efficiently\",\"Variance Reduced Policy Evaluation with Smooth Function Approximation\",\"Learning GANs and Ensembles Using Discrepancy\",\"Co-Generation with GANs using AIS based HMC\",\"AttentionXML: Label Tree-based Attention-Aware Deep Model for High-Performance Extreme Multi-Label Text Classification\",\"Addressing Sample Complexity in Visual Tasks Using HER and Hallucinatory GANs\",\"Abstract Reasoning with Distracting Features\",\"Generalized Block-Diagonal Structure Pursuit: Learning Soft Latent Task Assignment against Negative Transfer\",\"Adversarial Training and Robustness for Multiple Perturbations\",\"Doubly-Robust Lasso Bandit\",\"DM2C: Deep Mixed-Modal Clustering\",\"MaCow: Masked Convolutional Generative Flow\",\"Learning by Abstraction: The Neural State Machine\",\"Adaptive Gradient-Based Meta-Learning Methods\",\"Equipping Experts/Bandits with Long-term Memory\",\"A Regularized Approach to Sparse Optimal Policy in Reinforcement Learning\",\"Scalable inference of topic evolution via models for latent geometric structures\",\"Effective End-to-end Unsupervised Outlier Detection via Inlier Priority of Discriminative Network\",\"Deep Active Learning with a Neural Architecture Search\",\"Efficiently escaping saddle points on manifolds\",\"AutoAssist: A Framework to Accelerate Training of Deep Neural Networks\",\"DFNets: Spectral CNNs for Graphs with Feedback-Looped Filters\",\"Learning Dynamics of Attention: Human Prior for Interpretable Machine Reasoning\",\"Comparing Unsupervised Word Translation Methods Step by Step\",\"Learning from Bad Data via Generation\",\"Constrained deep neural network architecture search for IoT devices accounting for hardware calibration\",\"Quantum Entropy Scoring for Fast Robust Mean Estimation and Improved Outlier Detection\",\"Iterative Least Trimmed Squares for Mixed Linear Regression\",\"Dynamic Ensemble Modeling Approach to Nonstationary Neural Decoding in Brain-Computer Interfaces\",\"Divergence-Augmented Policy Optimization\",\"Intrinsic dimension of data representations in deep neural networks\",\"Towards a Zero-One Law for Column Subset Selection\",\"Compositional De-Attention Networks\",\"Dual Adversarial Semantics-Consistent Network for Generalized Zero-Shot Learning\",\"Learning and Generalization in Overparameterized Neural Networks, Going Beyond Two Layers\",\"Mining GOLD Samples for Conditional GANs\",\"Deep Model Transferability from Attribution Maps\",\"Fully Parameterized Quantile Function for Distributional Reinforcement Learning\",\"Direct Optimization through \\\\arg \\\\max for Discrete Variational Auto-Encoder\",\"Distributional Reward Decomposition for Reinforcement Learning\",\"L_DMI: A Novel Information-theoretic Loss Function for Training Deep Nets Robust to Label Noise\",\"Convergence Guarantees for Adaptive Bayesian Quadrature Methods\",\"Progressive Augmentation of GANs\",\"UniXGrad: A Universal, Adaptive Algorithm with Optimal Guarantees for Constrained Optimization\",\"Meta-Surrogate Benchmarking for Hyperparameter Optimization\",\"Learning to Perform Local Rewriting for Combinatorial Optimization\",\"Anti-efficient encoding in emergent communication\",\"Singleshot : a scalable Tucker tensor decomposition\",\"Neural Machine Translation with Soft Prototype\",\"Reliable training and estimation of variance networks\",\"Copula Multi-label Learning\",\"Bayesian Learning of Sum-Product Networks\",\"Bayesian Batch Active Learning as Sparse Subset Approximation\",\"Optimal Sparsity-Sensitive Bounds for Distributed Mean Estimation\",\"Global Sparse Momentum SGD for Pruning Very Deep Neural Networks\",\"Variational Bayesian Decision-making for Continuous Utilities\",\"The Normalization Method for Alleviating Pathological Sharpness in Wide Neural Networks\",\"Single-Model Uncertainties for Deep Learning\",\"Is Deeper Better only when Shallow is Good?\",\"Wasserstein Weisfeiler-Lehman Graph Kernels\",\"Domain Generalization via Model-Agnostic Learning of Semantic Features\",\"Grid Saliency for Context Explanations of Semantic Segmentation\",\"First-order methods almost always avoid saddle points: The case of vanishing step-sizes\",\"Maximum Mean Discrepancy Gradient Flow\",\"Oblivious Sampling Algorithms for Private Data Analysis\",\"Semi-supervisedly Co-embedding Attributed Networks\",\"From voxels to pixels and back: Self-supervision in natural-image reconstruction from fMRI\",\"Copulas as High-Dimensional Generative Models: Vine Copula Autoencoders\",\"Nonstochastic Multiarmed Bandits with Unrestricted Delays\",\"BIVA: A Very Deep Hierarchy of Latent Variables for Generative Modeling\",\"Code Generation as a Dual Task of Code Summarization\",\"Diffeomorphic Temporal Alignment Nets\",\"Weakly Supervised Instance Segmentation using the Bounding Box Tightness Prior\",\"On the Power and Limitations of Random Features for Understanding Neural Networks\",\"Efficient Pure Exploration in Adaptive Round model\",\"Multi-objects Generation with Amortized Structural Regularization\",\"Neural Shuffle-Exchange Networks - Sequence Processing in O(n log n) Time\",\"DetNAS: Backbone Search for Object Detection\",\"Stochastic Proximal Langevin Algorithm: Potential Splitting and Nonasymptotic Rates\",\"Fast AutoAugment\",\"On the Convergence Rate of Training Recurrent Neural Networks\",\"Interval timing in deep reinforcement learning agents\",\"Graph-based Discriminators: Sample Complexity and Expressiveness\",\"Large Scale Structure of Neural Network Loss Landscapes\",\"Learning Nonsymmetric Determinantal Point Processes\",\"Hypothesis Set Stability and Generalization\",\"Learning Object Bounding Boxes for 3D Instance Segmentation on Point Clouds\",\"Precision-Recall Balanced Topic Modelling\",\"Learning Sparse Distributions using Iterative Hard Thresholding\",\"Discriminative Topic Modeling with Logistic LDA\",\"Quantum Wasserstein Generative Adversarial Networks\",\"Blow: a single-scale hyperconditioned flow for non-parallel raw-audio voice conversion\",\"Hyperparameter Learning via Distributional Transfer\",\"Discriminator optimal transport\",\"High-dimensional multivariate forecasting with low-rank Gaussian Copula Processes\",\"Are Anchor Points Really Indispensable in Label-Noise Learning?\",\"Aligning Visual Regions and Textual Concepts for Semantic-Grounded Image Representations\",\"Differentiable Ranking and Sorting using Optimal Transport\",\"Dichotomize and Generalize: PAC-Bayesian Binary Activated Deep Neural Networks\",\"Likelihood-Free Overcomplete ICA and Applications In Causal Discovery\",\"Interior-Point Methods Strike Back: Solving the Wasserstein Barycenter Problem\",\"Beyond Vector Spaces: Compact Data Representation as Differentiable Weighted Graphs\",\"Subspace Detours: Building Transport Plans that are Optimal on Subspace Projections\",\"Efficient Smooth Non-Convex Stochastic Compositional Optimization via Stochastic Recursive Gradient Descent\",\"On the convergence of single-call stochastic extra-gradient methods\",\"Infra-slow brain dynamics as a marker for cognitive function and decline\",\"Robust Principal Component Analysis with Adaptive Neighbors\",\"High-Quality Self-Supervised Deep Image Denoising\",\"Dynamics of stochastic gradient descent for two-layer neural networks in the teacher-student setup\",\"GIFT: Learning Transformation-Invariant Dense Visual Descriptors via Group CNNs\",\"Online Prediction of Switching Graph Labelings with Cluster Specialists\",\"Graph-Based Semi-Supervised Learning with Non-ignorable Non-response\",\"BatchBALD: Efficient and Diverse Batch Acquisition for Deep Bayesian Active Learning\",\"A Mean Field Theory of Quantized Deep Networks: The Quantization-Depth Trade-Off\",\"Beyond Confidence Regions: Tight Bayesian Ambiguity Sets for Robust MDPs\",\"Cross-lingual Language Model Pretraining\",\"Approximate Bayesian Inference for a Mechanistic Model of Vesicle Release at a Ribbon Synapse\",\"Updates of Equilibrium Prop Match Gradients of Backprop Through Time in an RNN with Static Input\",\"Universal Invariant and Equivariant Graph Neural Networks\",\"Are sample means in multi-armed bandits positively or negatively biased?\",\"On the Correctness and Sample Complexity of Inverse Reinforcement Learning\",\"VIREL: A Variational Inference Framework for Reinforcement Learning\",\"First Order Motion Model for Image Animation\",\"Tensor Monte Carlo: Particle Methods for the GPU era\",\"Unsupervised Emergence of Egocentric Spatial Structure from Sensorimotor Prediction\",\"Learning from Label Proportions with Generative Adversarial Networks\",\"Efficient and Thrifty Voting by Any Means Necessary\",\"PointDAN: A Multi-Scale 3D Domain Adaption Network for Point Cloud Representation\",\"ZO-AdaMM: Zeroth-Order Adaptive Momentum Method for Black-Box Optimization\",\"Non-Stationary Markov Decision Processes, a Worst-Case Approach using Model-Based Reinforcement Learning\",\"Depth-First Proof-Number Search with Heuristic Edge Cost and Application to Chemical Synthesis Planning\",\"Toward a Characterization of Loss Functions for Distribution Learning\",\"Coresets for Archetypal Analysis\",\"Emergence of Object Segmentation in Perturbed Generative Models\",\"Optimal Sparse Decision Trees\",\"Escaping from saddle points on Riemannian manifolds\",\"Multi-source Domain Adaptation for Semantic Segmentation\",\"Localized Structured Prediction\",\"Nonzero-sum Adversarial Hypothesis Testing Games\",\"Manifold-regression to predict from MEG/EEG brain signals without source modeling\",\"Modeling Tabular data using Conditional GAN\",\"Normalization Helps Training of Quantized LSTM\",\"Trajectory of Alternating Direction Method of Multipliers and Adaptive Acceleration\",\"Deep Scale-spaces: Equivariance Over Scale\",\"GRU-ODE-Bayes: Continuous Modeling of Sporadically-Observed Time Series\",\"Estimating Convergence of Markov chains with L-Lag Couplings\",\"Learning-Based Low-Rank Approximations\",\"Implicit Regularization in Deep Matrix Factorization\",\"List-decodable Linear Regression\",\"Learning elementary structures for 3D shape generation and matching\",\"On the Hardness of Robust Classification\",\"Foundations of Comparison-Based Hierarchical Clustering\",\"What the Vec? Towards Probabilistically Grounded Embeddings\",\"Minimizers of the Empirical Risk and Risk Monotonicity\",\"Explicit Planning for Efficient Exploration in Reinforcement Learning\",\"Lower Bounds on Adversarial Robustness from Optimal Transport\",\"Neural Spline Flows\",\"Phase Transitions and Cyclic Phenomena in Bandits with Switching Constraints\",\"Latent Weights Do Not Exist: Rethinking Binarized Neural Network Optimization\",\"Nonlinear scaling of resource allocation in sensory bottlenecks\",\"Constrained Reinforcement Learning Has Zero Duality Gap\",\"Symmetry-adapted generation of 3d point sets for the targeted discovery of molecules\",\"An adaptive nearest neighbor rule for classification\",\"Coresets for Clustering with Fairness Constraints\",\"PerspectiveNet: A Scene-consistent Image Generator for New View Synthesis in Real Indoor Environments\",\"MAVEN: Multi-Agent Variational Exploration\",\"Competitive Gradient Descent\",\"Globally Convergent Newton Methods for Ill-conditioned Generalized Self-concordant Losses\",\"Continual Unsupervised Representation Learning\",\"Self-Routing Capsule Networks\",\"The Parameterized Complexity of Cascading Portfolio Scheduling\",\"Maximum Expected Hitting Cost of a Markov Decision Process and Informativeness of Rewards\",\"Bipartite expander Hopfield networks as self-decoding high-capacity error correcting codes\",\"Sequence Modeling with Unconstrained Generation Order\",\"Probabilistic Logic Neural Networks for Reasoning\",\"A Polynomial Time Algorithm for Log-Concave Maximum Likelihood via Locally Exponential Families\",\"A Unifying Framework for Spectrum-Preserving Graph Sparsification and Coarsening\",\"Stochastic Runge-Kutta Accelerates Langevin Monte Carlo and Beyond\",\"The Implicit Bias of AdaGrad on Separable Data\",\"On two ways to use determinantal point processes for Monte Carlo integration\",\"LiteEval: A Coarse-to-Fine Framework for Resource Efficient Video Recognition\",\"How degenerate is the parametrization of neural networks with the ReLU activation function?\",\"Spike-Train Level Backpropagation for Training Deep Recurrent Spiking Neural Networks\",\"Re-examination of the Role of Latent Variables in Sequence Modeling\",\"Max-value Entropy Search for Multi-Objective Bayesian Optimization\",\"Stein Variational Gradient Descent With Matrix-Valued Kernels\",\"Crowdsourcing via Pairwise Co-occurrences: Identifiability and Algorithms\",\"Detecting Overfitting via Adversarial Examples\",\"A Unified Bellman Optimality Principle Combining Reward Maximization and Empowerment\",\"SMILe: Scalable Meta Inverse Reinforcement Learning through Context-Conditional Policies\",\"Towards Understanding the Importance of Shortcut Connections in Residual Networks\",\"Modular Universal Reparameterization: Deep Multi-task Learning Across Diverse Domains\",\"Solving Interpretable Kernel Dimensionality Reduction\",\"Interaction Hard Thresholding: Consistent Sparse Quadratic Regression in Sub-quadratic Time and Space\",\"A Model to Search for Synthesizable Molecules\",\"Post training 4-bit quantization of convolutional networks for rapid-deployment\",\"Fast and Flexible Multi-Task Classification using Conditional Neural Adaptive Processes\",\"Differentially Private Anonymized Histograms\",\"Dynamic Local Regret for Non-convex Online Forecasting\",\"Learning Local Search Heuristics for Boolean Satisfiability\",\"Provably Efficient Q-Learning with Low Switching Cost\",\"Solving graph compression via optimal transport\",\"PyTorch: An Imperative Style, High-Performance Deep Learning Library\",\"Stability of Graph Scattering Transforms\",\"A Debiased MDI Feature Importance Measure for Random Forests\",\"Provably Efficient Q-learning with Function Approximation via Distribution Shift Error Checking Oracle\",\"Sparse Logistic Regression Learns All Discrete Pairwise Graphical Models\",\"Fast Convergence of Natural Gradient Descent for Over-Parameterized Neural Networks\",\"Rapid Convergence of the Unadjusted Langevin Algorithm: Isoperimetry Suffices\",\"Learning Distributions Generated by One-Layer ReLU Networks\",\"Large-scale optimal transport map estimation using projection pursuit\",\"A Structured Prediction Approach for Generalization in Cooperative Multi-Agent Reinforcement Learning\",\"On Exact Computation with an Infinitely Wide Neural Net\",\"Loaded DiCE: Trading off Bias and Variance in Any-Order Score Function Gradient Estimators for Reinforcement Learning\",\"Chirality Nets for Human Pose Regression\",\"Efficient Approximation of Deep ReLU Networks for Functions on Low Dimensional Manifolds\",\"Fast Decomposable Submodular Function Minimization using Constrained Total Variation\",\"Which Algorithmic Choices Matter at Which Batch Sizes? Insights From a Noisy Quadratic Model\",\"Spherical Text Embedding\",\"M\\u00f6bius Transformation for Fast Inner Product Search on Graph\",\"Hyperbolic Graph Neural Networks\",\"Average Individual Fairness: Algorithms, Generalization and Experiments\",\"Fixing the train-test resolution discrepancy\",\"Modeling Dynamic Functional Connectivity with Latent Factor Gaussian Processes\",\"Manipulating a Learning Defender and Ways to Counteract\",\"Learning-In-The-Loop Optimization: End-To-End Control And Co-Design Of Soft Robots Through Learned Deep Latent Representations\",\"Learning to Infer Implicit Surfaces without 3D Supervision\",\"Fast and Accurate Least-Mean-Squares Solvers\",\"Certifiable Robustness to Graph Perturbations\",\"Fast Convergence of Belief Propagation to Global Optima: Beyond Correlation Decay\",\"Paradoxes in Fair Machine Learning\",\"Provably Global Convergence of Actor-Critic: A Case for Linear Quadratic Regulator with Ergodic Cost\",\"The spiked matrix model with generative priors\",\"Gradient Dynamics of Shallow Univariate ReLU Networks\",\"Robust and Communication-Efficient Collaborative Learning\",\"Multiclass Learning from Contradictions\",\"Learning from Trajectories via Subgoal Discovery\",\"Distributed Low-rank Matrix Factorization With Exact Consensus\",\"Online Normalization for Training Neural Networks\",\"The Synthesis of XNOR Recurrent Neural Networks with Stochastic Logic\",\"An adaptive Mirror-Prox method for variational inequalities with singular operators\",\"N-Gram Graph: Simple Unsupervised Representation for Graphs, with Applications to Molecules\",\"Characterizing the Exact Behaviors of Temporal Difference Learning Algorithms Using Markov Jump Linear System Theory\",\"Facility Location Problem in Differential Privacy Model Revisited\",\"Energy-Inspired Models: Learning with Sampler-Induced Distributions\",\"Finite-time Analysis of Approximate Policy Iteration for the Linear Quadratic Regulator\",\"A Universally Optimal Multistage Accelerated Stochastic Gradient Method\",\"From deep learning to mechanistic understanding in neuroscience: the structure of retinal prediction\",\"Large Memory Layers with Product Keys\",\"Learning Deterministic Weighted Automata with Queries and Counterexamples\",\"Wide Neural Networks of Any Depth Evolve as Linear Models Under Gradient Descent\",\"Time/Accuracy Tradeoffs for Learning a ReLU with respect to Gaussian Marginals\",\"Visualizing and Measuring the Geometry of BERT\",\"Self-Critical Reasoning for Robust Visual Question Answering\",\"Learning to Screen\",\"A Communication Efficient Stochastic Multi-Block Alternating Direction Method of Multipliers\",\"A Little Is Enough: Circumventing Defenses For Distributed Learning\",\"Error Correcting Output Codes Improve Probability Estimation and Adversarial Robustness of Deep Neural Networks\",\"A Robust Non-Clairvoyant Dynamic Mechanism for Contextual Auctions\",\"Finite-Sample Analysis for SARSA with Linear Function Approximation\",\"Who is Afraid of Big Bad Minima? Analysis of gradient-flow in spiked matrix-tensor models\",\"Graph Structured Prediction Energy Networks\",\"Private Learning Implies Online Learning: An Efficient Reduction\",\"Graph Agreement Models for Semi-Supervised Learning\",\"Latent distance estimation for random geometric graphs\",\"Seeing the Wind: Visual Wind Speed Prediction with a Coupled Convolutional and Recurrent Neural Network\",\"The Functional Neural Process\",\"Recurrent Registration Neural Networks for Deformable Image Registration\",\"Unsupervised State Representation Learning in Atari\",\"Unlocking Fairness: a Trade-off Revisited\",\"Fisher Efficient Inference of Intractable Models\",\"Thompson Sampling and Approximate Inference\",\"PRNet: Self-Supervised Learning for Partial-to-Partial Registration\",\"Surrogate Objectives for Batch Policy Optimization in One-step Decision Making\",\"Modelling heterogeneous distributions with an Uncountable Mixture of Asymmetric Laplacians\",\"Learning Macroscopic Brain Connectomes via Group-Sparse Factorization\",\"Approximating the Permanent by Sampling from Adaptive Partitions\",\"Retrosynthesis Prediction with Conditional Graph Logic Network\",\"Procrastinating with Confidence: Near-Optimal, Anytime, Adaptive Algorithm Configuration\",\"Online Learning via the Differential Privacy Lens\",\"PerspectiveNet: 3D Object Detection from a Single RGB Image via Perspective Points\",\"Parameter elimination in particle Gibbs sampling\",\"This Looks Like That: Deep Learning for Interpretable Image Recognition\",\"Adaptively Aligned Image Captioning via Adaptive Attention Time\",\"Accurate Uncertainty Estimation and Decomposition in Ensemble Learning\",\"Learning Bayesian Networks with Low Rank Conditional Probability Tables\",\"Equal Opportunity in Online Classification with Partial Feedback\",\"Modeling Expectation Violation in Intuitive Physics with Coarse Probabilistic Object Representations\",\"Neural Multisensory Scene Inference\",\"Regret Bounds for Thompson Sampling in Episodic Restless Bandit Problems\",\"What Can ResNet Learn Efficiently, Going Beyond Kernels?\",\"Better Transfer Learning with Inferred Successor Maps\",\"Unsupervised Co-Learning on G-Manifolds Across Irreducible Representations\",\"Defending Against Neural Fake News\",\"Sample Adaptive MCMC\",\"A Stochastic Composite Gradient Method with Incremental Variance Reduction\",\"Nonparametric Density Estimation & Convergence Rates for GANs under Besov IPM Losses\",\"STAR-Caps: Capsule Networks with Straight-Through Attentive Routing\",\"Limitations of Lazy Training of Two-layers Neural Network\",\"Reconciling meta-learning and continual learning with online mixtures of tasks\",\"Distributionally Robust Optimization and Generalization in Kernel Methods\",\"A General Theory of Equivariant CNNs on Homogeneous Spaces\",\"Trivializations for Gradient-Based Optimization on Manifolds\",\"Write, Execute, Assess: Program Synthesis with a REPL\",\"A Meta-Analysis of Overfitting in Machine Learning\",\"(Nearly) Efficient Algorithms for the Graph Matching Problem on Correlated Random Graphs\",\"Preference-Based Batch and Sequential Teaching: Towards a Unified View of Models\",\"Online Continuous Submodular Maximization: From Full-Information to Bandit Feedback\",\"Sampling Networks and Aggregate Simulation for Online POMDP Planning\",\"Correlation in Extensive-Form Games: Saddle-Point Formulation and Benchmarks\",\"GNNExplainer: Generating Explanations for Graph Neural Networks\",\"Linear Stochastic Bandits Under Safety Constraints\",\"A coupled autoencoder approach for multi-modal analysis of cell types\",\"Towards Automatic Concept-based Explanations\",\"Deep Generative Video Compression\",\"Budgeted Reinforcement Learning in Continuous State Space\",\"Discovery of Useful Questions as Auxiliary Tasks\",\"Sinkhorn Barycenters with Free Support via Frank-Wolfe Algorithm\",\"Finding the Needle in the Haystack with Convolutions: on the benefits of architectural bias\",\"Correlation clustering with local objectives\",\"Multiclass Performance Metric Elicitation\",\"Algorithmic Analysis and Statistical Estimation of SLOPE via Approximate Message Passing\",\"Explicit Explore-Exploit Algorithms in Continuous State Spaces\",\"ADDIS: an adaptive discarding algorithm for online FDR control with conservative nulls\",\"Slice-based Learning: A Programming Model for Residual Learning in Critical Data Slices\",\"Don't Blame the ELBO! A Linear VAE Perspective on Posterior Collapse\",\"Language as an Abstraction for Hierarchical Deep Reinforcement Learning\",\"Efficient online learning with kernels for adversarial large scale problems\",\"A Linearly Convergent Method for Non-Smooth Non-Convex Optimization on the Grassmannian with Applications to Robust Subspace and Dictionary Learning\",\"ObjectNet: A large-scale bias-controlled dataset for pushing the limits of object recognition models\",\"Certified Adversarial Robustness with Additive Noise\",\"Tight Dimensionality Reduction for Sketching Low Degree Polynomial Kernels\",\"Non-Cooperative Inverse Reinforcement Learning\",\"DINGO: Distributed Newton-Type Method for Gradient-Norm Optimization\",\"Sobolev Independence Criterion\",\"Maximum Entropy Monte-Carlo Planning\",\"Learning from brains how to regularize machines\",\"Using Statistics to Automate Stochastic Optimization\",\"Zero-shot Knowledge Transfer via Adversarial Belief Matching\",\"Differentiable Convex Optimization Layers\",\"Random Tessellation Forests\",\"Learning Nearest Neighbor Graphs from Noisy Distance Samples\",\"Lookahead Optimizer: k steps forward, 1 step back\",\"Learning to Predict 3D Objects with an Interpolation-based Differentiable Renderer\",\"Covariate-Powered Empirical Bayes Estimation\",\"Understanding the Role of Momentum in Stochastic Gradient Methods\",\"A neurally plausible model for online recognition and postdiction in a dynamical environment\",\"Guided Meta-Policy Search\",\"Towards Optimal Off-Policy Evaluation for Reinforcement Learning with Marginalized Importance Sampling\",\"Contextual Bandits with Cross-Learning\",\"Evaluating Protein Transfer Learning with TAPE\",\"A Bayesian Theory of Conformity in Collective Decision Making\",\"Regularization Matters: Generalization and Optimization of Neural Nets v.s. their Induced Kernel\",\"Data-dependent Sample Complexity of Deep Neural Networks via Lipschitz Augmentation\",\"A Benchmark for Interpretability Methods in Deep Neural Networks\",\"Memory Efficient Adaptive Optimization\",\"Dynamic Incentive-Aware Learning: Robust Pricing in Contextual Auctions\",\"Convergence-Rate-Matching Discretization of Accelerated Optimization Flows Through Opportunistic State-Triggered Control\",\"A Unified Framework for Data Poisoning Attack to Graph-based Semi-supervised Learning\",\"Compositional generalization through meta sequence-to-sequence learning\",\"Bayesian Joint Estimation of Multiple Graphical Models\",\"Practical Two-Step Lookahead Bayesian Optimization\",\"Leader Stochastic Gradient Descent for Distributed Training of Deep Learning Models\",\"A Convex Relaxation Barrier to Tight Robustness Verification of Neural Networks\",\"Neural Jump Stochastic Differential Equations\",\"Learning metrics for persistence-based summaries and applications for graph classification\",\"On the Value of Target Data in Transfer Learning\",\"Stochastic Variance Reduced Primal Dual Algorithms for Empirical Composition Optimization\",\"On Robustness of Principal Component Regression\",\"Meta Learning with Relational Information for Short Sequences\",\"Residual Flows for Invertible Generative Modeling\",\"Multi-Agent Common Knowledge Reinforcement Learning\",\"Learning to Learn By Self-Critique\",\"Wide Feedforward or Recurrent Neural Networks of Any Architecture are Gaussian Processes\",\"Neural Networks with Cheap Differential Operators\",\"Transductive Zero-Shot Learning with Visual Structure Constraint\",\"Dying Experts: Efficient Algorithms with Optimal Regret Bounds\",\"Model Similarity Mitigates Test Set Overuse\",\"A unified theory for the origin of grid cells through the lens of pattern formation\",\"On Sample Complexity Upper and Lower Bounds for Exact Ranking from Noisy Comparisons\",\"Hierarchical Decision Making by Generating and Following Natural Language Instructions\",\"SHE: A Fast and Accurate Deep Neural Network for Encrypted Data\",\"Locality-Sensitive Hashing for f-Divergences: Mutual Information Loss and Beyond\",\"A Game Theoretic Approach to Class-wise Selective Rationalization\",\"Efficiently avoiding saddle points with zero order methods: No gradients required\",\"Metamers of neural networks reveal divergence from human perceptual systems\",\"Spatial-Aware Feature Aggregation for Image based Cross-View Geo-Localization\",\"Decentralized sketching of low rank matrices\",\"Average Case Column Subset Selection for Entrywise \\\\ell_1-Norm Loss\",\"Efficient Forward Architecture Search\",\"Unsupervised Meta-Learning for Few-Shot Image Classification\",\"Learning Mixtures of Plackett-Luce Models from Structured Partial Orders\",\"Certainty Equivalence is Efficient for Linear Quadratic Control\",\"Scalable Bayesian inference of dendritic voltage via spatiotemporal recurrent state space models\",\"Logarithmic Regret for Online Control\",\"Elliptical Perturbations for Differential Privacy\",\"Devign: Effective Vulnerability Identification by Learning Comprehensive Program Semantics via Graph Neural Networks\",\"KNG: The K-Norm Gradient Mechanism\",\"CXPlain: Causal Explanations for Model Interpretation under Uncertainty\",\"Regularized Anderson Acceleration for Off-Policy Deep Reinforcement Learning\",\"STREETS: A Novel Camera Network Dataset for Traffic Flow\",\"Sequential Neural Processes\",\"Policy Continuation with Hindsight Inverse Dynamics\",\"Learning to Self-Train for Semi-Supervised Few-Shot Classification\",\"Temporal FiLM: Capturing Long-Range Sequence Dependencies with Feature-Wise Modulations.\",\"From Complexity to Simplicity: Adaptive ES-Active Subspaces for Blackbox Optimization\",\"On the Expressive Power of Deep Polynomial Neural Networks\",\"DETOX: A Redundancy-based Framework for Faster and More Robust Gradient Aggregation\",\"Can SGD Learn Recurrent Neural Networks with Provable Generalization?\",\"Limits of Private Learning with Access to Public Data\",\"Discrete Object Generation with Reversible Inductive Construction\",\"Efficient Near-Optimal Testing of Community Changes in Balanced Stochastic Block Models\",\"Keeping Your Distance: Solving Sparse Reward Tasks Using Self-Balancing Shaped Rewards\",\"Superset Technique for Approximate Recovery in One-Bit Compressed Sensing\",\"Bandits with Feedback Graphs and Switching Costs\",\"Functional Adversarial Attacks\",\"Statistical-Computational Tradeoff in Single Index Models\",\"On Fenchel Mini-Max Learning\",\"MarginGAN: Adversarial Training in Semi-Supervised Learning\",\"Poincar\\u00e9 Recurrence, Cycles and Spurious Equilibria in Gradient-Descent-Ascent for Non-Convex Non-Concave Zero-Sum Games\",\"A unified variance-reduced accelerated gradient method for convex optimization\",\"Nearly Tight Bounds for Robust Proper Learning of Halfspaces with a Margin\",\"Same-Cluster Querying for Overlapping Clusters\",\"Efficient Convex Relaxations for Streaming PCA\",\"Learning Robust Global Representations by Penalizing Local Predictive Power\",\"Unsupervised Curricula for Visual Meta-Reinforcement Learning\",\"Sample Complexity of Learning Mixture of Sparse Linear Regressions\",\"Large Scale Adversarial Representation Learning\",\"G2SAT: Learning to Generate SAT Formulas\",\"Neural Trust Region/Proximal Policy Optimization Attains Globally Optimal Policy\",\"Dimensionality reduction: theoretical perspective on practical measures\",\"Oracle-Efficient Algorithms for Online Linear Optimization with Bandit Feedback\",\"Multilabel reductions: what is my loss optimising?\",\"Tight Sample Complexity of Learning One-hidden-layer Convolutional Neural Networks\",\"Deep Gamblers: Learning to Abstain with Portfolio Theory\",\"Two Time-scale Off-Policy TD Learning: Non-asymptotic Analysis over Markovian Samples\",\"Transfer Learning via Minimizing the Performance Gap Between Domains\",\"Splitting Steepest Descent for Growing Neural Architectures\",\"Sequential Experimental Design for Transductive Linear Bandits\",\"Time Matters in Regularizing Deep Networks: Weight Decay and Data Augmentation Affect Early Learning Dynamics, Matter Little Near Convergence\",\"Outlier-Robust High-Dimensional Sparse Estimation via Iterative Filtering\",\"Variational Graph Recurrent Neural Networks\",\"Semi-Implicit Graph Variational Auto-Encoders\",\"Unsupervised Learning of Object Keypoints for Perception and Control\",\"A Model-Based Reinforcement Learning with Adversarial Training for Online Recommendation\",\"Optimizing Generalized Rate Metrics with Three Players\",\"Consistency-based Semi-supervised Learning for Object detection\",\"Rates of Convergence for Large-scale Nearest Neighbor Classification\",\"An Embedding Framework for Consistent Polyhedral Surrogates\",\"Cross-Modal Learning with Adversarial Samples\",\"Fast-rate PAC-Bayes Generalization Bounds via Shifted Rademacher Processes\",\"Input-Cell Attention Reduces Vanishing Saliency of Recurrent Neural Networks\",\"Program Synthesis and Semantic Parsing with Learned Code Idioms\",\"Generalization Bounds of Stochastic Gradient Descent for Wide and Deep Neural Networks\",\"High-Dimensional Optimization in Adaptive Random Subspaces\",\"Random Projections with Asymmetric Quantization\",\"Superposition of many models into one\",\"Private Testing of Distributions via Sample Permutations\",\"McDiarmid-Type Inequalities for Graph-Dependent Variables and Stability Bounds\",\"How to Initialize your Network? Robust Initialization for WeightNorm & ResNets\",\"On Making Stochastic Classifiers Deterministic\",\"Statistical Analysis of Nearest Neighbor Methods for Anomaly Detection\",\"Improving Black-box Adversarial Attacks with a Transfer-based Prior\",\"Break the Ceiling: Stronger Multi-scale Deep Graph Convolutional Networks\",\"Statistical Model Aggregation via Parameter Matching\",\"On the (In)fidelity and Sensitivity of Explanations\",\"Exponential Family Estimation via Adversarial Dynamics Embedding\",\"The Broad Optimality of Profile Maximum Likelihood\",\"MintNet: Building Invertible Neural Networks with Masked Convolutions\",\"Information-Theoretic Generalization Bounds for SGLD via Data-Dependent Estimates\",\"On Distributed Averaging for Stochastic k-PCA\",\"Controllable Unsupervised Text Attribute Transfer via Editing Entangled Latent Representation\",\"MaxGap Bandit: Adaptive Algorithms for Approximate Ranking\",\"Bias Correction of Learned Generative Models using Likelihood-Free Importance Weighting\",\"Online Forecasting of Total-Variation-bounded Sequences\",\"Local SGD with Periodic Averaging: Tighter Analysis and Adaptive Synchronization\",\"Data Parameters: A New Family of Parameters for Learning a Differentiable Curriculum\",\"Unified Sample-Optimal Property Estimation in Near-Linear Time\",\"Region Mutual Information Loss for Semantic Segmentation\",\"Learning Stable Deep Dynamics Models\",\"Image Captioning: Transforming Objects into Words\",\"Greedy Sampling for Approximate Clustering in the Presence of Outliers\",\"Adversarial Fisher Vectors for Unsupervised Representation Learning\",\"On Tractable Computation of Expected Predictions\",\"Levenshtein Transformer\",\"Unlabeled Data Improves Adversarial Robustness\",\"Machine Teaching of Active Sequential Learners\",\"Gaussian-Based Pooling for Convolutional Neural Networks\",\"Meta Architecture Search\",\"NAOMI: Non-Autoregressive Multiresolution Sequence Imputation\",\"Layer-Dependent Importance Sampling for Training Deep and Large Graph Convolutional Networks\",\"Two Generator Game: Learning to Sample via Linear Goodness-of-Fit Test\",\"Distribution oblivious, risk-aware algorithms for multi-armed bandits with unbounded rewards\",\"Private Stochastic Convex Optimization with Optimal Rates\",\"Provably Robust Deep Learning via Adversarially Trained Smoothed Classifiers\",\"Demystifying Black-box Models with Symbolic Metamodels\",\"Neural Temporal-Difference Learning Converges to Global Optima\",\"Privacy-Preserving Q-Learning with Functional Noise in Continuous Spaces\",\"Attentive State-Space Modeling of Disease Progression\",\"Online EXP3 Learning in Adversarial Bandits with Delayed Feedback\",\"A Direct tilde{O}(1/epsilon) Iteration Parallel Algorithm for Optimal Transport\",\"Faster Boosting with Smaller Memory\",\"Variance Reduction for Matrix Games\",\"Learning Neural Networks with Adaptive Regularization\",\"Distributed estimation of the inverse Hessian by determinantal averaging\",\"Smoothing Structured Decomposable Circuits\",\"Efficient and Accurate Estimation of Lipschitz Constants for Deep Neural Networks\",\"Provable Non-linear Inductive Matrix Completion\",\"Communication-Efficient Distributed Blockwise Momentum SGD with Error-Feedback\",\"Sparse Variational Inference: Bayesian Coresets from Scratch\",\"Personalizing Many Decisions with High-Dimensional Covariates\",\"A Necessary and Sufficient Stability Notion for Adaptive Generalization\",\"Necessary and Sufficient Geometries for Gradient Methods\",\"Landmark Ordinal Embedding\",\"Identification of Conditional Causal Effects under Markov Equivalence\",\"The Thermodynamic Variational Objective\",\"Global Guarantees for Blind Demodulation with Generative Priors\",\"Exact sampling of determinantal point processes with sublinear time preprocessing\",\"Geometry-Aware Neural Rendering\",\"Variational Temporal Abstraction\",\"Subquadratic High-Dimensional Hierarchical Clustering\",\"Learning Auctions with Robust Incentive Guarantees\",\"Policy Optimization Provably Converges to Nash Equilibria in Zero-Sum Linear Quadratic Games\",\"Uniform convergence may be unable to explain generalization in deep learning\",\"A Zero-Positive Learning Approach for Diagnosing Software Performance Regressions\",\"DTWNet: a Dynamic Time Warping Network\",\"Structured Graph Learning Via Laplacian Spectral Constraints\",\"Thresholding Bandit with Optimal Aggregate Regret\",\"Towards Explaining the Regularization Effect of Initial Large Learning Rate in Training Neural Networks\",\"Rethinking Kernel Methods for Node Representation Learning on Graphs\",\"Causal Confusion in Imitation Learning\",\"Optimizing Generalized PageRank Methods for Seed-Expansion Community Detection\",\"The Case for Evaluating Causal Models Using Interventional Measures and Empirical Data\",\"Dimension-Free Bounds for Low-Precision Training\",\"Concentration of risk measures: A Wasserstein distance approach\",\"Meta-Inverse Reinforcement Learning with Probabilistic Context Variables\",\"Stabilizing Off-Policy Q-Learning via Bootstrapping Error Reduction\",\"Bayesian Optimization with Unknown Search Space\",\"On the Downstream Performance of Compressed Word Embeddings\",\"Gradient based sample selection for online continual learning\",\"Multivariate Distributionally Robust Convex Regression under Absolute Error Loss\",\"Attribution-Based Confidence Metric For Deep Neural Networks\",\"Neural Relational Inference with Fast Modular Meta-learning\",\"Theoretical evidence for adversarial robustness through randomization\",\"Online Continual Learning with Maximal Interfered Retrieval\",\"Neural Attribution for Semantic Bug-Localization in Student Programs\",\"Adaptive Temporal-Difference Learning for Policy Evaluation with Per-State Uncertainty Estimates\",\"SPoC: Search-based Pseudocode to Code\",\"Generative Modeling by Estimating Gradients of the Data Distribution\",\"Adversarial Music: Real world Audio Adversary against Wake-word Detection System\",\"Prediction of Spatial Point Processes: Regularized Method with Out-of-Sample Guarantees\",\"Debiased Bayesian inference for average treatment effects\",\"Margin-Based Generalization Lower Bounds for Boosted Classifiers\",\"Connections Between Mirror Descent, Thompson Sampling and the Information Ratio\",\"Graph Transformer Networks\",\"Learning to Confuse: Generating Training Time Adversarial Data with Auto-Encoder\",\"The Impact of Regularization on High-dimensional Logistic Regression\",\"Adaptive Density Estimation for Generative Models\",\"Fast and Provable ADMM for Learning with Generative Priors\",\"Weighted Linear Bandits for Non-Stationary Environments\",\"Improved Regret Bounds for Bandit Combinatorial Optimization\",\"Pareto Multi-Task Learning\",\"SIC-MMAB: Synchronisation Involves Communication in Multiplayer Multi-Armed Bandits\",\"Novel positional encodings to enable tree-based transformers\",\"A Domain Agnostic Measure for Monitoring and Evaluating GANs\",\"Submodular Function Minimization with Noisy Evaluation Oracle\",\"Counting the Optimal Solutions in Graphical Models\",\"Bootstrapping Upper Confidence Bound\",\"Modelling the Dynamics of Multiagent Q-Learning in Repeated Symmetric Games: a Mean Field Theoretic Approach\",\"Integer Discrete Flows and Lossless Compression\",\"Deep Multimodal Multilinear Fusion with High-order Polynomial Pooling\",\"Structured Prediction with Projection Oracles\",\"A Primal Dual Formulation For Deep Learning With Constraints\",\"Screening Sinkhorn Algorithm for Regularized Optimal Transport\",\"PAC-Bayes Un-Expected Bernstein Inequality\",\"Are Labels Required for Improving Adversarial Robustness?\",\"Tight Regret Bounds for Model-Based Reinforcement Learning with Greedy Policies\",\"Multi-objective Bayesian optimisation with preferences over objectives\",\"Think out of the \\\"Box\\\": Generically-Constrained Asynchronous Composite Optimization and Hedging\",\"Calibration tests in multi-class classification: A unifying framework\",\"Classification Accuracy Score for Conditional Generative Models\",\"Theoretical Analysis of Adversarial Learning: A Minimax Approach\",\"Multiagent Evaluation under Incomplete Information\",\"Tree-Sliced Variants of Wasserstein Distances\",\"Beyond temperature scaling: Obtaining well-calibrated multi-class probabilities with Dirichlet calibration\",\"Robustness Verification of Tree-based Models\",\"Comparing distributions: \\\\ell_1 geometry improves kernel two-sample testing\",\"Fast and Accurate Stochastic Gradient Estimation\",\"Towards Interpretable Reinforcement Learning Using Attention Augmented Agents\",\"Theoretical Limits of Pipeline Parallel Optimization and Application to Distributed Deep Learning\",\"Universality in Learning from Linear Measurements\",\"Root Mean Square Layer Normalization\",\"Exponentially convergent stochastic k-PCA without variance reduction\",\"Planning in entropy-regularized Markov decision processes and games\",\"R2D2: Reliable and Repeatable Detector and Descriptor\",\"Selective Sampling-based Scalable Sparse Subspace Clustering\",\"A General Framework for Symmetric Property Estimation\",\"Structured Variational Inference in Continuous Cox Process Models\",\"Generalization of Reinforcement Learners with Working and Episodic Memory\",\"Efficient Identification in Linear Structural Causal Models with Instrumental Cutsets\",\"Distribution Learning of a Random Spatial Field with a Location-Unaware Mobile Sensor\",\"Hindsight Credit Assignment\",\"Kernelized Bayesian Softmax for Text Generation\",\"When to Trust Your Model: Model-Based Policy Optimization\",\"Correlation Clustering with Adaptive Similarity Queries\",\"Control What You Can: Intrinsically Motivated Task-Planning Agent\",\"Selecting causal brain features with a single conditional independence test per feature\",\"A Generic Acceleration Framework for Stochastic Composite Optimization\",\"Continuous Hierarchical Representations with Poincar\\u00e9 Variational Auto-Encoders\",\"Beating SGD Saturation with Tail-Averaging and Minibatching\",\"Random Quadratic Forms with Dependence: Applications to Restricted Isometry and Beyond\",\"Continuous-time Models for Stochastic Optimization Algorithms\",\"Curriculum-guided Hindsight Experience Replay\",\"Implicit Semantic Data Augmentation for Deep Networks\",\"MetaInit: Initializing learning by learning to initialize\",\"Scalable Deep Generative Relational Model with High-Order Node Dependence\",\"Random Path Selection for Continual Learning\",\"Efficient Algorithms for Smooth Minimax Optimization\",\"Shadowing Properties of Optimization Algorithms\",\"Causal Regularization\",\"Learning Hawkes Processes from a handful of events\",\"Unsupervised Object Segmentation by Redrawing\",\"Regret Bounds for Learning State Representations in Reinforcement Learning\",\"Band-Limited Gaussian Processes: The Sinc Kernel\",\"Leveraging Labeled and Unlabeled Data for Consistent Fair Binary Classification\",\"Learning search spaces for Bayesian optimization: Another view of hyperparameter transfer learning\",\"Streaming Bayesian Inference for Crowdsourced Classification\",\"Neuropathic Pain Diagnosis Simulator for Causal Discovery Algorithm Evaluation\",\"Brain-Like Object Recognition with High-Performing Shallow Recurrent ANNs\",\"k-Means Clustering of Lines for Big Data\",\"Random Projections and Sampling Algorithms for Clustering of High-Dimensional Polygonal Curves\",\"Recurrent Space-time Graph Neural Networks\",\"Uncertainty on Asynchronous Time Event Prediction\",\"Accurate, reliable and fast robustness evaluation\",\"Sparse High-Dimensional Isotonic Regression\",\"Triad Constraints for Learning Causal Structure of Latent Variables\",\"On the Inductive Bias of Neural Tangent Kernels\",\"Cross-Domain Transferability of Adversarial Perturbations\",\"Shallow RNN: Accurate Time-series Classification on Resource Constrained Devices\",\"Kernel quadrature with DPPs\",\"REM: From Structural Entropy to Community Structure Deception\",\"Sim2real transfer learning for 3D human pose estimation: motion to the rescue\",\"Self-Supervised Deep Learning on Point Clouds by Reconstructing Space\",\"Minimum Stein Discrepancy Estimators\",\"Piecewise Strong Convexity of Neural Networks\",\"Fast and Furious Learning in Zero-Sum Games: Vanishing Regret with Non-Vanishing Step Sizes\",\"Generalization Bounds for Neural Networks via Approximate Description Length\",\"Provably robust boosted decision stumps and trees against adversarial attacks\",\"Convergence of Adversarial Training in Overparametrized Neural Networks\",\"A Composable Specification Language for Reinforcement Learning Tasks\",\"The Option Keyboard: Combining Skills in Reinforcement Learning\",\"Unified Language Model Pre-training for Natural Language Understanding and Generation\",\"Learning to Correlate in Multi-Player General-Sum Sequential Games\",\"Stochastic Continuous Greedy ++: When Upper and Lower Bounds Match\",\"Generative Well-intentioned Networks\",\"Learning step sizes for unfolded sparse coding\",\"Online-Within-Online Meta-Learning\",\"Biases for Emergent Communication in Multi-agent Reinforcement Learning\",\"Episodic Memory in Lifelong Language Learning\",\"Communication-efficient Distributed SGD with Sketching\",\"A Simple Baseline for Bayesian Uncertainty in Deep Learning\",\"Modeling Conceptual Understanding in Image Reference Games\",\"Near Neighbor: Who is the Fairest of Them All?\",\"Kalman Filter, Sensor Fusion, and Constrained Regression: Equivalences and Insights\",\"Outlier-robust estimation of a sparse linear model using \\\\ell_1-penalized Huber's M-estimator\",\"Learning nonlinear level sets for dimensionality reduction in function approximation\",\"Assessing Social and Intersectional Biases in Contextualized Word Representations\",\"Online Convex Matrix Factorization with Representative Regions\",\"Self-supervised GAN: Analysis and Improvement with Multi-class Minimax Game\",\"Extreme Classification in Log Memory using Count-Min Sketch: A Case Study of Amazon Search with 50M Products\",\"A Fourier Perspective on Model Robustness in Computer Vision\",\"The continuous Bernoulli: fixing a pervasive error in variational autoencoders\",\"Privacy Amplification by Mixing and Diffusion Mechanisms\",\"Variance Reduction in Bipartite Experiments through Correlation Clustering\",\"Gossip-based Actor-Learner Architectures for Deep Reinforcement Learning\",\"Metalearned Neural Memory\",\"Learning Multiple Markov Chains via Adaptive Allocation\",\"Deep Random Splines for Point Process Intensity Estimation of Neural Population Data\",\"Diffusion Improves Graph Learning\",\"Variational Bayes under Model Misspecification\",\"Global Convergence of Gradient Descent for Deep Linear Residual Networks\",\"Manifold denoising by Nonlinear Robust Principal Component Analysis\",\"On Differentially Private Graph Sparsification and Applications\",\"Near-Optimal Reinforcement Learning in Dynamic Treatment Regimes\",\"ODE2VAE: Deep generative second order ODEs with Bayesian neural networks\",\"Optimal Sampling and Clustering in the Stochastic Block Model\",\"Recurrent Kernel Networks\",\"Cold Case: The Lost MNIST Digits\",\"Hierarchical Optimal Transport for Multimodal Distribution Alignment\",\"Shaping Belief States with Generative Environment Models for RL\",\"Exploration via Hindsight Goal Generation\",\"Globally Optimal Learning for Structured Elliptical Losses\",\"Specific and Shared Causal Relation Modeling and Mechanism-Based Clustering\",\"Object landmark discovery through unsupervised adaptation\",\"Search-Guided, Lightly-Supervised Training of Structured Prediction Energy Networks\",\"Accelerating Rescaled Gradient Descent: Fast Optimization of Smooth Functions\",\"RUDDER: Return Decomposition for Delayed Rewards\",\"Graph Normalizing Flows\",\"Explanations can be manipulated and geometry is to blame\",\"Communication trade-offs for Local-SGD with large step size\",\"Non-normal Recurrent Neural Network (nnRNN): learning long time dependencies while improving expressivity with transient dynamics\",\"No-Regret Learning in Unknown Games with Correlated Payoffs\",\"Alleviating Label Switching with Optimal Transport\",\"Paraphrase Generation with Latent Bag of Words\",\"An Algorithmic Framework For Differentially Private Data Analysis on Trusted Processors\",\"Approximating Interactive Human Evaluation with Self-Play for Open-Domain Dialog Systems\",\"Compacting, Picking and Growing for Unforgetting Continual Learning\",\"A New Distribution on the Simplex with Auto-Encoding Applications\",\"AutoPrune: Automatic Network Pruning by Regularizing Auxiliary Parameters\",\"Learning about an exponential amount of conditional distributions\",\"A neurally plausible model learns successor representations in partially observable environments\",\"Towards modular and programmable architecture search\",\"Towards Hardware-Aware Tractable Learning of Probabilistic Models\",\"On Robustness to Adversarial Examples and Polynomial Optimization\",\"Rand-NSG: Fast Accurate Billion-point Nearest Neighbor Search on a Single Node\",\"MonoForest framework for tree ensemble analysis\",\"A Solvable High-Dimensional Model of GAN\",\"Bayesian Optimization under Heavy-tailed Payoffs\",\"Using Embeddings to Correct for Unobserved Confounding in Networks\",\"Combining Generative and Discriminative Models for Hybrid Inference\",\"A Graph Theoretic Additive Approximation of Optimal Transport\",\"Adversarial Robustness through Local Linearization\",\"Learning Fairness in Multi-Agent Systems\",\"Sampled Softmax with Random Fourier Features\",\"Primal-Dual Block Generalized Frank-Wolfe\",\"Semi-flat minima and saddle points by embedding neural networks to overparameterization\",\"GOT: An Optimal Transport framework for Graph comparison\",\"On Mixup Training: Improved Calibration and Predictive Uncertainty for Deep Neural Networks\",\"Complexity of Highly Parallel Non-Smooth Convex Optimization\",\"Inverting Deep Generative models, One layer at a time\",\"Calculating Optimistic Likelihoods Using (Geodesically) Convex Optimization\",\"The Implicit Metropolis-Hastings Algorithm\",\"An Inexact Augmented Lagrangian Framework for Nonconvex Optimization with Nonlinear Constraints\",\"Generalization in Reinforcement Learning with Selective Noise Injection and Information Bottleneck\",\"Accurate Layerwise Interpretable Competence Estimation\",\"Can you trust your model's uncertainty? Evaluating predictive uncertainty under dataset shift\",\"A New Perspective on Pool-Based Active Classification and False-Discovery Control\",\"Defending Neural Backdoors via Generative Distribution Modeling\",\"Are Sixteen Heads Really Better than One?\",\"Multi-resolution Multi-task Gaussian Processes\",\"Variational Bayesian Optimal Experimental Design\",\"Universal Approximation of Input-Output Maps by Temporal Convolutional Nets\",\"Provable Certificates for Adversarial Examples: Fitting a Ball in the Union of Polytopes\",\"Reinforcement Learning with Convex Constraints\",\"User-Specified Local Differential Privacy in Unconstrained Adaptive Online Learning\",\"Stochastic Bandits with Context Distributions\",\"Recovering Bandits\",\"Inducing brain-relevant bias in natural language processing models\",\"Computing Linear Restrictions of Neural Networks\",\"Using a Logarithmic Mapping to Enable Lower Discount Factors in Reinforcement Learning\",\"Learning Positive Functions with Pseudo Mirror Descent\",\"Correlation Priors for Reinforcement Learning\",\"Fast, Provably convergent IRLS Algorithm for p-norm Linear Regression\",\"A Similarity-preserving Network Trained on Transformed Images Recapitulates Salient Features of the Fly Motion Detection Circuit\",\"Differentially Private Covariance Estimation\",\"Outlier Detection and Robust PCA Using a Convex Measure of Innovation\",\"Integrating Markov processes with structural causal modeling enables counterfactual inference in complex systems\",\"Are Disentangled Representations Helpful for Abstract Visual Reasoning?\",\"Constraint-based Causal Structure Learning with Consistent Separating Sets\",\"PowerSGD: Practical Low-Rank Gradient Compression for Distributed Optimization\",\"Unsupervised Discovery of Temporal Structure in Noisy Data with Dynamical Components Analysis\",\"Stochastic Frank-Wolfe for Composite Convex Minimization\",\"Robust Attribution Regularization\",\"Sample Efficient Active Learning of Causal Trees\",\"Computational Mirrors: Blind Inverse Light Transport by Deep Matrix Factorization\",\"Efficient Neural Architecture Transformation Search in Channel-Level for Object Detection\",\"When to use parametric models in reinforcement learning?\",\"General E(2)-Equivariant Steerable CNNs\",\"Characterization and Learning of Causal Graphs with Latent Variables from Soft Interventions\",\"Structure Learning with Side Information: Sample Complexity\",\"Untangling in Invariant Speech Recognition\",\"Flexible information routing in neural populations through stochastic comodulation\",\"Generalization Bounds in the Predict-then-Optimize Framework\",\"Categorized Bandits\",\"Worst-Case Regret Bounds for Exploration via Randomized Value Functions\",\"Efficient characterization of electrically evoked responses for neural interfaces\",\"Differentially Private Distributed Data Summarization under Covariate Shift\",\"Hamiltonian descent for composite objectives\",\"Implicit Regularization of Accelerated Methods in Hilbert Spaces\",\"Non-Asymptotic Pure Exploration by Solving Games\",\"Implicit Posterior Variational Inference for Deep Gaussian Processes\",\"Deep Multi-State Dynamic Recurrent Neural Networks Operating on Wavelet Based Neural Features for Robust Brain Machine Interfaces\",\"Censored Semi-Bandits: A Framework for Resource Allocation with Censored Feedback\",\"Cormorant: Covariant Molecular Neural Networks\",\"Reverse KL-Divergence Training of Prior Networks: Improved Uncertainty and Adversarial Robustness\",\"Reflection Separation using a Pair of Unpolarized and Polarized Images\",\"Policy Poisoning in Batch Reinforcement Learning and Control\",\"Low-Complexity Nonparametric Bayesian Online Prediction with Universal Guarantees\",\"Pure Exploration with Multiple Correct Answers\",\"Explaining Landscape Connectivity of Low-cost Solutions for Multilayer Nets\",\"On the Fairness of Disentangled Representations\",\"Compiler Auto-Vectorization with Imitation Learning\",\"A Generalized Algorithm for Multi-Objective Reinforcement Learning and Policy Adaptation\",\"Exact Gaussian Processes on a Million Data Points\",\"Bayesian Layers: A Module for Neural Network Uncertainty\",\"Learning Compositional Neural Programs with Recursive Tree Search and Planning\",\"Nonparametric Contextual Bandits in Metric Spaces with Unknown Metric\",\"Qsparse-local-SGD: Distributed SGD with Quantization, Sparsification and Local Computations\",\"Likelihood Ratios for Out-of-Distribution Detection\",\"Discrete Flows: Invertible Generative Models of Discrete Data\",\"A Self Validation Network for Object-Level Human Attention Estimation\",\"Model Selection for Contextual Bandits\",\"Sliced Gromov-Wasserstein\",\"Towards Practical Alternating Least-Squares for CCA\",\"Deep Leakage from Gradients\",\"Invariance-inducing regularization using worst-case transformations suffices to boost accuracy and spatial robustness\",\"Algorithm-Dependent Generalization Bounds for Overparameterized Deep Residual Networks\",\"Value Function in Frequency Domain and the Characteristic Value Iteration Algorithm\",\"Icebreaker: Element-wise Efficient Information Acquisition with a Bayesian Deep Latent Gaussian Model\",\"Algorithmic Guarantees for Inverse Imaging with Untrained Network Priors\",\"Planning with Goal-Conditioned Policies\",\"Don't take it lightly: Phasing optical random projections with unknown operators\",\"Generating Diverse High-Fidelity Images with VQ-VAE-2\",\"Generalized Matrix Means for Semi-Supervised Learning with Multilayer Graphs\",\"Online Optimal Control with Linear Dynamics and Predictions: Algorithms and Regret Analysis\",\"Missing Not at Random in Matrix Completion: The Effectiveness of Estimating Missingness Probabilities Under a Low Nuclear Norm Assumption\",\"MelGAN: Generative Adversarial Networks for Conditional Waveform Synthesis\",\"Offline Contextual Bandits with High Probability Fairness Guarantees\",\"Solving a Class of Non-Convex Min-Max Games Using Iterative First Order Methods\",\"Semantic-Guided Multi-Attention Localization for Zero-Shot Learning\",\"Interpreting and improving natural-language processing (in machines) with natural language-processing (in the brain)\",\"Function-Space Distributions over Kernels\",\"The Step Decay Schedule: A Near Optimal, Geometrically Decaying Learning Rate Procedure For Least Squares\",\"Compositional Plan Vectors\",\"Locally Private Learning without Interaction Requires Separation\",\"Robust Bi-Tempered Logistic Loss Based on Bregman Divergences\",\"Computational Separations between Sampling and Optimization\",\"Surfing: Iterative Optimization Over Incrementally Trained Deep Networks\",\"Learning to Optimize in Swarms\",\"On Human-Aligned Risk Minimization\",\"Semi-Parametric Efficient Policy Learning with Continuous Actions\",\"Multi-task Learning for Aggregated Data using Gaussian Processes\",\"Minimal Variance Sampling in Stochastic Gradient Boosting\",\"Beyond the Single Neuron Convex Barrier for Neural Network Certification\",\"An Algorithm to Learn Polytree Networks with Hidden Nodes\",\"Efficiently Learning Fourier Sparse Set Functions\",\"Projected Stein Variational Newton: A Fast and Scalable Bayesian Inference Method in High Dimensions\",\"Invariance and identifiability issues for word embeddings\",\"Generalization Error Analysis of Quantized Compressive Learning\",\"Multi-Criteria Dimensionality Reduction with Applications to Fairness\",\"Efficient Rematerialization for Deep Networks\",\"Mo' States Mo' Problems: Emergency Stop Mechanisms from Observation\",\"Machine Learning Estimation of Heterogeneous Treatment Effects with Instruments\",\"Understanding Sparse JL for Feature Hashing\",\"Text-Based Interactive Recommendation via Constraint-Augmented Reinforcement Learning\",\"Flexible Modeling of Diversity with Strongly Log-Concave Distributions\",\"Momentum-Based Variance Reduction in Non-Convex SGD\",\"Search on the Replay Buffer: Bridging Planning and Reinforcement Learning\",\"Can Unconditional Language Models Recover Arbitrary Sentences?\",\"Group Retention when Using Machine Learning in Sequential Decision Making: the Interplay between User Dynamics and Fairness\",\"Faster width-dependent algorithm for mixed packing and covering LPs\",\"Flattening a Hierarchical Clustering through Active Learning\",\"DeepWave: A Recurrent Neural-Network for Real-Time Acoustic Imaging\",\"Certifying Geometric Robustness of Neural Networks\",\"Goal-conditioned Imitation Learning\",\"Robust exploration in linear quadratic reinforcement learning\",\"DRUM: End-To-End Differentiable Rule Mining On Knowledge Graphs\",\"Kernel Truncated Randomized Ridge Regression: Optimal Rates and Low Noise Acceleration\",\"Input-Output Equivalence of Unitary and Contractive RNNs\",\"Hamiltonian Neural Networks\",\"Preventing Gradient Attenuation in Lipschitz Constrained Convolutional Networks\",\"Structured and Deep Similarity Matching via Structured and Deep Hebbian Networks\",\"Understanding the Representation Power of Graph Neural Networks in Learning Graph Topology\",\"Multiple Futures Prediction\",\"Explicitly disentangling image content from translation and rotation with spatial-VAE\",\"Power analysis of knockoff filters for correlated designs\",\"A Kernel Loss for Solving the Bellman Equation\",\"Low-Rank Bandit Methods for High-Dimensional Dynamic Pricing\",\"Differential Privacy Has Disparate Impact on Model Accuracy\",\"Riemannian batch normalization for SPD neural networks\",\"Neural Taskonomy: Inferring the Similarity of Task-Derived Representations from Brain Activity\",\"Stacked Capsule Autoencoders\",\"Learning Reward Machines for Partially Observable Reinforcement Learning\",\"Learning Representations by Maximizing Mutual Information Across Views\",\"Amortized Bethe Free Energy Minimization for Learning MRFs\",\"Small ReLU networks are powerful memorizers: a tight analysis of memorization capacity\",\"Legendre Memory Units: Continuous-Time Representation in Recurrent Neural Networks\",\"Exact Combinatorial Optimization with Graph Convolutional Neural Networks\",\"Fast structure learning with modular regularization\",\"Wasserstein Dependency Measure for Representation Learning\",\"TAB-VCR: Tags and Attributes based VCR Baselines\",\"Universality and individuality in neural dynamics across large populations of recurrent networks\",\"End-to-End Learning on 3D Protein Structure for Interface Prediction\",\"A Family of Robust Stochastic Operators for Reinforcement Learning\",\"Using Self-Supervised Learning Can Improve Model Robustness and Uncertainty\",\"Inherent Tradeoffs in Learning Fair Representations\",\"Are deep ResNets provably better than linear predictors?\",\"Reverse engineering recurrent networks for sentiment classification reveals line attractor dynamics\",\"BehaveNet: nonlinear embedding and Bayesian neural decoding of behavioral videos\",\"Variational Mixture-of-Experts Autoencoders for Multi-Modal Deep Generative Models\",\"Gradient-based Adaptive Markov Chain Monte Carlo\",\"On the Transfer of Inductive Bias from Simulation to the Real World: a New Disentanglement Dataset\",\"Imitation-Projected Programmatic Reinforcement Learning\",\"Learning Data Manipulation for Augmentation and Weighting\",\"Exploring Algorithmic Fairness in Robust Graph Covering Problems\",\"Abstraction based Output Range Analysis for Neural Networks\",\"Space and Time Efficient Kernel Density Estimation in High Dimensions\",\"PIDForest: Anomaly Detection via Partial Identification\",\"Generative Models for Graph-Based Protein Design\",\"The Geometry of Deep Networks: Power Diagram Subdivision\",\"Approximate Feature Collisions in Neural Nets\",\"Ease-of-Teaching and Language Structure from Emergent Communication\",\"Generalization in multitask deep neural classifiers: a statistical physics approach\",\"Optimistic Distributionally Robust Optimization for Nonparametric Likelihood Approximation\",\"On Relating Explanations and Adversarial Examples\",\"On the equivalence between graph isomorphism testing and function approximation with GNNs\",\"Surround Modulation: A Bio-inspired Connectivity Structure for Convolutional Neural Networks\",\"Self-attention with Functional Time Representation Learning\",\"Re-randomized Densification for One Permutation Hashing and Bin-wise Consistent Weighted Sampling\",\"Enabling hyperparameter optimization in sequential autoencoders for spiking neural data\",\"Efficient Algorithms for Non-convex Isotonic Regression through Submodular Optimization\",\"Structure-Aware Convolutional Neural Networks\",\"Kalman Normalization: Normalizing Internal Representations Across Network Layers\",\"HOGWILD!-Gibbs can be PanAccurate\",\"Text-Adaptive Generative Adversarial Networks: Manipulating Images with Natural Language\",\"IntroVAE: Introspective Variational Autoencoders for Photographic Image Synthesis\",\"Doubly Robust Bayesian Inference for Non-Stationary Streaming Data with \\\\beta-Divergences\",\"Adapted Deep Embeddings: A Synthesis of Methods for k-Shot Inductive Transfer Learning\",\"Generalized Inverse Optimization through Online Learning\",\"An Off-policy Policy Gradient Theorem Using Emphatic Weightings\",\"Supervised autoencoders: Improving generalization performance with unsupervised regularizers\",\"Visual Object Networks: Image Generation with Disentangled 3D Representations\",\"Understanding Weight Normalized Deep Neural Networks with Rectified Linear Units\",\"Learning Pipelines with Limited Data and Domain Knowledge: A Study in Parsing Physics Problems\",\"Learning long-range spatial dependencies with horizontal gated recurrent units\",\"Joint Sub-bands Learning with Clique Structures for Wavelet Domain Super-Resolution\",\"Fast Similarity Search via Optimal Sparse Lifting\",\"Learning Deep Disentangled Embeddings With the F-Statistic Loss\",\"Geometrically Coupled Monte Carlo Sampling\",\"Cooperative Holistic Scene Understanding: Unifying 3D Object, Layout, and Camera Pose Estimation\",\"An Efficient Pruning Algorithm for Robust Isotonic Regression\",\"PAC-learning in the presence of adversaries\",\"Sparse DNNs with Improved Adversarial Robustness\",\"Snap ML: A Hierarchical Framework for Machine Learning\",\"See and Think: Disentangling Semantic Scene Completion\",\"Chain of Reasoning for Visual Question Answering\",\"Sigsoftmax: Reanalysis of the Softmax Bottleneck\",\"Deep Non-Blind Deconvolution via Generalized Low-Rank Approximation\",\"Bayesian Pose Graph Optimization via Bingham Distributions and Tempered Geodesic MCMC\",\"MetaAnchor: Learning to Detect Objects with Customized Anchors\",\"Image Inpainting via Generative Multi-column Convolutional Neural Networks\",\"On Misinformation Containment in Online Social Networks\",\"A^2-Nets: Double Attention Networks\",\"Self-Supervised Generation of Spatial Audio for 360\\u00b0 Video\",\"How Many Samples are Needed to Estimate a Convolutional Neural Network?\",\"Algorithmic Regularization in Learning Deep Homogeneous Models: Layers are Automatically Balanced\",\"Optimization for Approximate Submodularity\",\"(Probably) Concave Graph Matching\",\"Deep Defense: Training DNNs with Improved Adversarial Robustness\",\"Rest-Katyusha: Exploiting the Solution's Structure via Scheduled Restart Schemes\",\"Implicit Reparameterization Gradients\",\"Training DNNs with Hybrid Block Floating Point\",\"A Model for Learned Bloom Filters and Optimizing by Sandwiching\",\"Soft-Gated Warping-GAN for Pose-Guided Person Image Synthesis\",\"Deep Functional Dictionaries: Learning Consistent Semantic Structures on 3D Models from Functions\",\"Nonlocal Neural Networks, Nonlocal Diffusion and Nonlocal Modeling\",\"Are ResNets Provably Better than Linear Predictors?\",\"Learning to Decompose and Disentangle Representations for Video Prediction\",\"Multi-Task Learning as Multi-Objective Optimization\",\"Combinatorial Optimization with Graph Convolutional Networks and Guided Tree Search\",\"Self-Erasing Network for Integral Object Attention\",\"LinkNet: Relational Embedding for Scene Graph\",\"How to Start Training: The Effect of Initialization and Architecture\",\"Which Neural Net Architectures Give Rise to Exploding and Vanishing Gradients?\",\"Explanations based on the Missing: Towards Contrastive Explanations with Pertinent Negatives\",\"HitNet: Hybrid Ternary Recurrent Neural Network\",\"A Unified Framework for Extensive-Form Game Abstraction with Bounds\",\"Removing the Feature Correlation Effect of Multiplicative Noise\",\"Maximum-Entropy Fine Grained Classification\",\"On Learning Markov Chains\",\"A Neural Compositional Paradigm for Image Captioning\",\"Quantifying Learning Guarantees for Convex but Inconsistent Surrogates\",\"Dialog-based Interactive Image Retrieval\",\"SPIDER: Near-Optimal Non-Convex Optimization via Stochastic Path-Integrated Differential Estimator\",\"Are GANs Created Equal? A Large-Scale Study\",\"Learning Disentangled Joint Continuous and Discrete Representations\",\"TADAM: Task dependent adaptive metric for improved few-shot learning\",\"Do Less, Get More: Streaming Submodular Maximization with Subsampling\",\"Deep Neural Nets with Interpolating Function as Output Activation\",\"FishNet: A Versatile Backbone for Image, Region, and Pixel Level Prediction\",\"Visual Memory for Robust Path Following\",\"KDGAN: Knowledge Distillation with Generative Adversarial Networks\",\"Long short-term memory and Learning-to-learn in networks of spiking neurons\",\"Greedy Hash: Towards Fast Optimization for Accurate Hash Coding in CNN\",\"Informative Features for Model Comparison\",\"PointCNN: Convolution On X-Transformed Points\",\"Connectionist Temporal Classification with Maximum Entropy Regularization\",\"Large Margin Deep Networks for Classification\",\"Generalizing Graph Matching beyond Quadratic Assignment Model\",\"Solving Large Sequential Games with the Excessive Gap Technique\",\"Discrimination-aware Channel Pruning for Deep Neural Networks\",\"On the Dimensionality of Word Embedding\",\"Reinforced Continual Learning\",\"Uncertainty-Aware Attention for Reliable Interpretation and Prediction\",\"DropMax: Adaptive Variational Softmax\",\"Posterior Concentration for Sparse Deep Learning\",\"A flexible model for training action localization with varying levels of supervision\",\"A Deep Bayesian Policy Reuse Approach Against Non-Stationary Agents\",\"Empirical Risk Minimization in Non-interactive Local Differential Privacy Revisited\",\"Low-shot Learning via Covariance-Preserving Adversarial Augmentation Networks\",\"Learning semantic similarity in a continuous space\",\"MetaReg: Towards Domain Generalization using Meta-Regularization\",\"Boosted Sparse and Low-Rank Tensor Regression\",\"Domain-Invariant Projection Learning for Zero-Shot Recognition\",\"Neural-Symbolic VQA: Disentangling Reasoning from Vision and Language Understanding\",\"Frequency-Domain Dynamic Pruning for Convolutional Neural Networks\",\"Quadratic Decomposable Submodular Function Minimization\",\"A Block Coordinate Ascent Algorithm for Mean-Variance Optimization\",\"\\\\ell_1-regression with Heavy-tailed Distributions\",\"Neural Nearest Neighbors Networks\",\"Efficient nonmyopic batch active search\",\"A Game-Theoretic Approach to Recommendation Systems with Strategic Content Providers\",\"Interactive Structure Learning with Structural Query-by-Committee\",\"Global Geometry of Multichannel Sparse Blind Deconvolution on the Sphere\",\"Video-to-Video Synthesis\",\"How To Make the Gradients Small Stochastically: Even Faster Convex and Nonconvex SGD\",\"Synthesized Policies for Transfer and Adaptation across Tasks and Environments\",\"Adversarial vulnerability for any classifier\",\"Evolution-Guided Policy Gradient in Reinforcement Learning\",\"Toddler-Inspired Visual Object Learning\",\"Alternating optimization of decision trees, with application to learning sparse oblique trees\",\"FD-GAN: Pose-guided Feature Distilling GAN for Robust Person Re-identification\",\"New Insight into Hybrid Stochastic Gradient Descent: Beyond With-Replacement Sampling and Convexity\",\"The Lingering of Gradients: How to Reuse Gradients Over Time\",\"Unsupervised Learning of View-invariant Action Representations\",\"Fairness Behind a Veil of Ignorance: A Welfare Analysis for Automated Decision Making\",\"Global Gated Mixture of Second-order Pooling for Improving Deep Convolutional Neural Networks\",\"Image-to-image translation for cross-domain disentanglement\",\"Gradient Sparsification for Communication-Efficient Distributed Optimization\",\"Revisiting Multi-Task Learning with ROCK: a Deep Residual Auxiliary Block for Visual Detection\",\"Adaptive Online Learning in Dynamic Environments\",\"FRAGE: Frequency-Agnostic Word Representation\",\"Generative Neural Machine Translation\",\"Found Graph Data and Planted Vertex Covers\",\"Joint Active Feature Acquisition and Classification with Variable-Size Set Encoding\",\"Regularization Learning Networks: Deep Learning for Tabular Datasets\",\"Multitask Boosting for Survival Analysis with Competing Risks\",\"Geometry Based Data Generation\",\"SLAYER: Spike Layer Error Reassignment in Time\",\"On Oracle-Efficient PAC RL with Rich Observations\",\"Gradient Descent for Spiking Neural Networks\",\"Generalizing Tree Probability Estimation via Bayesian Networks\",\"Where Do You Think You're Going?: Inferring Beliefs about Dynamics from Behavior\",\"Designing by Training: Acceleration Neural Network for Fast High-Dimensional Convolution\",\"Understanding the Role of Adaptivity in Machine Teaching: The Case of Version Space Learners\",\"A loss framework for calibrated anomaly detection\",\"PacGAN: The power of two samples in generative adversarial networks\",\"Variational Memory Encoder-Decoder\",\"Stochastic Composite Mirror Descent: Optimal Bounds with High Probabilities\",\"Hybrid Retrieval-Generation Reinforced Agent for Medical Image Report Generation\",\"Overcoming Language Priors in Visual Question Answering with Adversarial Regularization\",\"Hybrid Knowledge Routed Modules for Large-scale Object Detection\",\"Bilinear Attention Networks\",\"Parsimonious Quantile Regression of Financial Asset Tail Dynamics via Sequential Learning\",\"Multi-Class Learning: From Theory to Algorithm\",\"Multivariate Time Series Imputation with Generative Adversarial Networks\",\"Learning Versatile Filters for Efficient Convolutional Neural Networks\",\"Accelerated Stochastic Matrix Inversion: General Theory and Speeding up BFGS Rules for Faster Second-Order Optimization\",\"DifNet: Semantic Segmentation by Diffusion Networks\",\"Conditional Adversarial Domain Adaptation\",\"Neighbourhood Consensus Networks\",\"Relating Leverage Scores and Density using Regularized Christoffel Functions\",\"Non-Local Recurrent Network for Image Restoration\",\"Bayesian Semi-supervised Learning with Graph Gaussian Processes\",\"Foreground Clustering for Joint Segmentation and Localization in Videos and Images\",\"Video Prediction via Selective Sampling\",\"Distilled Wasserstein Learning for Word Embedding and Topic Modeling\",\"Learning to Exploit Stability for 3D Scene Parsing\",\"Neural Guided Constraint Logic Programming for Program Synthesis\",\"Genetic-Gated Networks for Deep Reinforcement Learning\",\"Fighting Boredom in Recommender Systems with Linear Reinforcement Learning\",\"Enhancing the Accuracy and Fairness of Human Decision Making\",\"Temporal Regularization for Markov Decision Process\",\"The Pessimistic Limits and Possibilities of Margin-based Losses in Semi-supervised Learning\",\"Simple random search of static linear policies is competitive for reinforcement learning\",\"Generating Informative and Diverse Conversational Responses via Adversarial Information Maximization\",\"Entropy and mutual information in models of deep neural networks\",\"Collaborative Learning for Deep Neural Networks\",\"High Dimensional Linear Regression using Lattice Basis Reduction\",\"Symbolic Graph Reasoning Meets Convolutions\",\"DVAE#: Discrete Variational Autoencoders with Relaxed Boltzmann Priors\",\"Partially-Supervised Image Captioning\",\"3D-Aware Scene Manipulation via Inverse Graphics\",\"Random Feature Stein Discrepancies\",\"Distributed Stochastic Optimization via Adaptive SGD\",\"Precision and Recall for Time Series\",\"Deep Attentive Tracking via Reciprocative Learning\",\"Virtual Class Enhanced Discriminative Embedding Learning\",\"Attention in Convolutional LSTM for Gesture Recognition\",\"Pelee: A Real-Time Object Detection System on Mobile Devices\",\"Universal Growth in Production Economies\",\"Bayesian Model Selection Approach to Boundary Detection with Non-Local Priors\",\"Efficient Stochastic Gradient Hard Thresholding\",\"SplineNets: Continuous Neural Decision Graphs\",\"Generalized Zero-Shot Learning with Deep Calibration Network\",\"Neural Architecture Search with Bayesian Optimisation and Optimal Transport\",\"Embedding Logical Queries on Knowledge Graphs\",\"Learning Optimal Reserve Price against Non-myopic Bidders\",\"Sequential Context Encoding for Duplicate Removal\",\"Discovery of Latent 3D Keypoints via End-to-end Geometric Reasoning\",\"Nonparametric learning from Bayesian models with randomized objective functions\",\"SEGA: Variance Reduction via Gradient Sketching\",\"Automatic Program Synthesis of Long Programs with a Learned Garbage Collector\",\"One-Shot Unsupervised Cross Domain Translation\",\"Regularizing by the Variance of the Activations' Sample-Variances\",\"Overlapping Clustering Models, and One (class) SVM to Bind Them All\",\"Algorithmic Linearly Constrained Gaussian Processes\",\"DeepExposure: Learning to Expose Photos with Asynchronously Reinforced Adversarial Learning\",\"Norm matters: efficient and accurate normalization schemes in deep networks\",\"Dual Principal Component Pursuit: Improved Analysis and Efficient Algorithms\",\"MULAN: A Blind and Off-Grid Method for Multichannel Echo Retrieval\",\"Mixture Matrix Completion\",\"Trajectory Convolution for Action Recognition\",\"The Description Length of Deep Learning models\",\"A Smoothed Analysis of the Greedy Algorithm for the Linear Contextual Bandit Problem\",\"Revisiting Decomposable Submodular Function Minimization with Incidence Relations\",\"A Practical Algorithm for Distributed Clustering and Outlier Detection\",\"Learning to Reconstruct Shapes from Unseen Classes\",\"BourGAN: Generative Networks with Metric Embeddings\",\"Smoothed analysis of the low-rank approach for smooth semidefinite programs\",\"Zero-Shot Transfer with Deictic Object-Oriented Representation in Reinforcement Learning\",\"Overfitting or perfect fitting? Risk bounds for classification and regression rules that interpolate\",\"Breaking the Span Assumption Yields Fast Finite-Sum Minimization\",\"Structured Local Minima in Sparse Blind Deconvolution\",\"GIANT: Globally Improved Approximate Newton Method for Distributed Optimization\",\"Modelling sparsity, heterogeneity, reciprocity and community structure in temporal interaction data\",\"Non-monotone Submodular Maximization in Exponentially Fewer Iterations\",\"MetaGAN: An Adversarial Approach to Few-Shot Learning\",\"Local Differential Privacy for Evolving Data\",\"Gaussian Process Conditional Density Estimation\",\"Meta-Gradient Reinforcement Learning\",\"Modular Networks: Learning to Decompose Neural Computation\",\"Learning to Navigate in Cities Without a Map\",\"Query Complexity of Bayesian Private Learning\",\"A theory on the absence of spurious solutions for nonconvex and nonsmooth optimization\",\"Recurrent World Models Facilitate Policy Evolution\",\"Ridge Regression and Provable Deterministic Ridge Leverage Score Sampling\",\"Wasserstein Variational Inference\",\"How Does Batch Normalization Help Optimization?\",\"Verifiable Reinforcement Learning via Policy Extraction\",\"Leveraged volume sampling for linear regression\",\"Model Agnostic Supervised Local Explanations\",\"A Linear Speedup Analysis of Distributed Deep Learning with Sparse and Quantized Communication\",\"Active Learning for Non-Parametric Regression Using Purely Random Trees\",\"Tree-to-tree Neural Networks for Program Translation\",\"Batch-Instance Normalization for Adaptively Style-Invariant Neural Networks\",\"Structural Causal Bandits: Where to Intervene?\",\"Answerer in Questioner's Mind: Information Theoretic Approach to Goal-Oriented Visual Dialog\",\"A Unified Feature Disentangler for Multi-Domain Image Translation and Manipulation\",\"Online Learning with an Unknown Fairness Metric\",\"Isolating Sources of Disentanglement in Variational Autoencoders\",\"Contextual bandits with surrogate losses: Margin bounds and efficient algorithms\",\"Representation Learning for Treatment Effect Estimation from Observational Data\",\"Representation Balancing MDPs for Off-policy Policy Evaluation\",\"Out of the Box: Reasoning with Graph Convolution Nets for Factual Visual Question Answering\",\"Causal Discovery from Discrete Data using Hidden Compact Representation\",\"Natasha 2: Faster Non-Convex Optimization Than SGD\",\"Minimax Statistical Learning with Wasserstein distances\",\"Provable Variational Inference for Constrained Log-Submodular Models\",\"Learning Hierarchical Semantic Image Manipulation through Structured Representations\",\"Processing of missing data by neural networks\",\"Safe Active Learning for Time-Series Modeling with Gaussian Processes\",\"Optimal Algorithms for Non-Smooth Distributed Optimization in Networks\",\"Computing Higher Order Derivatives of Matrix and Tensor Expressions\",\"Paraphrasing Complex Network: Network Compression via Factor Transfer\",\"Analytic solution and stationary phase approximation for the Bayesian lasso and elastic net\",\"Demystifying excessively volatile human learning: A Bayesian persistent prior and a neural approximation\",\"Empirical Risk Minimization Under Fairness Constraints\",\"Unsupervised Learning of Shape and Pose with Differentiable Point Clouds\",\"Continuous-time Value Function Approximation in Reproducing Kernel Hilbert Spaces\",\"Gradient Descent Meets Shift-and-Invert Preconditioning for Eigenvector Computation\",\"Factored Bandits\",\"Delta-encoder: an effective sample synthesis method for few-shot object recognition\",\"Metric on Nonlinear Dynamical Systems with Perron-Frobenius Operators\",\"Learning a High Fidelity Pose Invariant Model for High-resolution Face Frontalization\",\"Mirrored Langevin Dynamics\",\"Moonshine: Distilling with Cheap Convolutions\",\"Stochastic Cubic Regularization for Fast Nonconvex Optimization\",\"Adaptation to Easy Data in Prediction with Limited Advice\",\"Differentially Private Bayesian Inference for Exponential Families\",\"Playing hard exploration games by watching YouTube\",\"Dialog-to-Action: Conversational Question Answering Over a Large-Scale Knowledge Base\",\"Norm-Ranging LSH for Maximum Inner Product Search\",\"Optimization over Continuous and Multi-dimensional Decisions with Observational Data\",\"Fast Estimation of Causal Interactions using Wold Processes\",\"When do random forests fail?\",\"Near Optimal Exploration-Exploitation in Non-Communicating Markov Decision Processes\",\"Optimistic optimization of a Brownian\",\"Practical Methods for Graph Two-Sample Testing\",\"NAIS-Net: Stable Deep Networks from Non-Autonomous Differential Equations\",\"On the Global Convergence of Gradient Descent for Over-parameterized Models using Optimal Transport\",\"Constructing Deep Neural Networks by Bayesian Network Structure Learning\",\"Weakly Supervised Dense Event Captioning in Videos\",\"Faithful Inversion of Generative Models for Effective Amortized Inference\",\"From Stochastic Planning to Marginal MAP\",\"On Binary Classification in Extreme Regions\",\"Near-Optimal Policies for Dynamic Multinomial Logit Assortment Selection Models\",\"Q-learning with Nearest Neighbors\",\"Global Convergence of Langevin Dynamics Based Algorithms for Nonconvex Optimization\",\"Asymptotic optimality of adaptive importance sampling\",\"Learning latent variable structured prediction models with Gaussian perturbations\",\"The Nearest Neighbor Information Estimator is Adaptively Near Minimax Rate-Optimal\",\"Deep Reinforcement Learning of Marked Temporal Point Processes\",\"Evidential Deep Learning to Quantify Classification Uncertainty\",\"Parsimonious Bayesian deep networks\",\"Single-Agent Policy Tree Search With Guarantees\",\"Semi-crowdsourced Clustering with Deep Generative Models\",\"The committee machine: Computational to statistical gaps in learning a two-layers neural network\",\"Realistic Evaluation of Deep Semi-Supervised Learning Algorithms\",\"Contextual Combinatorial Multi-armed Bandits with Volatile Arms and Submodular Reward\",\"Training deep learning based denoisers without ground truth data\",\"Re-evaluating evaluation\",\"Deep, complex, invertible networks for inversion of transmission effects in multimode optical fibres\",\"Multivariate Convolutional Sparse Coding for Electromagnetic Brain Signals\",\"Data-Efficient Hierarchical Reinforcement Learning\",\"Speaker-Follower Models for Vision-and-Language Navigation\",\"Inequity aversion improves cooperation in intertemporal social dilemmas\",\"Learning Gaussian Processes by Minimizing PAC-Bayesian Generalization Bounds\",\"Probabilistic Matrix Factorization for Automated Machine Learning\",\"Stochastic Spectral and Conjugate Descent Methods\",\"Recurrent Relational Networks\",\"But How Does It Work in Theory? Linear SVM with Random Features\",\"Learning to Optimize Tensor Programs\",\"Boosting Black Box Variational Inference\",\"Nearly tight sample complexity bounds for learning mixtures of Gaussians via sample compression schemes\",\"Actor-Critic Policy Optimization in Partially Observable Multiagent Environments\",\"Step Size Matters in Deep Learning\",\"Derivative Estimation in Random Design\",\"Zeroth-order (Non)-Convex Stochastic Optimization via Conditional Gradient and Gradient Updates\",\"Latent Gaussian Activity Propagation: Using Smoothness and Structure to Separate and Localize Sounds in Large Noisy Environments\",\"Hybrid-MST: A Hybrid Active Sampling Strategy for Pairwise Preference Aggregation\",\"Infinite-Horizon Gaussian Processes\",\"Dimensionality Reduction for Stationary Time Series via Stochastic Nonconvex Optimization\",\"Sequence-to-Segment Networks for Segment Detection\",\"Scaling the Poisson GLM to massive neural datasets through polynomial approximations\",\"Multiplicative Weights Updates with Constant Step-Size in Graphical Constant-Sum Games\",\"Why Is My Classifier Discriminatory?\",\"Multi-Layered Gradient Boosting Decision Trees\",\"Learn What Not to Learn: Action Elimination with Deep Reinforcement Learning\",\"Communication Efficient Parallel Algorithms for Optimization on Manifolds\",\"Neural Code Comprehension: A Learnable Representation of Code Semantics\",\"Tight Bounds for Collaborative PAC Learning via Multiplicative Weights\",\"BinGAN: Learning Compact Binary Descriptors with a Regularized GAN\",\"Modern Neural Networks Generalize on Small Data Sets\",\"Escaping Saddle Points in Constrained Optimization\",\"Adversarial Attacks on Stochastic Bandits\",\"Optimal Subsampling with Influence Functions\",\"A Bandit Approach to Sequential Experimental Design with False Discovery Control\",\"Equality of Opportunity in Classification: A Causal Approach\",\"Towards Understanding Acceleration Tradeoff between Momentum and Asynchrony in Nonconvex Stochastic Optimization\",\"Unsupervised Attention-guided Image-to-Image Translation\",\"Inferring Networks From Random Walk-Based Node Similarities\",\"NEON2: Finding Local Minima via First-Order Oracles\",\"Zeroth-Order Stochastic Variance Reduction for Nonconvex Optimization\",\"Online Structured Laplace Approximations for Overcoming Catastrophic Forgetting\",\"DeepProbLog: Neural Probabilistic Logic Programming\",\"Convergence of Cubic Regularization for Nonconvex Optimization under KL Property\",\"Direct Estimation of Differences in Causal Graphs\",\"Sublinear Time Low-Rank Approximation of Distance Matrices\",\"Variational PDEs for Acceleration on Manifolds and Application to Diffeomorphisms\",\"Bayesian Inference of Temporal Task Specifications from Demonstrations\",\"Data center cooling using model-predictive control\",\"Acceleration through Optimistic No-Regret Dynamics\",\"Lipschitz regularity of deep neural networks: analysis and efficient estimation\",\"Minimax Estimation of Neural Net Distance\",\"Leveraging the Exact Likelihood of Deep Latent Variable Models\",\"Bipartite Stochastic Block Models with Tiny Clusters\",\"Learning sparse neural networks via sensitivity-driven regularization\",\"Faster Online Learning of Optimal Threshold for Consistent F-measure Optimization\",\"Direct Runge-Kutta Discretization Achieves Acceleration\",\"Adversarial Examples that Fool both Computer Vision and Time-Limited Humans\",\"Stochastic Nested Variance Reduction for Nonconvex Optimization\",\"Faster Neural Networks Straight from JPEG\",\"TopRank: A practical algorithm for online stochastic ranking\",\"Learning from discriminative feature feedback\",\"RetGK: Graph Kernels based on Return Probabilities of Random Walks\",\"Deep Generative Markov State Models\",\"Early Stopping for Nonparametric Testing\",\"Solving Non-smooth Constrained Programs with Lower Complexity than \\\\mathcal{O}(1/\\\\varepsilon): A Primal-Dual Homotopy Smoothing Approach\",\"Heterogeneous Bitwidth Binarization in Convolutional Neural Networks\",\"Unsupervised Learning of Object Landmarks through Conditional Image Generation\",\"Probabilistic Neural Programmed Networks for Scene Generation\",\"The streaming rollout of deep networks - towards fully model-parallel execution\",\"KONG: Kernels for ordered-neighborhood graphs\",\"GumBolt: Extending Gumbel trick to Boltzmann priors\",\"Neural Networks Trained to Solve Differential Equations Learn General Representations\",\"Beauty-in-averageness and its contextual modulations: A Bayesian statistical account\",\"Distributed Weight Consolidation: A Brain Segmentation Case Study\",\"Efficient Projection onto the Perfect Phylogeny Model\",\"TETRIS: TilE-matching the TRemendous Irregular Sparsity\",\"Cooperative neural networks (CoNN): Exploiting prior independence structure for improved classification\",\"Differentially Private Robust Low-Rank Approximation\",\"Meta-Learning MCMC Proposals\",\"An Information-Theoretic Analysis for Thompson Sampling with Many Actions\",\"Flexible and accurate inference and learning for deep generative models\",\"The Price of Privacy for Low-rank Factorization\",\"Regret Bounds for Robust Adaptive Control of the Linear Quadratic Regulator\",\"Bilevel Distance Metric Learning for Robust Image Recognition\",\"Differentially Private Uniformly Most Powerful Tests for Binomial Data\",\"Scalable Coordinated Exploration in Concurrent Reinforcement Learning\",\"Integrated accounts of behavioral and neuroimaging data using flexible recurrent neural network models\",\"BML: A High-performance, Low-cost Gradient Synchronization Algorithm for DML Training\",\"Inexact trust-region algorithms on Riemannian manifolds\",\"Can We Gain More from Orthogonality Regularizations in Training Deep Networks?\",\"Binary Rating Estimation with Graph Side Information\",\"SimplE Embedding for Link Prediction in Knowledge Graphs\",\"Differentially Private Contextual Linear Bandits\",\"Submodular Field Grammars: Representation, Inference, and Application to Image Parsing\",\"A Bridging Framework for Model Optimization and Deep Propagation\",\"Completing State Representations using Spectral Learning\",\"Optimization of Smooth Functions with Noisy Observations: Local Minimax Rates\",\"Adding One Neuron Can Eliminate All Bad Local Minima\",\"Mean-field theory of graph neural networks in graph partitioning\",\"The Physical Systems Behind Optimization Algorithms\",\"Mallows Models for Top-k Lists\",\"Amortized Inference Regularization\",\"Maximum Causal Tsallis Entropy Imitation Learning\",\"Limited Memory Kelley's Method Converges for Composite Convex and Submodular Objectives\",\"Semi-Supervised Learning with Declaratively Specified Entropy Constraints\",\"End-to-end Symmetry Preserving Inter-atomic Potential Energy Model for Finite and Extended Systems\",\"Sparsified SGD with Memory\",\"Exponentiated Strongly Rayleigh Distributions\",\"Importance Weighting and Variational Inference\",\"Transfer Learning from Speaker Verification to Multispeaker Text-To-Speech Synthesis\",\"Expanding Holographic Embeddings for Knowledge Completion\",\"Lifelong Inverse Reinforcement Learning\",\"Explaining Deep Learning Models -- A Bayesian Non-parametric Approach\",\"Third-order Smoothness Helps: Faster Stochastic Optimization Algorithms for Finding Local Minima\",\"COLA: Decentralized Linear Learning\",\"MiME: Multilevel Medical Embedding of Electronic Health Records for Predictive Healthcare\",\"Adaptive Sampling Towards Fast Graph Representation Learning\",\"Hunting for Discriminatory Proxies in Linear Regression Models\",\"Towards Robust Detection of Adversarial Examples\",\"Active Matting\",\"Learning filter widths of spectral decompositions with wavelets\",\"Byzantine Stochastic Gradient Descent\",\"PG-TS: Improved Thompson Sampling for Logistic Contextual Bandits\",\"Spectral Filtering for General Linear Dynamical Systems\",\"On Learning Intrinsic Rewards for Policy Gradient Methods\",\"Boolean Decision Rules via Column Generation\",\"Adversarial Text Generation via Feature-Mover's Distance\",\"Fast Rates of ERM and Stochastic Approximation: Adaptive to Error Bound Conditions\",\"Learning Bounds for Greedy Approximation with Explicit Feature Maps from Multiple Kernels\",\"A Mathematical Model For Optimal Decisions In A Representative Democracy\",\"Negotiable Reinforcement Learning for Pareto Optimal Sequential Decision-Making\",\"Non-metric Similarity Graphs for Maximum Inner Product Search\",\"Recurrently Controlled Recurrent Networks\",\"Fast greedy algorithms for dictionary selection with generalized sparsity constraints\",\"Deep Reinforcement Learning in a Handful of Trials using Probabilistic Dynamics Models\",\"A Smoother Way to Train Structured Prediction Models\",\"Context-dependent upper-confidence bounds for directed exploration\",\"A Unified View of Piecewise Linear Neural Network Verification\",\"Hierarchical Graph Representation Learning with Differentiable Pooling\",\"Non-Ergodic Alternating Proximal Augmented Lagrangian Algorithms with Optimal Rates\",\"Information-based Adaptive Stimulus Selection to Optimize Communication Efficiency in Brain-Computer Interfaces\",\"Porcupine Neural Networks: Approximating Neural Network Landscapes\",\"Fairness Through Computationally-Bounded Awareness\",\"Adaptive Negative Curvature Descent with Applications in Non-convex Optimization\",\"Is Q-Learning Provably Efficient?\",\"Interpreting Neural Network Judgments via Minimal, Stable, and Symbolic Corrections\",\"Measures of distortion for machine learning\",\"On the Local Minima of the Empirical Risk\",\"Densely Connected Attention Propagation for Reading Comprehension\",\"Bandit Learning with Positive Externalities\",\"Learning Confidence Sets using Support Vector Machines\",\"Efficient Neural Network Robustness Certification with General Activation Functions\",\"Hessian-based Analysis of Large Batch Training and Robustness to Adversaries\",\"Neural Edit Operations for Biological Sequences\",\"Objective and efficient inference for couplings in neuronal networks\",\"Learning from Group Comparisons: Exploiting Higher Order Interactions\",\"Supervising Unsupervised Learning\",\"Nonparametric Bayesian Lomax delegate racing for survival analysis with competing risks\",\"Adversarially Robust Generalization Requires More Data\",\"Improving Exploration in Evolution Strategies for Deep Reinforcement Learning via a Population of Novelty-Seeking Agents\",\"Practical exact algorithm for trembling-hand equilibrium refinements in games\",\"LAG: Lazily Aggregated Gradient for Communication-Efficient Distributed Learning\",\"Scalable Robust Matrix Factorization with Nonconvex Loss\",\"Power-law efficient neural codes provide general link between perceptual bias and discriminability\",\"Geometry-Aware Recurrent Neural Networks for Active Visual Recognition\",\"Unsupervised Adversarial Invariance\",\"Content preserving text generation with attribute controls\",\"Multi-armed Bandits with Compensation\",\"GradiVeQ: Vector Quantization for Bandwidth-Efficient Gradient Aggregation in Distributed CNN Training\",\"Learning in Games with Lossy Feedback\",\"Scalable methods for 8-bit training of neural networks\",\"Dropping Symmetry for Fast Symmetric Nonnegative Matrix Factorization\",\"Link Prediction Based on Graph Neural Networks\",\"Why so gloomy? A Bayesian explanation of human pessimism bias in the multi-armed bandit task\",\"Near-Optimal Time and Sample Complexities for Solving Markov Decision Processes with a Generative Model\",\"ChannelNets: Compact and Efficient Convolutional Neural Networks via Channel-Wise Convolutions\",\"Causal Inference and Mechanism Clustering of A Mixture of Additive Noise Models\",\"Contour location via entropy reduction leveraging multiple information sources\",\"Assessing Generative Models via Precision and Recall\",\"Multiple-Step Greedy Policies in Approximate and Online Reinforcement Learning\",\"A Convex Duality Framework for GANs\",\"Horizon-Independent Minimax Linear Regression\",\"Exploiting Numerical Sparsity for Efficient Learning : Faster Eigenvector Computation and Regression\",\"Experimental Design for Cost-Aware Learning of Causal Graphs\",\"Task-Driven Convolutional Recurrent Models of the Visual System\",\"Meta-Reinforcement Learning of Structured Exploration Strategies\",\"Sample Efficient Stochastic Gradient Iterative Hard Thresholding Method for Stochastic Sparse Linear Regression with Limited Attribute Observation\",\"Semi-supervised Deep Kernel Learning: Regression with Unlabeled Data by Minimizing Predictive Variance\",\"Generalizing to Unseen Domains via Adversarial Data Augmentation\",\"Hyperbolic Neural Networks\",\"Breaking the Curse of Horizon: Infinite-Horizon Off-Policy Estimation\",\"Learning Task Specifications from Demonstrations\",\"Learning a latent manifold of odor representations from neural responses in piriform cortex\",\"Fully Understanding The Hashing Trick\",\"Evolved Policy Gradients\",\"The Spectrum of the Fisher Information Matrix of a Single-Hidden-Layer Neural Network\",\"Learning Concave Conditional Likelihood Models for Improved Analysis of Tandem Mass Spectra\",\"Differentially Private k-Means with Constant Multiplicative Error\",\"Policy Optimization via Importance Sampling\",\"Estimating Learnability in the Sublinear Data Regime\",\"Algorithmic Assurance: An Active Approach to Algorithmic Testing using Bayesian Optimisation\",\"Community Exploration: From Offline Optimization to Online Learning\",\"A Dual Framework for Low-rank Tensor Completion\",\"Low-rank Interaction with Sparse Additive Effects Model for Large Data Frames\",\"Inference Aided Reinforcement Learning for Incentive Mechanism Design in Crowdsourcing\",\"Middle-Out Decoding\",\"First-order Stochastic Algorithms for Escaping From Saddle Points in Almost Linear Time\",\"To Trust Or Not To Trust A Classifier\",\"Reparameterization Gradient for Non-differentiable Models\",\"A Simple Proximal Stochastic Gradient Method for Nonsmooth Nonconvex Optimization\",\"Multimodal Generative Models for Scalable Weakly-Supervised Learning\",\"How Much Restricted Isometry is Needed In Nonconvex Matrix Recovery?\",\"Occam's razor is insufficient to infer the preferences of irrational agents\",\"Manifold Structured Prediction\",\"Fast Greedy MAP Inference for Determinantal Point Process to Improve Recommendation Diversity\",\"Learning Others' Intentional Models in Multi-Agent Settings Using Interactive POMDPs\",\"Contextual Pricing for Lipschitz Buyers\",\"Online Improper Learning with an Approximation Oracle\",\"Bandit Learning in Concave N-Person Games\",\"On Fast Leverage Score Sampling and Optimal Learning\",\"Unsupervised Video Object Segmentation for Deep Reinforcement Learning\",\"Efficient inference for time-varying behavior during learning\",\"Learning convex polytopes with margin\",\"Critical initialisation for deep signal propagation in noisy rectifier neural networks\",\"Insights on representational similarity in neural networks with canonical correlation\",\"Variational Inference with Tail-adaptive f-Divergence\",\"Mental Sampling in Multimodal Representations\",\"Adversarially Robust Optimization with Gaussian Processes\",\"Learning to Multitask\",\"Loss Functions for Multiset Prediction\",\"Computing Kantorovich-Wasserstein Distances on d-dimensional histograms using (d+1)-partite graphs\",\"Neural Interaction Transparency (NIT): Disentangling Learned Interactions for Improved Interpretability\",\"CapProNet: Deep Feature Learning via Orthogonal Projections onto Capsule Subspaces\",\"Gamma-Poisson Dynamic Matrix Factorization Embedded with Metadata Influence\",\"Masking: A New Perspective of Noisy Supervision\",\"On GANs and GMMs\",\"Differential Properties of Sinkhorn Approximation for Learning with Wasserstein Distance\",\"Practical Deep Stereo (PDS): Toward applications-friendly deep stereo matching\",\"A Bayes-Sard Cubature Method\",\"Dual Swap Disentangling\",\"Diverse Ensemble Evolution: Curriculum Data-Model Marriage\",\"Binary Classification from Positive-Confidence Data\",\"Deep Generative Models for Distribution-Preserving Lossy Compression\",\"Exact natural gradient in deep linear networks and its application to the nonlinear case\",\"Constructing Fast Network through Deconstruction of Convolution\",\"Memory Replay GANs: Learning to Generate New Categories without Forgetting\",\"The Convergence of Sparsified Gradient Methods\",\"Automating Bayesian optimization with Bayesian optimization\",\"Stacked Semantics-Guided Attention Model for Fine-Grained Zero-Shot Learning\",\"Dirichlet-based Gaussian Processes for Large-scale Calibrated Classification\",\"Multi-Task Zipping via Layer-wise Neuron Sharing\",\"Dimensionally Tight Bounds for Second-Order Hamiltonian Monte Carlo\",\"Approximation algorithms for stochastic clustering\",\"Evolutionary Stochastic Gradient Descent for Optimization of Deep Neural Networks\",\"Learning to Infer Graphics Programs from Hand-Drawn Images\",\"Graphical Generative Adversarial Networks\",\"Variational Learning on Aggregate Outputs with Gaussian Processes\",\"MacNet: Transferring Knowledge from Machine Comprehension to Sequence-to-Sequence Models\",\"Poison Frogs! Targeted Clean-Label Poisoning Attacks on Neural Networks\",\"Information Constraints on Auto-Encoding Variational Bayes\",\"Recurrent Transformer Networks for Semantic Correspondence\",\"Online convex optimization for cumulative constraints\",\"Predict Responsibly: Improving Fairness and Accuracy by Learning to Defer\",\"Deep State Space Models for Unconditional Word Generation\",\"ResNet with one-neuron hidden layers is a Universal Approximator\",\"Transfer of Value Functions via Variational Methods\",\"The Cluster Description Problem - Complexity Results, Formulations and Approximations\",\"Sharp Bounds for Generalized Uniformity Testing\",\"Deep Neural Networks with Box Convolutions\",\"Learning towards Minimum Hyperspherical Energy\",\"LF-Net: Learning Local Features from Images\",\"SLANG: Fast Structured Covariance Approximations for Bayesian Deep Learning with Natural Gradient\",\"Tangent: Automatic differentiation using source-code transformation for dynamically typed array programming\",\"Multi-domain Causal Structure Learning in Linear Systems\",\"Privacy Amplification by Subsampling: Tight Analyses via Couplings and Divergences\",\"Exponentially Weighted Imitation Learning for Batched Historical Data\",\"Algebraic tests of general Gaussian latent tree models\",\"Navigating with Graph Representations for Fast and Scalable Decoding of Neural Language Models\",\"Deep Structured Prediction with Nonlinear Output Transformations\",\"Sequential Test for the Lowest Mean: From Thompson to Murphy Sampling\",\"Distributed Learning without Distress: Privacy-Preserving Empirical Risk Minimization\",\"A no-regret generalization of hierarchical softmax to extreme multi-label classification\",\"Efficient Formal Safety Analysis of Neural Networks\",\"Bayesian Distributed Stochastic Gradient Descent\",\"Visualizing the Loss Landscape of Neural Nets\",\"The Limits of Post-Selection Generalization\",\"Graph Convolutional Policy Network for Goal-Directed Molecular Graph Generation\",\"On Controllable Sparse Alternatives to Softmax\",\"L4: Practical loss-based stepsize adaptation for deep learning\",\"Learning Latent Subspaces in Variational Autoencoders\",\"Turbo Learning for CaptionBot and DrawingBot\",\"Learning to Teach with Dynamic Loss Functions\",\"Multi-View Silhouette and Depth Decomposition for High Resolution 3D Object Representation\",\"Size-Noise Tradeoffs in Generative Networks\",\"Online Adaptive Methods, Universality and Acceleration\",\"Compact Generalized Non-local Network\",\"On the Local Hessian in Back-propagation\",\"The Everlasting Database: Statistical Validity at a Fair Price\",\"Lipschitz-Margin Training: Scalable Certification of Perturbation Invariance for Deep Neural Networks\",\"Proximal SCOPE for Distributed Sparse Learning\",\"On Coresets for Logistic Regression\",\"Neural Ordinary Differential Equations\",\"Unsupervised Learning of Artistic Styles with Archetypal Style Analysis\",\"Approximating Real-Time Recurrent Learning with Random Kronecker Factors\",\"Contamination Attacks and Mitigation in Multi-Party Machine Learning\",\"An Improved Analysis of Alternating Minimization for Structured Multi-Response Regression\",\"Incorporating Context into Language Encoding Models for fMRI\",\"CatBoost: unbiased boosting with categorical features\",\"Query K-means Clustering and the Double Dixie Cup Problem\",\"Training Neural Networks Using Features Replay\",\"Modeling Dynamic Missingness of Implicit Feedback for Recommendation\",\"Representation Learning of Compositional Data\",\"Model-based targeted dimensionality reduction for neuronal population data\",\"On gradient regularizers for MMD GANs\",\"Heterogeneous Multi-output Gaussian Process Prediction\",\"Large-Scale Stochastic Sampling from the Probability Simplex\",\"Policy Regret in Repeated Games\",\"A Theory-Based Evaluation of Nearest Neighbor Models Put Into Practice\",\"Banach Wasserstein GAN\",\"Provable Gaussian Embedding with One Observation\",\"BRITS: Bidirectional Recurrent Imputation for Time Series\",\"M-Walk: Learning to Walk over Graphs using Monte Carlo Tree Search\",\"Extracting Relationships by Multi-Domain Matching\",\"Efficient Gradient Computation for Structured Output Learning with Rational and Tropical Losses\",\"Generative Probabilistic Novelty Detection with Adversarial Autoencoders\",\"Diminishing Returns Shape Constraints for Interpretability and Regularization\",\"Scalable Hyperparameter Transfer Learning\",\"Stochastic Nonparametric Event-Tensor Decomposition\",\"Scaling Gaussian Process Regression with Derivatives\",\"Differentially Private Testing of Identity and Closeness of Discrete Distributions\",\"Bayesian Adversarial Learning\",\"Efficient Convex Completion of Coupled Tensors using Coupled Nuclear Norms\",\"Maximizing Induced Cardinality Under a Determinantal Point Process\",\"Causal Inference with Noisy and Missing Covariates via Matrix Factorization\",\"rho-POMDPs have Lipschitz-Continuous epsilon-Optimal Value Functions\",\"Online Structure Learning for Feed-Forward and Recurrent Sum-Product Networks\",\"Uncertainty Sampling is Preconditioned Stochastic Gradient Descent on Zero-One Loss\",\"A Probabilistic U-Net for Segmentation of Ambiguous Images\",\"Unorganized Malicious Attacks Detection\",\"Causal Inference via Kernel Deviance Measures\",\"Bayesian Alignments of Warped Multi-Output Gaussian Processes\",\"Hybrid Macro/Micro Level Backpropagation for Training Deep Spiking Neural Networks\",\"Gen-Oja: Simple & Efficient Algorithm for Streaming Generalized Eigenvector Computation\",\"Efficient online algorithms for fast-rate regret bounds under sparsity\",\"GILBO: One Metric to Measure Them All\",\"Predictive Uncertainty Estimation via Prior Networks\",\"Dual Policy Iteration\",\"A probabilistic population code based on neural samples\",\"Manifold-tiling Localized Receptive Fields are Optimal in Similarity-preserving Neural Networks\",\"On the Convergence and Robustness of Training GANs with Regularized Optimal Transport\",\"Model-Agnostic Private Learning\",\"Constrained Generation of Semantically Valid Graphs via Regularizing Variational Autoencoders\",\"Provably Correct Automatic Sub-Differentiation for Qualified Programs\",\"Deep Homogeneous Mixture Models: Representation, Separation, and Approximation\",\"Parameters as interacting particles: long time convergence and asymptotic error scaling of neural networks\",\"Hierarchical Reinforcement Learning for Zero-shot Generalization with Subtask Dependencies\",\"A Simple Unified Framework for Detecting Out-of-Distribution Samples and Adversarial Attacks\",\"End-to-End Differentiable Physics for Learning and Control\",\"BRUNO: A Deep Recurrent Model for Exchangeable Data\",\"Stimulus domain transfer in recurrent models for large scale cortical population prediction on video\",\"Mapping Images to Scene Graphs with Permutation-Invariant Structured Prediction\",\"Distributed Multi-Player Bandits - a Game of Thrones Approach\",\"Efficient Loss-Based Decoding on Graphs for Extreme Classification\",\"Chaining Mutual Information and Tightening Generalization Bounds\",\"Implicit Probabilistic Integrators for ODEs\",\"Learning Attentional Communication for Multi-Agent Cooperation\",\"Training Deep Models Faster with Robust, Approximate Importance Sampling\",\"Bandit Learning with Implicit Feedback\",\"Unsupervised Text Style Transfer using Language Models as Discriminators\",\"Relational recurrent neural networks\",\"Streaming Kernel PCA with \\\\tilde{O}(\\\\sqrt{n}) Random Features\",\"REFUEL: Exploring Sparse Features in Deep Reinforcement Learning for Fast Disease Diagnosis\",\"Bayesian Model-Agnostic Meta-Learning\",\"Disconnected Manifold Learning for Generative Adversarial Networks\",\"Unsupervised Cross-Modal Alignment of Speech and Text Embedding Spaces\",\"Learning Signed Determinantal Point Processes through the Principal Minor Assignment Problem\",\"Out-of-Distribution Detection using Multiple Semantic Label Representations\",\"Stochastic Chebyshev Gradient Descent for Spectral Optimization\",\"Revisiting (\\\\epsilon, \\\\gamma, \\\\tau)-similarity learning for domain adaptation\",\"How to tell when a clustering is (approximately) correct using convex relaxations\",\"Constant Regret, Generalized Mixability, and Mirror Descent\",\"A Bayesian Approach to Generative Adversarial Imitation Learning\",\"Plug-in Estimation in High-Dimensional Linear Inverse Problems: A Rigorous Analysis\",\"Constrained Cross-Entropy Method for Safe Reinforcement Learning\",\"Multi-Agent Generative Adversarial Imitation Learning\",\"Adaptive Learning with Unknown Information Flows\",\"Forecasting Treatment Responses Over Time Using Recurrent Marginal Structural Networks\",\"Generative modeling for protein structures\",\"Inference in Deep Gaussian Processes using Stochastic Gradient Hamiltonian Monte Carlo\",\"Knowledge Distillation by On-the-Fly Native Ensemble\",\"Non-Adversarial Mapping with VAEs\",\"Generalisation in humans and deep neural networks\",\"Towards Text Generation with Adversarially Learned Neural Outlines\",\"cpSGD: Communication-efficient and differentially-private distributed SGD\",\"GPyTorch: Blackbox Matrix-Matrix Gaussian Process Inference with GPU Acceleration\",\"Diffusion Maps for Textual Network Embedding\",\"Simple, Distributed, and Accelerated Probabilistic Programming\",\"VideoCapsuleNet: A Simplified Network for Action Detection\",\"Rectangular Bounding Process\",\"Improved Algorithms for Collaborative PAC Learning\",\"Sparse Attentive Backtracking: Temporal Credit Assignment Through Reminding\",\"Communication Compression for Decentralized Training\",\"Depth-Limited Solving for Imperfect-Information Games\",\"Training Deep Neural Networks with 8-bit Floating Point Numbers\",\"Scalar Posterior Sampling with Applications\",\"Understanding Batch Normalization\",\"Adversarial Scene Editing: Automatic Object Removal from Weak Supervision\",\"Attacks Meet Interpretability: Attribute-steered Detection of Adversarial Samples\",\"On Neuronal Capacity\",\"Breaking the Activation Function Bottleneck through Adaptive Parameterization\",\"Learning Loop Invariants for Program Verification\",\"Cooperative Learning of Audio and Video Models from Self-Supervised Synchronization\",\"Towards Robust Interpretability with Self-Explaining Neural Networks\",\"Deep State Space Models for Time Series Forecasting\",\"Constrained Graph Variational Autoencoders for Molecule Design\",\"Learning Libraries of Subroutines for Neurally\\u2013Guided Bayesian Program Induction\",\"Neural Architecture Optimization\",\"Preference Based Adaptation for Learning Objectives\",\"Distributed k-Clustering for Data with Heavy Noise\",\"Beyond Log-concavity: Provable Guarantees for Sampling Multi-modal Distributions using Simulated Tempering Langevin Monte Carlo\",\"A General Method for Amortizing Variational Filtering\",\"A Reduction for Efficient LDA Topic Reconstruction\",\"Cluster Variational Approximations for Structure Learning of Continuous-Time Bayesian Networks from Incomplete Data\",\"RenderNet: A deep convolutional network for differentiable rendering from 3D shapes\",\"Robust Hypothesis Testing Using Wasserstein Uncertainty Sets\",\"Robust Detection of Adversarial Attacks by Modeling the Intrinsic Properties of Deep Neural Networks\",\"Monte-Carlo Tree Search for Constrained POMDPs\",\"Learning to Repair Software Vulnerabilities with Generative Adversarial Networks\",\"Layer-Wise Coordination between Encoder and Decoder for Neural Machine Translation\",\"Dirichlet belief networks for topic structure learning\",\"Stochastic Expectation Maximization with Variance Reduction\",\"Submodular Maximization via Gradient Ascent: The Case of Deep Submodular Functions\",\"The challenge of realistic music generation: modelling raw audio at scale\",\"Spectral Signatures in Backdoor Attacks\",\"Reward learning from human preferences and demonstrations in Atari\",\"Approximate Knowledge Compilation by Online Collapsed Importance Sampling\",\"Neural Arithmetic Logic Units\",\"Pipe-SGD: A Decentralized Pipelined SGD Framework for Distributed Deep Net Training\",\"Improved Expressivity Through Dendritic Neural Networks\",\"Efficient Anomaly Detection via Matrix Sketching\",\"Learning to Specialize with Knowledge Distillation for Visual Question Answering\",\"A Lyapunov-based Approach to Safe Reinforcement Learning\",\"Credit Assignment For Collective Multiagent RL With Global Rewards\",\"Statistical Optimality of Stochastic Gradient Descent on Hard Learning Problems through Multiple Passes\",\"Does mitigating ML's impact disparity require treatment disparity?\",\"Proximal Graphical Event Models\",\"Bayesian Control of Large MDPs with Unknown Dynamics in Data-Poor Environments\",\"Learning Overparameterized Neural Networks via Stochastic Gradient Descent on Structured Data\",\"Hamiltonian Variational Auto-Encoder\",\"Modelling and unsupervised learning of symmetric deformable object categories\",\"Graphical model inference: Sequential Monte Carlo meets deterministic approximations\",\"Statistical mechanics of low-rank tensor decomposition\",\"Variational Bayesian Monte Carlo\",\"Sample-Efficient Reinforcement Learning with Stochastic Ensemble Value Expansion\",\"Efficient Online Portfolio with Logarithmic Regret\",\"Algorithms and Theory for Multiple-Source Adaptation\",\"Online Reciprocal Recommendation with Theoretical Performance Guarantees\",\"The promises and pitfalls of Stochastic Gradient Langevin Dynamics\",\"How SGD Selects the Global Minima in Over-parameterized Learning: A Dynamical Stability Perspective\",\"Differentiable MPC for End-to-end Planning and Control\",\"Bilevel learning of the Group Lasso structure\",\"Constructing Unrestricted Adversarial Examples with Generative Models\",\"Information-theoretic Limits for Community Detection in Network Models\",\"Learning Conditioned Graph Structures for Interpretable Visual Question Answering\",\"Distributionally Robust Graphical Models\",\"Transfer Learning with Neural AutoML\",\"Stochastic Primal-Dual Method for Empirical Risk Minimization with O(1) Per-Iteration Complexity\",\"On preserving non-discrimination when combining expert advice\",\"Learning to Play With Intrinsically-Motivated, Self-Aware Agents\",\"Scaling provable adversarial defenses\",\"Deep Network for the Integrated 3D Sensing of Multiple People in Natural Images\",\"Almost Optimal Algorithms for Linear Stochastic Bandits with Heavy-Tailed Payoffs\",\"Data-dependent PAC-Bayes priors via differential privacy\",\"Deep Poisson gamma dynamical systems\",\"Dimensionality Reduction has Quantifiable Imperfections: Two Geometric Bounds\",\"Teaching Inverse Reinforcement Learners via Features and Demonstrations\",\"Wasserstein Distributionally Robust Kalman Filtering\",\"Generalisation of structural knowledge in the hippocampal-entorhinal system\",\"Graph Oracle Models, Lower Bounds, and Gaps for Parallel Stochastic Optimization\",\"Adversarial Regularizers in Inverse Problems\",\"Clustering Redemption\\u2013Beyond the Impossibility of Kleinberg\\u2019s Axioms\",\"Co-teaching: Robust training of deep neural networks with extremely noisy labels\",\"Variational Inverse Control with Events: A General Framework for Data-Driven Reward Definition\",\"A convex program for bilinear inversion of sparse vectors\",\"Adversarial Multiple Source Domain Adaptation\",\"Neural Tangent Kernel: Convergence and Generalization in Neural Networks\",\"Contextual Stochastic Block Models\",\"A Likelihood-Free Inference Framework for Population Genetic Data using Exchangeable Neural Networks\",\"Sequential Attend, Infer, Repeat: Generative Modelling of Moving Objects\",\"Randomized Prior Functions for Deep Reinforcement Learning\",\"Compact Representation of Uncertainty in Clustering\",\"Learning without the Phase: Regularized PhaseMax Achieves Optimal Sample Complexity\",\"Multilingual Anchoring: Interactive Topic Modeling and Alignment Across Languages\",\"Estimators for Multivariate Information Measures in General Probability Spaces\",\"DeepPINK: reproducible feature selection in deep neural networks\",\"HOUDINI: Lifelong Learning as Program Synthesis\",\"Searching for Efficient Multi-Scale Architectures for Dense Image Prediction\",\"Orthogonally Decoupled Variational Gaussian Processes\",\"Dendritic cortical microcircuits approximate the backpropagation algorithm\",\"Learning Plannable Representations with Causal InfoGAN\",\"Uniform Convergence of Gradients for Non-Convex Learning and Optimization\",\"Automatic differentiation in ML: Where we are and where we should be going\",\"A Bayesian Nonparametric View on Count-Min Sketch\",\"Generalized Cross Entropy Loss for Training Deep Neural Networks with Noisy Labels\",\"Loss Surfaces, Mode Connectivity, and Fast Ensembling of DNNs\",\"Flexible neural representation for physics prediction\",\"Legendre Decomposition for Tensors\",\"Reinforcement Learning of Theorem Proving\",\"Data Amplification: A Unified and Competitive Approach to Property Estimation\",\"Group Equivariant Capsule Networks\",\"Stein Variational Gradient Descent as Moment Matching\",\"Differential Privacy for Growing Databases\",\"Exploration in Structured Reinforcement Learning\",\"A Statistical Recurrent Model on the Manifold of Symmetric Positive Definite Matrices\",\"Balanced Policy Evaluation and Learning\",\"Distributed Multitask Reinforcement Learning with Quadratic Convergence\",\"Improving Neural Program Synthesis with Inferred Execution Traces\",\"Adaptive Path-Integral Autoencoders: Representation Learning and Planning for Dynamical Systems\",\"Policy-Conditioned Uncertainty Sets for Robust Markov Decision Processes\",\"GLoMo: Unsupervised Learning of Transferable Relational Graphs\",\"Online Learning of Quantum States\",\"Wavelet regression and additive models for irregularly spaced data\",\"Inferring Latent Velocities from Weather Radar Data using Gaussian Processes\",\"A Structured Prediction Approach for Label Ranking\",\"Efficient High Dimensional Bayesian Optimization with Additivity and Quadrature Fourier Features\",\"FastGRNN: A Fast, Accurate, Stable and Tiny Kilobyte Sized Gated Recurrent Neural Network\",\"Reversible Recurrent Neural Networks\",\"SING: Symbol-to-Instrument Neural Generator\",\"Learning Compressed Transforms with Low Displacement Rank\",\"Theoretical Linear Convergence of Unfolded ISTA and Its Practical Weights and Thresholds\",\"Iterative Value-Aware Model Learning\",\"Invariant Representations without Adversarial Training\",\"Robot Learning in Homes: Improving Generalization and Reducing Dataset Bias\",\"Learning Safe Policies with Expert Guidance\",\"Bayesian multi-domain learning for cancer subtype discovery from next-generation sequencing count data\",\"Learning SMaLL Predictors\",\"Phase Retrieval Under a Generative Prior\",\"Quadrature-based features for kernel approximation\",\"Reducing Network Agnostophobia\",\"A Stein variational Newton method\",\"Watch Your Step: Learning Node Embeddings via Graph Attention\",\"Visual Reinforcement Learning with Imagined Goals\",\"Deep Predictive Coding Network with Local Recurrent Processing for Object Recognition\",\"PAC-Bayes bounds for stable algorithms with instance-dependent priors\",\"Beyond Grids: Learning Graph Representations for Visual Recognition\",\"The Limit Points of (Optimistic) Gradient Descent in Min-Max Optimization\",\"Coordinate Descent with Bandit Sampling\",\"Deep Dynamical Modeling and Control of Unsteady Fluid Flows\",\"Confounding-Robust Policy Improvement\",\"The Importance of Sampling inMeta-Reinforcement Learning\",\"Representer Point Selection for Explaining Deep Neural Networks\",\"The Effect of Network Width on the Performance of Large-batch Training\",\"SNIPER: Efficient Multi-Scale Training\",\"The Sample Complexity of Semi-Supervised Learning with Nonparametric Mixture Models\",\"Hardware Conditioned Policies for Multi-Robot Transfer Learning\",\"Co-regularized Alignment for Unsupervised Domain Adaptation\",\"Statistical and Computational Trade-Offs in Kernel K-Means\",\"Assessing the Scalability of Biologically-Motivated Deep Learning Algorithms and Architectures\",\"Learning Attractor Dynamics for Generative Memory\",\"The emergence of multiple retinal cell types through efficient coding of natural movies\",\"Gather-Excite: Exploiting Feature Context in Convolutional Neural Networks\",\"The Global Anchor Method for Quantifying Linguistic Shifts and Domain Adaptation\",\"Identification and Estimation of Causal Effects from Dependent Data\",\"Deepcode: Feedback Codes via Deep Learning\",\"Learning and Testing Causal Models with Interventions\",\"Implicit Bias of Gradient Descent on Linear Convolutional Networks\",\"DAGs with NO TEARS: Continuous Optimization for Structure Learning\",\"PAC-Bayes Tree: weighted subtrees with guarantees\",\"Multi-objective Maximization of Monotone Submodular Functions with Cardinality Constraint\",\"Sanity Checks for Saliency Maps\",\"Probabilistic Model-Agnostic Meta-Learning\",\"Reinforcement Learning with Multiple Experts: A Bayesian Model Combination Approach\",\"e-SNLI: Natural Language Inference with Natural Language Explanations\",\"Fast Approximate Natural Gradient Descent in a Kronecker Factored Eigenbasis\",\"Learning convex bounds for linear quadratic control policy synthesis\",\"Neural Proximal Gradient Descent for Compressive Imaging\",\"Towards Understanding Learning Representations: To What Extent Do Different Neural Networks Learn the Same Representation\",\"Optimal Algorithms for Continuous Non-monotone Submodular and DR-Submodular Maximization\",\"An intriguing failing of convolutional neural networks and the CoordConv solution\",\"Trading robust representations for sample complexity through self-supervised visual experience\",\"Invertibility of Convolutional Generative Networks from Partial Measurements\",\"Ex ante coordination and collusion in zero-sum multi-player extensive-form games\",\"Multi-Agent Reinforcement Learning via Double Averaging Primal-Dual Optimization\",\"Improving Online Algorithms via ML Predictions\",\"Global Non-convex Optimization with Discretized Diffusions\",\"Theoretical guarantees for EM under misspecified Gaussian mixture models\",\"Coupled Variational Bayes via Optimization Embedding\",\"Improving Explorability in Variational Inference with Annealed Variational Objectives\",\"Latent Alignment and Variational Attention\",\"Towards Deep Conversational Recommendations\",\"Unsupervised Depth Estimation, 3D Face Rotation and Replacement\",\"Generalization Bounds for Uniformly Stable Algorithms\",\"Deep Anomaly Detection Using Geometric Transformations\",\"Large Scale computation of Means and Clusters for Persistence Diagrams using Optimal Transport\",\"Entropy Rate Estimation for Markov Chains with Large State Space\",\"Adaptive Methods for Nonconvex Optimization\",\"Object-Oriented Dynamics Predictor\",\"Adaptive Skip Intervals: Temporal Abstraction for Recurrent Dynamical Models\",\"Scalable End-to-End Autonomous Vehicle Testing via Rare-event Simulation\",\"Reinforcement Learning for Solving the Vehicle Routing Problem\",\"ATOMO: Communication-efficient Learning via Atomic Sparsification\",\"Dynamic Network Model from Partial Observations\",\"Life-Long Disentangled Representation Learning with Cross-Domain Latent Homologies\",\"Maximizing acquisition functions for Bayesian optimization\",\"On Markov Chain Gradient Descent\",\"Variance-Reduced Stochastic Gradient Descent on Streaming Data\",\"Online Robust Policy Learning in the Presence of Unknown Adversaries\",\"Uplift Modeling from Separate Labels\",\"Learning Invariances using the Marginal Likelihood\",\"Non-delusional Q-learning and value-iteration\",\"Using Large Ensembles of Control Variates for Variational Inference\",\"Post: Device Placement with Cross-Entropy Minimization and Proximal Policy Optimization\",\"Learning to Reason with Third Order Tensor Products\",\"Memory Augmented Policy Optimization for Program Synthesis and Semantic Parsing\",\"Persistence Fisher Kernel: A Riemannian Manifold Kernel for Persistence Diagrams\",\"Neural Voice Cloning with a Few Samples\",\"Blind Deconvolutional Phase Retrieval via Convex Programming\",\"Scalable Laplacian K-modes\",\"A Retrieve-and-Edit Framework for Predicting Structured Outputs\",\"Testing for Families of Distributions via the Fourier Transform\",\"Thwarting Adversarial Examples: An L_0-Robust Sparse Fourier Transform\",\"Blockwise Parallel Decoding for Deep Autoregressive Models\",\"Low-Rank Tucker Decomposition of Large Tensors Using TensorSketch\",\"A Simple Cache Model for Image Recognition\",\"Clebsch\\u2013Gordan Nets: a Fully Fourier Space Spherical Convolutional Neural Network\",\"Bayesian Nonparametric Spectral Estimation\",\"A Spectral View of Adversarially Robust Features\",\"Synaptic Strength For Convolutional Neural Network\",\"Human-in-the-Loop Interpretability Prior\",\"Learning To Learn Around A Common Mean\",\"Backpropagation with Callbacks: Foundations for Efficient and Expressive Differentiable Programming\",\"Learning with SGD and Random Features\",\"Total stochastic gradient algorithms and applications in reinforcement learning\",\"Glow: Generative Flow with Invertible 1x1 Convolutions\",\"Nonparametric Density Estimation under Adversarial Losses\",\"Generalizing Point Embeddings using the Wasserstein Space of Elliptical Distributions\",\"Learning to Share and Hide Intentions using Information Regularization\",\"Predictive Approximate Bayesian Computation via Saddle Points\",\"Robustness of conditional GANs to noisy labels\",\"Robust Learning of Fixed-Structure Bayesian Networks\",\"Improving Simple Models with Confidence Profiles\",\"PCA of high dimensional random walks with comparison to neural network training\",\"Learning to Solve SMT Formulas\",\"Lifted Weighted Mini-Bucket\",\"Learning and Inference in Hilbert Space with Quantum Graphical Models\",\"Unsupervised Image-to-Image Translation Using Domain-Specific Variational Information Bound\",\"Adversarial Risk and Robustness: General Definitions and Implications for the Uniform Distribution\",\"Gaussian Process Prior Variational Autoencoders\",\"3D Steerable CNNs: Learning Rotationally Equivariant Features in Volumetric Data\",\"Context-aware Synthesis and Placement of Object Instances\",\"Convex Elicitation of Continuous Properties\",\"Mesh-TensorFlow: Deep Learning for Supercomputers\",\"Learning Abstract Options\",\"Bounded-Loss Private Prediction Markets\",\"Temporal alignment and latent Gaussian process factor inference in population spike trains\",\"Using Trusted Data to Train Deep Networks on Labels Corrupted by Severe Noise\",\"Discretely Relaxing Continuous Variables for tractable Variational Inference\",\"Regret bounds for meta Bayesian optimization with an unknown Gaussian process prior\",\"Diversity-Driven Exploration Strategy for Deep Reinforcement Learning\",\"Deep Generative Models with Learnable Knowledge Constraints\",\"The Sparse Manifold Transform\",\"Bayesian Structure Learning by Recursive Bootstrap\",\"Complex Gated Recurrent Neural Networks\",\"Learning a Warping Distance from Unlabeled Time Series Using Sequence Autoencoders\",\"Streamlining Variational Inference for Constraint Satisfaction Problems\",\"Fast deep reinforcement learning using online adjustments from the past\",\"Improved Network Robustness with Adversary Critic\",\"Regret Bounds for Online Portfolio Selection with a Cardinality Constraint\",\"Sketching Method for Large Scale Combinatorial Inference\",\"Connecting Optimization and Regularization Paths\",\"Fully Neural Network Based Speech Recognition on Mobile and Embedded Devices\",\"Understanding Regularized Spectral Clustering via Graph Conductance\",\"Data-Driven Clustering via Parameterized Lloyd's Families\",\"Learning Beam Search Policies via Imitation Learning\",\"Benefits of over-parameterization with EM\",\"Thermostat-assisted continuously-tempered Hamiltonian Monte Carlo for Bayesian learning\",\"Robust Subspace Approximation in a Stream\",\"Mean Field for the Stochastic Blockmodel: Optimization Landscape and Convergence Issues\",\"Analysis of Krylov Subspace Solutions of Regularized Non-Convex Quadratic Problems\",\"Autoconj: Recognizing and Exploiting Conjugacy Without a Domain-Specific Language\",\"DropBlock: A regularization method for convolutional networks\",\"Forward Modeling for Partial Observation Strategy Games - A StarCraft Defogger\",\"With Friends Like These, Who Needs Adversaries?\",\"Decentralize and Randomize: Faster Algorithm for Wasserstein Barycenters\",\"Joint Autoregressive and Hierarchical Priors for Learned Image Compression\",\"Learning Temporal Point Processes via Reinforcement Learning\",\"Bias and Generalization in Deep Generative Models: An Empirical Study\",\"Fast and Effective Robustness Certification\",\"Support Recovery for Orthogonal Matching Pursuit: Upper and Lower bounds\",\"Differentially Private Change-Point Detection\",\"Multi-value Rule Sets for Interpretable Classification with Feature-Efficient Representations\",\"Domain Adaptation by Using Causal Inference to Predict Invariant Conditional Distributions\",\"Smoothed Analysis of Discrete Tensor Decomposition and Assemblies of Neurons\",\"MixLasso: Generalized Mixed Regression via Convex Atomic-Norm Regularization\",\"Semidefinite relaxations for certifying robustness to adversarial examples\",\"Removing Hidden Confounding by Experimental Grounding\",\"Topkapi: Parallel and Fast Sketches for Finding Top-K Frequent Elements\",\"Contrastive Learning from Pairwise Measurements\",\"Point process latent variable models of larval zebrafish behavior\",\"Computationally and statistically efficient learning of causal Bayes nets using path queries\",\"Sparse PCA from Sparse Linear Regression\",\"Multiple Instance Learning for Efficient Sequential Data Classification on Resource-constrained Devices\",\"Transfer of Deep Reactive Policies for MDP Planning\",\"The Price of Fair PCA: One Extra dimension\",\"GroupReduce: Block-Wise Low-Rank Approximation for Neural Language Model Shrinking\",\"Wider and Deeper, Cheaper and Faster: Tensorized LSTMs for Sequence Learning\",\"Concentration of Multilinear Functions of the Ising Model with Applications to Network Data\",\"Deep Subspace Clustering Networks\",\"Attentional Pooling for Action Recognition\",\"On the Consistency of Quick Shift\",\"Breaking the Nonsmooth Barrier: A Scalable Parallel Method for Composite Optimization\",\"Dual-Agent GANs for Photorealistic and Identity Preserving Profile Face Synthesis\",\"Dilated Recurrent Neural Networks\",\"Hunt For The Unique, Stable, Sparse And Fast Feature Learning On Graphs\",\"Scalable Generalized Linear Bandits: Online Computation and Hashing\",\"Probabilistic Models for Integration Error in the Assessment of Functional Cardiac Models\",\"Machine Learning with Adversaries: Byzantine Tolerant Gradient Descent\",\"Dynamic Safe Interruptibility for Decentralized Multi-Agent Reinforcement Learning\",\"Interactive Submodular Bandit\",\"Learning to See Physics via Visual De-animation\",\"Label Efficient Learning of Transferable Representations acrosss Domains and Tasks\",\"Decoding with Value Networks for Neural Machine Translation\",\"Parametric Simplex Method for Sparse Learning\",\"Group Sparse Additive Machine\",\"Uprooting and Rerooting Higher-Order Graphical Models\",\"The Unreasonable Effectiveness of Structured Random Orthogonal Embeddings\",\"From Parity to Preference-based Notions of Fairness in Classification\",\"Inferring Generative Model Structure with Static Analysis\",\"Structured Embedding Models for Grouped Data\",\"A Linear-Time Kernel Goodness-of-Fit Test\",\"Cortical microcircuits as gated-recurrent neural networks\",\"k-Support and Ordered Weighted Sparsity for Overlapping Groups: Hardness and Algorithms\",\"A simple model of recognition and recall memory\",\"On Structured Prediction Theory with Calibrated Convex Surrogate Losses\",\"Best of Both Worlds: Transferring Knowledge from Discriminative Learning to a Generative Visual Dialog Model\",\"MaskRNN: Instance Level Video Object Segmentation\",\"Gated Recurrent Convolution Neural Network for OCR\",\"Towards Accurate Binary Convolutional Neural Network\",\"Semi-Supervised Learning for Optical Flow with Generative Adversarial Networks\",\"Learning a Multi-View Stereo Machine\",\"Phase Transitions in the Pooled Data Problem\",\"Universal Style Transfer via Feature Transforms\",\"On the Model Shrinkage Effect of Gamma Process Edge Partition Models\",\"Pose Guided Person Image Generation\",\"Inference in Graphical Models via Semidefinite Programming Hierarchies\",\"Variable Importance Using Decision Trees\",\"Preventing Gradient Explosions in Gated Recurrent Units\",\"On the Power of Truncated SVD for General High-rank Matrix Estimation Problems\",\"f-GANs in an Information Geometric Nutshell\",\"Toward Multimodal Image-to-Image Translation\",\"Mixture-Rank Matrix Approximation for Collaborative Filtering\",\"Continuous DR-submodular Maximization: Structure and Algorithms\",\"Learning with Average Top-k Loss\",\"Learning multiple visual domains with residual adapters\",\"Dykstra's Algorithm, ADMM, and Coordinate Descent: Connections, Insights, and Extensions\",\"Learning Spherical Convolution for Fast Features from 360\\u00b0 Imagery\",\"MarrNet: 3D Shape Reconstruction via 2.5D Sketches\",\"Multimodal Learning and Reasoning for Visual Question Answering\",\"Adversarial Surrogate Losses for Ordinal Regression\",\"Hypothesis Transfer Learning via Transformation Functions\",\"Controllable Invariance through Adversarial Feature Learning\",\"Convergence Analysis of Two-layer Neural Networks with ReLU Activation\",\"Doubly Accelerated Stochastic Variance Reduced Dual Averaging Method for Regularized Empirical Risk Minimization\",\"Langevin Dynamics with Continuous Tempering for Training Deep Neural Networks\",\"Efficient Online Linear Optimization with Approximation Algorithms\",\"Geometric Descent Method for Convex Composite Minimization\",\"Diffusion Approximations for Online Principal Component Estimation and Global Convergence\",\"Avoiding Discrimination through Causal Reasoning\",\"Nonparametric Online Regression while Learning the Metric\",\"Recycling Privileged Learning and Distribution Matching for Fairness\",\"Safe and Nested Subgame Solving for Imperfect-Information Games\",\"Unsupervised Image-to-Image Translation Networks\",\"Coded Distributed Computing for Inverse Problems\",\"A Screening Rule for l1-Regularized Ising Model Estimation\",\"Improved Dynamic Regret for Non-degenerate Functions\",\"Learning Efficient Object Detection Models with Knowledge Distillation\",\"One-Sided Unsupervised Domain Mapping\",\"Deep Mean-Shift Priors for Image Restoration\",\"Greedy Algorithms for Cone Constrained Optimization with Convergence Guarantees\",\"A New Theory for Matrix Completion\",\"Robust Hypothesis Test for Nonlinear Effect with Gaussian Processes\",\"Lower bounds on the robustness to adversarial perturbations\",\"Minimizing a Submodular Function from Samples\",\"Introspective Classification with Convolutional Nets\",\"Label Distribution Learning Forests\",\"Unsupervised learning of object frames by dense equivariant image labelling\",\"Compression-aware Training of Deep Networks\",\"Multiscale Semi-Markov Dynamics for Intracortical Brain-Computer Interfaces\",\"PredRNN: Recurrent Neural Networks for Predictive Learning using Spatiotemporal LSTMs\",\"Detrended Partial Cross Correlation for Brain Connectivity Analysis\",\"Contrastive Learning for Image Captioning\",\"Safe Model-based Reinforcement Learning with Stability Guarantees\",\"Online multiclass boosting\",\"Matching on Balanced Nonlinear Representations for Treatment Effects Estimation\",\"Learning Overcomplete HMMs\",\"GP CaKe: Effective brain connectivity with causal kernels\",\"Decoupling \\\"when to update\\\" from \\\"how to update\\\"\",\"Self-Normalizing Neural Networks\",\"Learning to Pivot with Adversarial Networks\",\"SchNet: A continuous-filter convolutional neural network for modeling quantum interactions\",\"Active Bias: Training More Accurate Neural Networks by Emphasizing High Variance Samples\",\"Differentiable Learning of Submodular Models\",\"Inductive Representation Learning on Large Graphs\",\"Subset Selection and Summarization in Sequential Data\",\"Question Asking as Program Generation\",\"Revisiting Perceptron: Efficient and Label-Optimal Learning of Halfspaces\",\"Gradient Descent Can Take Exponential Time to Escape Saddle Points\",\"Union of Intersections (UoI) for Interpretable Data Driven Discovery and Prediction\",\"One-Shot Imitation Learning\",\"Learning the Morphology of Brain Signals Using Alpha-Stable Convolutional Sparse Coding\",\"Integration Methods and Optimization Algorithms\",\"Sharpness, Restart and Acceleration\",\"Learning Koopman Invariant Subspaces for Dynamic Mode Decomposition\",\"Soft-to-Hard Vector Quantization for End-to-End Learning Compressible Representations\",\"Learning spatiotemporal piecewise-geodesic trajectories from longitudinal manifold-valued data\",\"Improving Regret Bounds for Combinatorial Semi-Bandits with Probabilistically Triggered Arms and Its Applications\",\"Predictive-State Decoders: Encoding the Future into Recurrent Networks\",\"Optimistic posterior sampling for reinforcement learning: worst-case regret bounds\",\"Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results\",\"Matching neural paths: transfer from recognition to correspondence search\",\"Linearly constrained Gaussian processes\",\"Fixed-Rank Approximation of a Positive-Semidefinite Matrix from Streaming Data\",\"Multi-Modal Imitation Learning from Unstructured Demonstrations using Generative Adversarial Nets\",\"Learning to Inpaint for Image Compression\",\"Adaptive Bayesian Sampling with Monte Carlo EM\",\"ADMM without a Fixed Penalty Parameter: Faster Convergence with New Adaptive Penalization\",\"Shape and Material from Sound\",\"Flexible statistical inference for mechanistic models of neural dynamics\",\"Online Prediction with Selfish Experts\",\"Tensor Biclustering\",\"DPSCREEN: Dynamic Personalized Screening\",\"Learning Unknown Markov Decision Processes: A Thompson Sampling Approach\",\"Testing and Learning on Distributions with Symmetric Noise Invariance\",\"A Dirichlet Mixture Model of Hawkes Processes for Event Sequence Clustering\",\"Deanonymization in the Bitcoin P2P Network\",\"Accelerated consensus via Min-Sum Splitting\",\"Generalized Linear Model Regression under Distance-to-set Penalties\",\"Adaptive stimulus selection for optimizing neural population responses\",\"Nonbacktracking Bounds on the Influence in Independent Cascade Models\",\"Learning with Feature Evolvable Streams\",\"Online Convex Optimization with Stochastic Constraints\",\"Max-Margin Invariant Features from Transformed Unlabelled Data\",\"Regularized Modal Regression with Applications in Cognitive Impairment Prediction\",\"Translation Synchronization via Truncated Least Squares\",\"From which world is your graph\",\"A New Alternating Direction Method for Linear Programming\",\"Regret Analysis for Continuous Dueling Bandit\",\"Best Response Regression\",\"TernGrad: Ternary Gradients to Reduce Communication in Distributed Deep Learning\",\"Learning Affinity via Spatial Propagation Networks\",\"Linear regression without correspondence\",\"NeuralFDR: Learning Discovery Thresholds from Hypothesis Features\",\"Cost efficient gradient boosting\",\"Probabilistic Rule Realization and Selection\",\"Nearest-Neighbor Sample Compression: Efficiency, Consistency, Infinite Dimensions\",\"A Scale Free Algorithm for Stochastic Bandits with Bounded Kurtosis\",\"Learning Multiple Tasks with Multilinear Relationship Networks\",\"Deep Hyperalignment\",\"Online to Offline Conversions, Universality and Adaptive Minibatch Sizes\",\"Stochastic Optimization with Variance Reduction for Infinite Datasets with Finite Sum Structure\",\"Deep Learning with Topological Signatures\",\"Predicting User Activity Level In Point Processes With Mass Transport Equation\",\"Submultiplicative Glivenko-Cantelli and Uniform Convergence of Revenues\",\"Deep Dynamic Poisson Factorization Model\",\"Positive-Unlabeled Learning with Non-Negative Risk Estimator\",\"Optimal Sample Complexity of M-wise Data for Top-K Ranking\",\"Reliable Decision Support using Counterfactual Models\",\"QSGD: Communication-Efficient SGD via Gradient Quantization and Encoding\",\"Convergent Block Coordinate Descent for Training Tikhonov Regularized Deep Neural Networks\",\"Train longer, generalize better: closing the generalization gap in large batch training of neural networks\",\"Flexpoint: An Adaptive Numerical Format for Efficient Training of Deep Neural Networks\",\"Model evidence from nonequilibrium simulations\",\"Minimal Exploration in Structured Stochastic Bandits\",\"Learned D-AMP: Principled Neural Network based Compressive Image Recovery\",\"Deliberation Networks: Sequence Generation Beyond One-Pass Decoding\",\"Adaptive Clustering through Semidefinite Programming\",\"Log-normality and Skewness of Estimated State/Action Values in Reinforcement Learning\",\"Repeated Inverse Reinforcement Learning\",\"The Numerics of GANs\",\"Practical Bayesian Optimization for Model Fitting with Bayesian Adaptive Direct Search\",\"Learning Chordal Markov Networks via Branch and Bound\",\"Revenue Optimization with Approximate Bid Predictions\",\"Solving Most Systems of Random Quadratic Equations\",\"Unsupervised Learning of Disentangled and Interpretable Representations from Sequential Data\",\"Lookahead Bayesian Optimization with Inequality Constraints\",\"Hierarchical Methods of Moments\",\"Interpretable and Globally Optimal Prediction for Textual Grounding using Image Concepts\",\"Revisit Fuzzy Neural Network: Demystifying Batch Normalization and ReLU with Generalized Hamming Network\",\"Speeding Up Latent Variable Gaussian Graphical Model Estimation via Nonconvex Optimization\",\"Batch Renormalization: Towards Reducing Minibatch Dependence in Batch-Normalized Models\",\"Generating steganographic images via adversarial training\",\"Near-linear time approximation algorithms for optimal transport via Sinkhorn iteration\",\"PixelGAN Autoencoders\",\"Consistent Multitask Learning with Nonlinear Output Relations\",\"Alternating minimization for dictionary learning with random initialization\",\"Learning ReLUs via Gradient Descent\",\"Stabilizing Training of Generative Adversarial Networks through Regularization\",\"Expectation Propagation with Stochastic Kinetic Model in Complex Interaction Systems\",\"Data-Efficient Reinforcement Learning in Continuous State-Action Gaussian-POMDPs\",\"Compatible Reward Inverse Reinforcement Learning\",\"First-Order Adaptive Sample Size Methods to Reduce Complexity of Empirical Risk Minimization\",\"Hiding Images in Plain Sight: Deep Steganography\",\"Neural Program Meta-Induction\",\"Bayesian Dyadic Trees and Histograms for Regression\",\"A graph-theoretic approach to multitasking\",\"Consistent Robust Regression\",\"Natural Value Approximators: Learning when to Trust Past Estimates\",\"Bandits Dueling on Partially Ordered Sets\",\"Elementary Symmetric Polynomials for Optimal Experimental Design\",\"Emergence of Language with Multi-agent Games: Learning to Communicate with Sequences of Symbols\",\"Training Deep Networks without Learning Rates Through Coin Betting\",\"Pixels to Graphs by Associative Embedding\",\"Runtime Neural Pruning\",\"Eigenvalue Decay Implies Polynomial-Time Learnability for Neural Networks\",\"MMD GAN: Towards Deeper Understanding of Moment Matching Network\",\"The Reversible Residual Network: Backpropagation Without Storing Activations\",\"Fast Rates for Bandit Optimization with Upper-Confidence Frank-Wolfe\",\"Zap Q-Learning\",\"Expectation Propagation for t-Exponential Family Using q-Algebra\",\"Few-Shot Learning Through an Information Retrieval Lens\",\"Formal Guarantees on the Robustness of a Classifier against Adversarial Manipulation\",\"Associative Embedding: End-to-End Learning for Joint Detection and Grouping\",\"Practical Locally Private Heavy Hitters\",\"Large-Scale Quadratically Constrained Quadratic Program via Low-Discrepancy Sequences\",\"Inhomogeneous Hypergraph Clustering with Applications\",\"Differentiable Learning of Logical Rules for Knowledge Base Reasoning\",\"Deep Multi-task Gaussian Processes for Survival Analysis with Competing Risks\",\"Masked Autoregressive Flow for Density Estimation\",\"Non-convex Finite-Sum Optimization Via SCSG Methods\",\"Beyond normality: Learning sparse probabilistic graphical models in the non-Gaussian setting\",\"An inner-loop free solution to inverse problems using deep neural networks\",\"OnACID: Online Analysis of Calcium Imaging Data in Real Time\",\"Collaborative PAC Learning\",\"Fast Black-box Variational Inference through Stochastic Trust-Region Optimization\",\"Scalable Demand-Aware Recommendation\",\"SGD Learns the Conjugate Kernel Class of the Network\",\"Noise-Tolerant Interactive Learning Using Pairwise Comparisons\",\"Analyzing Hidden Representations in End-to-End Automatic Speech Recognition Systems\",\"Generative Local Metric Learning for Kernel Regression\",\"Information Theoretic Properties of Markov Random Fields, and their Algorithmic Applications\",\"Fitting Low-Rank Tensors in Constant Time\",\"Deep Supervised Discrete Hashing\",\"Using Options and Covariance Testing for Long Horizon Off-Policy Policy Evaluation\",\"How regularization affects the critical points in linear networks\",\"Fisher GAN\",\"Information-theoretic analysis of generalization capability of learning algorithms\",\"Sparse Approximate Conic Hulls\",\"Rigorous Dynamics and Consistent Estimation in Arbitrarily Conditioned Linear Systems\",\"Toward Goal-Driven Neural Network Models for the Rodent Whisker-Trigeminal System\",\"Accuracy First: Selecting a Differential Privacy Level for Accuracy Constrained ERM\",\"EX2: Exploration with Exemplar Models for Deep Reinforcement Learning\",\"Multitask Spectral Learning of Weighted Automata\",\"Multi-way Interacting Regression via Factorization Machines\",\"Predicting Organic Reaction Outcomes with Weisfeiler-Lehman Network\",\"Practical Data-Dependent Metric Compression with Provable Guarantees\",\"REBAR: Low-variance, unbiased gradient estimates for discrete latent variable models\",\"Nonlinear random matrix theory for deep learning\",\"Parallel Streaming Wasserstein Barycenters\",\"ELF: An Extensive, Lightweight and Flexible Research Platform for Real-time Strategy Games\",\"Dual Discriminator Generative Adversarial Nets\",\"Dynamic Revenue Sharing\",\"Decomposition-Invariant Conditional Gradient for General Polytopes with Line Search\",\"VAIN: Attentional Multi-agent Predictive Modeling\",\"An Empirical Bayes Approach to Optimizing Machine Learning Algorithms\",\"Differentially Private Empirical Risk Minimization Revisited: Faster and More General\",\"Variational Inference via \\\\chi Upper Bound Minimization\",\"On Quadratic Convergence of DC Proximal Newton Algorithm in Nonconvex Sparse Learning\",\"#Exploration: A Study of Count-Based Exploration for Deep Reinforcement Learning\",\"An Empirical Study on The Properties of Random Bases for Kernel Methods\",\"Bridging the Gap Between Value and Policy Based Reinforcement Learning\",\"Premise Selection for Theorem Proving by Deep Graph Embedding\",\"A Bayesian Data Augmentation Approach for Learning Deep Models\",\"Principles of Riemannian Geometry in Neural Networks\",\"Cold-Start Reinforcement Learning with Softmax Policy Gradient\",\"Online Dynamic Programming\",\"Alternating Estimation for Structured High-Dimensional Multi-Response Models\",\"Convolutional Gaussian Processes\",\"Estimation of the covariance structure of heavy-tailed distributions\",\"Decomposable Submodular Function Minimization: Discrete and Continuous\",\"Gauging Variational Inference\",\"Deep Recurrent Neural Network-Based Identification of Precursor microRNAs\",\"Robust Estimation of Neural Signals in Calcium Imaging\",\"State Aware Imitation Learning\",\"Beyond Parity: Fairness Objectives for Collaborative Filtering\",\"A PAC-Bayesian Analysis of Randomized Learning with Application to Stochastic Gradient Descent\",\"Fully Decentralized Policies for Multi-Agent Systems: An Information Theoretic Approach\",\"Model-Powered Conditional Independence Test\",\"Deep Voice 2: Multi-Speaker Neural Text-to-Speech\",\"Variance-based Regularization with Convex Objectives\",\"Deep Lattice Networks and Partial Monotonic Functions\",\"Continual Learning with Deep Generative Replay\",\"AIDE: An algorithm for measuring the accuracy of probabilistic inference algorithms\",\"Learning Causal Structures Using Regression Invariance\",\"Online Influence Maximization under Independent Cascade Model with Semi-Bandit Feedback\",\"Near Minimax Optimal Players for the Finite-Time 3-Expert Prediction Problem\",\"Reinforcement Learning under Model Mismatch\",\"Hierarchical Attentive Recurrent Tracking\",\"Tomography of the London Underground: a Scalable Model for Origin-Destination Data\",\"Rotting Bandits\",\"Unbiased estimates for linear regression via volume sampling\",\"Approximation Bounds for Hierarchical Clustering: Average Linkage, Bisecting K-means, and Local Search\",\"Adaptive Accelerated Gradient Converging Method under H\\\\\\\"{o}lderian Error Bound Condition\",\"Stein Variational Gradient Descent as Gradient Flow\",\"Partial Hard Thresholding: Towards A Principled Analysis of Support Recovery\",\"Shallow Updates for Deep Reinforcement Learning\",\"LightGBM: A Highly Efficient Gradient Boosting Decision Tree\",\"Adversarial Ranking for Language Generation\",\"Regret Minimization in MDPs with Options without Prior Knowledge\",\"Net-Trim: Convex Pruning of Deep Neural Networks with Performance Guarantee\",\"Graph Matching via Multiplicative Update Algorithm\",\"Dynamic Importance Sampling for Anytime Bounds of the Partition Function\",\"Is the Bellman residual a bad proxy?\",\"Generalization Properties of Learning with Random Features\",\"Differentially private Bayesian learning on distributed data\",\"Learning to Compose Domain-Specific Transformations for Data Augmentation\",\"Wasserstein Learning of Deep Generative Point Process Models\",\"Ensemble Sampling\",\"Language Modeling with Recurrent Highway Hypernetworks\",\"Adaptive SVRG Methods under Error Bound Conditions with Unknown Growth Parameter\",\"Bayesian Compression for Deep Learning\",\"Streaming Sparse Gaussian Process Approximations\",\"VEEGAN: Reducing Mode Collapse in GANs using Implicit Variational Learning\",\"Sparse Embedded k-Means Clustering\",\"Dynamic-Depth Context Tree Weighting\",\"A Regularized Framework for Sparse and Structured Neural Attention\",\"Multi-output Polynomial Networks and Factorization Machines\",\"Clustering Billions of Reads for DNA Data Storage\",\"Multi-Objective Non-parametric Sequential Prediction\",\"A Universal Analysis of Large-Scale Regularized Least Squares Solutions\",\"Deep Sets\",\"ExtremeWeather: A large-scale climate dataset for semi-supervised detection, localization, and understanding of extreme weather events\",\"Process-constrained batch Bayesian optimisation\",\"Bayesian Inference of Individualized Treatment Effects using Multi-task Gaussian Processes\",\"Spherical convolutions and their application in molecular modelling\",\"Efficient Optimization for Linear Dynamical Systems with Applications to Clustering and Sparse Coding\",\"On Optimal Generalizability in Parametric Learning\",\"Near Optimal Sketching of Low-Rank Tensor Regression\",\"Tractability in Structured Probability Spaces\",\"Model-based Bayesian inference of neural activity and connectivity from all-optical interrogation of a neural circuit\",\"Gaussian process based nonlinear latent structure discovery in multivariate spike train data\",\"Neural system identification for large populations separating \\u201cwhat\\u201d and \\u201cwhere\\u201d\",\"Certified Defenses for Data Poisoning Attacks\",\"Eigen-Distortions of Hierarchical Representations\",\"Limitations on Variance-Reduction and Acceleration Schemes for Finite Sums Optimization\",\"Unsupervised Sequence Classification using Sequential Output Statistics\",\"Subset Selection under Noise\",\"Collecting Telemetry Data Privately\",\"Concrete Dropout\",\"Adaptive Batch Size for Safe Policy Gradients\",\"A Disentangled Recognition and Nonlinear Dynamics Model for Unsupervised Learning\",\"PASS-GLM: polynomial approximate sufficient statistics for scalable Bayesian GLM inference\",\"Bayesian GAN\",\"Off-policy evaluation for slate recommendation\",\"A multi-agent reinforcement learning model of common-pool resource appropriation\",\"On the Optimization Landscape of Tensor Decompositions\",\"High-Order Attention Models for Visual Question Answering\",\"Sparse convolutional coding for neuronal assembly detection\",\"Quantifying how much sensory information in a neural code is relevant for behavior\",\"Geometric Matrix Completion with Recurrent Multi-Graph Neural Networks\",\"Reducing Reparameterization Gradient Variance\",\"Visual Reference Resolution using Attention Memory for Visual Dialog\",\"Joint distribution optimal transportation for domain adaptation\",\"Multiresolution Kernel Approximation for Gaussian Process Regression\",\"Collapsed variational Bayes for Markov jump processes\",\"Universal consistency and minimax rates for online Mondrian Forests\",\"Welfare Guarantees from Data\",\"Diving into the shallows: a computational perspective on large-scale shallow learning\",\"End-to-end Differentiable Proving\",\"Influence Maximization with \\\\varepsilon-Almost Submodular Threshold Functions\",\"InfoGAIL: Interpretable Imitation Learning from Visual Demonstrations\",\"Variational Laws of Visual Attention for Dynamic Scenes\",\"Recursive Sampling for the Nystrom Method\",\"Interpolated Policy Gradient: Merging On-Policy and Off-Policy Gradient Estimation for Deep Reinforcement Learning\",\"Dynamic Routing Between Capsules\",\"Incorporating Side Information by Adaptive Convolution\",\"Conic Scan-and-Cover algorithms for nonparametric topic modeling\",\"FALKON: An Optimal Large Scale Kernel Method\",\"Structured Generative Adversarial Networks\",\"Conservative Contextual Linear Bandits\",\"Variational Memory Addressing in Generative Models\",\"On Tensor Train Rank Minimization : Statistical Efficiency and Scalable Algorithm\",\"Scalable Levy Process Priors for Spectral Kernel Learning\",\"Deep Hyperspherical Learning\",\"Learning Deep Structured Multi-Scale Features using Attention-Gated CRFs for Contour Prediction\",\"On-the-fly Operation Batching in Dynamic Computation Graphs\",\"Nonlinear Acceleration of Stochastic Algorithms\",\"Optimized Pre-Processing for Discrimination Prevention\",\"YASS: Yet Another Spike Sorter\",\"Independence clustering (without a matrix)\",\"Fast amortized inference of neural activity from calcium imaging data with variational autoencoders\",\"Adaptive Active Hypothesis Testing under Limited Information\",\"Streaming Weak Submodularity: Interpreting Neural Networks on the Fly\",\"Successor Features for Transfer in Reinforcement Learning\",\"Counterfactual Fairness\",\"Prototypical Networks for Few-shot Learning\",\"Triple Generative Adversarial Nets\",\"Efficient Sublinear-Regret Algorithms for Online Sparse Linear Regression with Limited Observation\",\"Mapping distinct timescales of functional interactions among brain networks\",\"Multi-Armed Bandits with Metric Movement Costs\",\"Learning A Structured Optimal Bipartite Graph for Co-Clustering\",\"Learning Low-Dimensional Metrics\",\"The Marginal Value of Adaptive Gradient Methods in Machine Learning\",\"Aggressive Sampling for Multi-class to Binary Reduction with Applications to Text Classification\",\"Deconvolutional Paragraph Representation Learning\",\"Random Permutation Online Isotonic Regression\",\"A Unified Game-Theoretic Approach to Multiagent Reinforcement Learning\",\"Inverse Filtering for Hidden Markov Models\",\"Non-parametric Structured Output Networks\",\"Learning Active Learning from Data\",\"VAE Learning via Stein Variational Gradient Descent\",\"Reconstructing perceived faces from brain activations with deep adversarial neural decoding\",\"Efficient Use of Limited-Memory Accelerators for Linear Learning on Heterogeneous Systems\",\"Temporal Coherency based Criteria for Predicting Video Frames using Deep Multi-stage Generative Adversarial Networks\",\"Sobolev Training for Neural Networks\",\"Multi-Information Source Optimization\",\"Deep Reinforcement Learning from Human Preferences\",\"On the Fine-Grained Complexity of Empirical Risk Minimization: Kernel Methods and Neural Networks\",\"Policy Gradient With Value Function Approximation For Collective Multiagent Planning\",\"Adversarial Symmetric Variational Autoencoder\",\"Unified representation of tractography and diffusion-weighted MRI data using sparse multidimensional arrays\",\"A Minimax Optimal Algorithm for Crowdsourcing\",\"Estimating Accuracy from Unlabeled Data: A Probabilistic Logic Approach\",\"A Decomposition of Forecast Error in Prediction Markets\",\"Safe Adaptive Importance Sampling\",\"Variational Walkback: Learning a Transition Operator as a Stochastic Recurrent Net\",\"Polynomial Codes: an Optimal Design for High-Dimensional Coded Matrix Multiplication\",\"Unsupervised Learning of Disentangled Representations from Video\",\"Federated Multi-Task Learning\",\"Is Input Sparsity Time Possible for Kernel Low-Rank Approximation?\",\"The Expxorcist: Nonparametric Graphical Models Via Conditional Exponential Densities\",\"Improved Graph Laplacian via Geometric Self-Consistency\",\"Dual Path Networks\",\"Faster and Non-ergodic O(1/K) Stochastic Alternating Direction Method of Multipliers\",\"A Probabilistic Framework for Nonlinearities in Stochastic Neural Networks\",\"Distral: Robust multitask reinforcement learning\",\"Online Learning of Optimal Bidding Strategy in Repeated Multi-Commodity Auctions\",\"Trimmed Density Ratio Estimation\",\"Training recurrent networks to generate hypotheses about how the brain solves hard navigation problems\",\"Visual Interaction Networks: Learning a Physics Simulator from Video\",\"Reconstruct & Crush Network\",\"Streaming Robust Submodular Maximization: A Partitioned Thresholding Approach\",\"Simple strategies for recovering inner products from coarsely quantized random projections\",\"Discovering Potential Correlations via Hypercontractivity\",\"Doubly Stochastic Variational Inference for Deep Gaussian Processes\",\"Ranking Data with Continuous Labels through Oriented Recursive Partitions\",\"Scalable Model Selection for Belief Networks\",\"Targeting EEG/LFP Synchrony with Neural Nets\",\"Near-Optimal Edge Evaluation in Explicit Generalized Binomial Graphs\",\"Non-Stationary Spectral Kernels\",\"Overcoming Catastrophic Forgetting by Incremental Moment Matching\",\"Balancing information exposure in social networks\",\"SafetyNets: Verifiable Execution of Deep Neural Networks on an Untrusted Cloud\",\"Query Complexity of Clustering with Side Information\",\"QMDP-Net: Deep Learning for Planning under Partial Observability\",\"Robust Optimization for Non-Convex Objectives\",\"Thy Friend is My Friend: Iterative Collaborative Filtering for Sparse Matrix Estimation\",\"Adaptive Classification for Prediction Under a Budget\",\"Convergence rates of a partition based Bayesian multivariate density estimation method\",\"Affine-Invariant Online Optimization and the Low-rank Experts Problem\",\"Beyond Worst-case: A Probabilistic Analysis of Affine Policies in Dynamic Optimization\",\"A Unified Approach to Interpreting Model Predictions\",\"Stochastic Approximation for Canonical Correlation Analysis\",\"Resurrecting the sigmoid in deep learning through dynamical isometry: theory and practice\",\"Sample and Computationally Efficient Learning Algorithms under S-Concave Distributions\",\"Scalable Variational Inference for Dynamical Systems\",\"Context Selection for Embedding Models\",\"Working hard to know your neighbor's margins: Local descriptor learning loss\",\"Accelerated Stochastic Greedy Coordinate Descent by Soft Thresholding Projection onto Simplex\",\"Multi-Task Learning for Contextual Bandits\",\"Learning to Prune Deep Neural Networks via Layer-wise Optimal Brain Surgeon\",\"Accelerated First-order Methods for Geodesically Convex Optimization on Riemannian Manifolds\",\"Selective Classification for Deep Neural Networks\",\"Minimax Estimation of Bandable Precision Matrices\",\"Monte-Carlo Tree Search by Best Arm Identification\",\"Group Additive Structure Identification for Kernel Nonparametric Regression\",\"Fast, Sample-Efficient Algorithms for Structured Phase Retrieval\",\"Hash Embeddings for Efficient Word Representations\",\"Online Learning for Multivariate Hawkes Processes\",\"Maximum Margin Interval Trees\",\"DropoutNet: Addressing Cold Start in Recommender Systems\",\"A simple neural network module for relational reasoning\",\"Q-LDA: Uncovering Latent Patterns in Text-based Sequential Decision Processes\",\"Online Reinforcement Learning in Stochastic Games\",\"Position-based Multiple-play Bandit Problem with Unknown Position Bias\",\"Active Exploration for Learning Symbolic Representations\",\"Clone MCMC: Parallel High-Dimensional Gaussian Gibbs Sampling\",\"Fair Clustering Through Fairlets\",\"Polynomial time algorithms for dual volume sampling\",\"Hindsight Experience Replay\",\"Stochastic and Adversarial Online Learning without Hyperparameters\",\"Teaching Machines to Describe Images with Natural Language Feedback\",\"Perturbative Black Box Variational Inference\",\"GibbsNet: Iterative Adversarial Inference for Deep Graphical Models\",\"PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space\",\"Regularizing Deep Neural Networks by Noise: Its Interpretation and Optimization\",\"Learning Graph Representations with Embedding Propagation\",\"Efficient Modeling of Latent Information in Supervised Learning using Gaussian Processes\",\"A-NICE-MC: Adversarial Training for MCMC\",\"Excess Risk Bounds for the Bayes Risk using Variational Inference in Latent Gaussian Models\",\"Real-Time Bidding with Side Information\",\"Saliency-based Sequential Image Attention with Multiset Prediction\",\"Variational Inference for Gaussian Process Models with Linear Complexity\",\"K-Medoids For K-Means Seeding\",\"Identifying Outlier Arms in Multi-Armed Bandit\",\"Online Learning with Transductive Regret\",\"Riemannian approach to batch normalization\",\"Self-supervised Learning of Motion Capture\",\"Triangle Generative Adversarial Networks\",\"PRUNE: Preserving Proximity and Global Ranking for Network Embedding\",\"Bayesian Optimization with Gradients\",\"Scalable trust-region method for deep reinforcement learning using Kronecker-factored approximation\",\"Renyi Differential Privacy Mechanisms for Posterior Sampling\",\"Online Learning with a Hint\",\"Identification of Gaussian Process State Space Models\",\"Robust Imitation of Diverse Behaviors\",\"Can Decentralized Algorithms Outperform Centralized Algorithms? A Case Study for Decentralized Parallel Stochastic Gradient Descent\",\"Local Aggregative Games\",\"A Sample Complexity Measure with Applications to Learning Optimal Auctions\",\"Thinking Fast and Slow with Deep Learning and Tree Search\",\"EEG-GRAPH: A Factor-Graph-Based Model for Capturing Spatial, Temporal, and Observational Relationships in Electroencephalograms\",\"Improving the Expected Improvement Algorithm\",\"Hybrid Reward Architecture for Reinforcement Learning\",\"Approximate Supermodularity Bounds for Experimental Design\",\"Maximizing Subset Accuracy with Recurrent Neural Networks in Multi-label Classification\",\"AdaGAN: Boosting Generative Models\",\"Straggler Mitigation in Distributed Optimization Through Data Encoding\",\"Multi-View Decision Processes: The Helper-AI Problem\",\"A Greedy Approach for Budgeted Maximum Inner Product Search\",\"SVD-Softmax: Fast Softmax Approximation on Large Vocabulary Neural Networks\",\"Plan, Attend, Generate: Planning for Sequence-to-Sequence Models\",\"Task-based End-to-end Model Learning in Stochastic Optimization\",\"ALICE: Towards Understanding Adversarial Learning for Joint Distribution Matching\",\"Finite Sample Analysis of the GTD Policy Evaluation Algorithms in Markov Setting\",\"On the Complexity of Learning Neural Networks\",\"Hierarchical Implicit Models and Likelihood-Free Variational Inference\",\"Semi-supervised Learning with GANs: Manifold Invariance with Improved Inference\",\"Approximation and Convergence Properties of Generative Adversarial Learning\",\"From Bayesian Sparsity to Gated Recurrent Nets\",\"Min-Max Propagation\",\"What Uncertainties Do We Need in Bayesian Deep Learning for Computer Vision?\",\"Gradient descent GAN optimization is locally stable\",\"Toward Robustness against Label Noise in Training Deep Discriminative Neural Networks\",\"Dualing GANs\",\"Deep Learning for Precipitation Nowcasting: A Benchmark and A New Model\",\"Do Deep Neural Networks Suffer from Crowding?\",\"Learning from Complementary Labels\",\"Online control of the false discovery rate with decaying memory\",\"Learning from uncertain curves: The 2-Wasserstein metric for Gaussian processes\",\"Discriminative State Space Models\",\"On Fairness and Calibration\",\"Imagination-Augmented Agents for Deep Reinforcement Learning\",\"Extracting low-dimensional dynamics from multiple large-scale neural population recordings by learning to predict correlations\",\"Unifying PAC and Regret: Uniform PAC Bounds for Episodic Reinforcement Learning\",\"Gradients of Generative Models for Improved Discriminative Analysis of Tandem Mass Spectra\",\"Asynchronous Parallel Coordinate Minimization for MAP Inference\",\"Multiscale Quantization for Fast Similarity Search\",\"Diverse and Accurate Image Description Using a Variational Auto-Encoder with an Additive Gaussian Encoding Space\",\"Improved Training of Wasserstein GANs\",\"Learning Populations of Parameters\",\"Clustering with Noisy Queries\",\"Higher-Order Total Variation Classes on Grids: Minimax Theory and Trend Filtering Methods\",\"Training Quantized Nets: A Deeper Understanding\",\"Permutation-based Causal Inference Algorithms with Interventions\",\"Time-dependent spatially varying graphical models, with application to brain fMRI data analysis\",\"Gradient Methods for Submodular Maximization\",\"Smooth Primal-Dual Coordinate Descent Algorithms for Nonsmooth Convex Optimization\",\"The Importance of Communities for Learning to Influence\",\"Multiplicative Weights Update with Constant Step-Size in Congestion Games: Convergence, Limit Cycles and Chaos\",\"Learning Neural Representations of Human Cognition across Many fMRI Studies\",\"A KL-LUCB algorithm for Large-Scale Crowdsourcing\",\"Collaborative Deep Learning in Fixed Topology Networks\",\"Fast-Slow Recurrent Neural Networks\",\"Learning Disentangled Representations with Semi-Supervised Deep Generative Models\",\"Self-Supervised Intrinsic Image Decomposition\",\"Exploring Generalization in Deep Learning\",\"A framework for Multi-A(rmed)/B(andit) Testing with Online FDR Control\",\"Fader Networks:Manipulating Images by Sliding Attributes\",\"Action Centered Contextual Bandits\",\"Estimating Mutual Information for Discrete-Continuous Mixtures\",\"Attention is All you Need\",\"Recurrent Ladder Networks\",\"Parameter-Free Online Learning via Model Selection\",\"Bregman Divergence for Stochastic Variance Reduction: Saddle-Point and Adversarial Prediction\",\"Unbounded cache model for online language modeling with open vocabulary\",\"Predictive State Recurrent Neural Networks\",\"Early stopping for kernel boosting algorithms: A general analysis with localized complexities\",\"SVCCA: Singular Vector Canonical Correlation Analysis for Deep Learning Dynamics and Interpretability\",\"Convolutional Phase Retrieval\",\"Estimating High-dimensional Non-Gaussian Multiple Index Models via Stein\\u2019s Lemma\",\"Gaussian Quadrature for Kernel Features\",\"Value Prediction Network\",\"A Learning Error Analysis for Structured Prediction with Approximate Inference\",\"Efficient Second-Order Online Kernel Learning with Adaptive Embedding\",\"Implicit Regularization in Matrix Factorization\",\"Optimal Shrinkage of Singular Values Under Random Data Contamination\",\"Countering Feedback Delays in Multi-Agent Learning\",\"Asynchronous Coordinate Descent under More Realistic Assumptions\",\"Linear Convergence of a Frank-Wolfe Type Algorithm over Trace-Norm Balls\",\"Hierarchical Clustering Beyond the Worst-Case\",\"Invariance and Stability of Deep Convolutional Representations\",\"Statistical Cost Sharing\",\"The Expressive Power of Neural Networks: A View from the Width\",\"Spectrally-normalized margin bounds for neural networks\",\"Robust and Efficient Transfer Learning with Hidden Parameter Markov Decision Processes\",\"Population Matching Discrepancy and Applications in Deep Learning\",\"Scalable Planning with Tensorflow for Hybrid Nonlinear Domains\",\"Boltzmann Exploration Done Right\",\"Learned in Translation: Contextualized Word Vectors\",\"Neural Discrete Representation Learning\",\"Generalizing GANs: A Turing Perspective\",\"Scalable Log Determinants for Gaussian Process Kernel Learning\",\"Poincar\\u00e9 Embeddings for Learning Hierarchical Representations\",\"Learning Combinatorial Optimization Algorithms over Graphs\",\"Robust Conditional Probabilities\",\"Learning with Bandit Feedback in Potential Games\",\"Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments\",\"Communication-Efficient Distributed Learning of Discrete Distributions\",\"Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles\",\"When Worlds Collide: Integrating Different Counterfactual Assumptions in Fairness\",\"Matrix Norm Estimation from a Few Entries\",\"Neural Networks for Efficient Bayesian Decoding of Natural Images from Retinal Neurons\",\"Causal Effect Inference with Deep Latent-Variable Models\",\"Learning Identifiable Gaussian Bayesian Networks in Polynomial Time and Sample Complexity\",\"Gradient Episodic Memory for Continual Learning\",\"Effective Parallelisation for Machine Learning\",\"Semisupervised Clustering, AND-Queries and Locally Encodable Source Coding\",\"Clustering Stable Instances of Euclidean k-means.\",\"Good Semi-supervised Learning That Requires a Bad GAN\",\"On Blackbox Backpropagation and Jacobian Sensing\",\"Protein Interface Prediction using Graph Convolutional Networks\",\"Solid Harmonic Wavelet Scattering: Predicting Quantum Molecular Energy from Invariant Descriptors of 3D Electronic Densities\",\"Towards Generalization and Simplicity in Continuous Control\",\"Random Projection Filter Bank for Time Series Data\",\"Filtering Variational Objectives\",\"On Frank-Wolfe and Equilibrium Computation\",\"Modulating early visual processing by language\",\"Learning Mixture of Gaussians with Streaming Data\",\"Practical Hash Functions for Similarity Estimation and Dimensionality Reduction\",\"GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium\",\"The Scaling Limit of High-Dimensional Online Independent Component Analysis\",\"Approximation Algorithms for \\\\ell_0-Low Rank Approximation\",\"The power of absolute discounting: all-dimensional distribution estimation\",\"Few-Shot Adversarial Domain Adaptation\",\"Spectral Mixture Kernels for Multi-Output Gaussian Processes\",\"Neural Expectation Maximization\",\"Learning Linear Dynamical Systems via Spectral Filtering\",\"Z-Forcing: Training Stochastic Recurrent Networks\",\"Learning Hierarchical Information Flow with Recurrent Neural Modules\",\"Neural Variational Inference and Learning in Undirected Graphical Models\",\"Subspace Clustering via Tangent Cones\",\"The Neural Hawkes Process: A Neurally Self-Modulating Multivariate Point Process\",\"Inverse Reward Design\",\"Structured Bayesian Pruning via Log-Normal Multiplicative Noise\",\"Attend and Predict: Understanding Gene Regulation by Selective Attention on Chromatin\",\"Acceleration and Averaging in Stochastic Descent Dynamics\",\"Kernel functions based on triplet comparisons\",\"An Error Detection and Correction Framework for Connectomics\",\"Style Transfer from Non-Parallel Text by Cross-Alignment\",\"Cross-Spectral Factor Analysis\",\"Stochastic Submodular Maximization: The Case of Coverage Functions\",\"Affinity Clustering: Hierarchical Clustering at Scale\",\"Unsupervised Transformation Learning via Convex Relaxations\",\"A Sharp Error Analysis for the Fused Lasso, with Application to Approximate Changepoint Screening\",\"Linear Time Computation of Moments in Sum-Product Networks\",\"A Meta-Learning Perspective on Cold-Start Recommendations for Items\",\"Predicting Scene Parsing and Motion Dynamics in the Future\",\"Sticking the Landing: Simple, Lower-Variance Gradient Estimators for Variational Inference\",\"Efficient Approximation Algorithms for Strings Kernel Based Sequence Classification\",\"Kernel Feature Selection via Conditional Covariance Minimization\",\"Convergence of Gradient EM on Multi-component Mixture of Gaussians\",\"Real Time Image Saliency for Black Box Classifiers\",\"Houdini: Fooling Deep Structured Visual and Speech Recognition Models with Adversarial Examples\",\"Efficient and Flexible Inference for Stochastic Systems\",\"When Cyclic Coordinate Descent Outperforms Randomized Coordinate Descent\",\"Active Learning from Peers\",\"Experimental Design for Learning Causal Graphs with Latent Variables\",\"Learning to Model the Tail\",\"Stochastic Mirror Descent in Variationally Coherent Optimization Problems\",\"On Separability of Loss Functions, and Revisiting Discriminative Vs Generative Models\",\"Maxing and Ranking with Few Assumptions\",\"On clustering network-valued data\",\"A General Framework for Robust Interactive Learning\",\"Multi-view Matrix Factorization for Linear Dynamical System Estimation\",\"Mean Field Residual Networks: On the Edge of Chaos\",\"Scan Order in Gibbs Sampling: Models in Which it Matters and Bounds on How Much\",\"Deep ADMM-Net for Compressive Sensing MRI\",\"A scaled Bregman theorem with applications\",\"Swapout: Learning an ensemble of deep architectures\",\"On Regularizing Rademacher Observation Losses\",\"Without-Replacement Sampling for Stochastic Gradient Methods\",\"Fast and Provably Good Seedings for k-Means\",\"Unsupervised Learning for Physical Interaction through Video Prediction\",\"High-Rank Matrix Completion and Clustering under Self-Expressive Models\",\"Learning a Probabilistic Latent Space of Object Shapes via 3D Generative-Adversarial Modeling\",\"Visual Dynamics: Probabilistic Future Frame Synthesis via Cross Convolutional Networks\",\"Human Decision-Making under Limited Time\",\"Incremental Boosting Convolutional Neural Network for Facial Action Unit Recognition\",\"Natural-Parameter Networks: A Class of Probabilistic Neural Networks\",\"Tree-Structured Reinforcement Learning for Sequential Object Localization\",\"Unsupervised Domain Adaptation with Residual Transfer Networks\",\"Verification Based Solution for Structured MAB Problems\",\"Minimizing Regret on Reflexive Banach Spaces and Nash Equilibria in Continuous Zero-Sum Games\",\"Linear dynamical neural population models through nonlinear embeddings\",\"SURGE: Surface Regularized Geometry Estimation from a Single Image\",\"Interpretable Distribution Features with Maximum Testing Power\",\"Sorting out typicality with the inverse moment matrix SOS polynomial\",\"Multi-armed Bandits: Competing with Optimal Sequences\",\"Multivariate tests of association based on univariate tests\",\"Learning What and Where to Draw\",\"The Sound of APALM Clapping: Faster Nonsmooth Nonconvex Optimization with Stochastic Asynchronous PALM\",\"Integrated perception with recurrent multi-task neural networks\",\"Learning from Small Sample Sets by Combining Unsupervised Meta-Training with CNNs\",\"CNNpack: Packing Convolutional Neural Networks in the Frequency Domain\",\"Cooperative Graphical Models\",\"f-GAN: Training Generative Neural Samplers using Variational Divergence Minimization\",\"Bayesian Optimization for Probabilistic Programs\",\"Hierarchical Question-Image Co-Attention for Visual Question Answering\",\"Optimal Sparse Linear Encoders and Sparse PCA\",\"FPNN: Field Probing Neural Networks for 3D Data\",\"CRF-CNN: Modeling Structured Information in Human Pose Estimation\",\"Fairness in Learning: Classic and Contextual Bandits\",\"Joint M-Best-Diverse Labelings as a Parametric Submodular Minimization\",\"Domain Separation Networks\",\"DISCO Nets : DISsimilarity COefficients Networks\",\"Multimodal Residual Learning for Visual QA\",\"CMA-ES with Optimal Covariance Update and Storage Complexity\",\"R-FCN: Object Detection via Region-based Fully Convolutional Networks\",\"GAP Safe Screening Rules for Sparse-Group Lasso\",\"Learning and Forecasting Opinion Dynamics in Social Networks\",\"Gradient-based Sampling: An Adaptive Importance Sampling for Least-squares\",\"Collaborative Recurrent Autoencoder: Recommend while Learning to Fill in the Blanks\",\"Mutual information for symmetric rank-one matrix estimation: A proof of the replica formula\",\"A Unified Approach for Learning the Parameters of Sum-Product Networks\",\"Training and Evaluating Multimodal Word Embeddings with Large-scale Web Annotated Images\",\"Stochastic Online AUC Maximization\",\"The Generalized Reparameterization Gradient\",\"Coupled Generative Adversarial Networks\",\"Exponential Family Embeddings\",\"Variational Information Maximization for Feature Selection\",\"Operator Variational Inference\",\"Fast learning rates with heavy-tailed losses\",\"Budgeted stream-based active learning via adaptive submodular maximization\",\"Learning feed-forward one-shot learners\",\"Learning User Perceived Clusters with Feature-Level Supervision\",\"Robust Spectral Detection of Global Structures in the Data by Learning a Regularization\",\"Residual Networks Behave Like Ensembles of Relatively Shallow Networks\",\"Adversarial Multiclass Classification: A Risk Minimization Perspective\",\"Solving Random Systems of Quadratic Equations via Truncated Generalized Gradient Flow\",\"Coin Betting and Parameter-Free Online Learning\",\"Deep Learning without Poor Local Minima\",\"Testing for Differences in Gaussian Graphical Models: Applications to Brain Connectivity\",\"A Constant-Factor Bi-Criteria Approximation Guarantee for k-means++\",\"Generating Videos with Scene Dynamics\",\"Neurally-Guided Procedural Models: Amortized Inference for Procedural Graphics Programs using Neural Networks\",\"A Powerful Generative Model Using Random Weights for the Deep Image Representation\",\"Optimizing affinity-based binary hashing using auxiliary coordinates\",\"Double Thompson Sampling for Dueling Bandits\",\"Generating Images with Perceptual Similarity Metrics based on Deep Networks\",\"Dynamic Filter Networks\",\"A Simple Practical Accelerated Method for Finite Sums\",\"Barzilai-Borwein Step Size for Stochastic Gradient Descent\",\"On Graph Reconstruction via Empirical Risk Minimization: Fast Learning Rates and Scalability\",\"Optimal spectral transportation with application to music transcription\",\"Regularized Nonlinear Acceleration\",\"SPALS: Fast Alternating Least Squares via Implicit Leverage Scores Sampling\",\"Single-Image Depth Perception in the Wild\",\"Computational and Statistical Tradeoffs in Learning to Rank\",\"Online Convex Optimization with Unconstrained Domains and Losses\",\"An ensemble diversity approach to supervised binary hashing\",\"Efficient Globally Convergent Stochastic Optimization for Canonical Correlation Analysis\",\"The Power of Adaptivity in Identifying Statistical Alternatives\",\"On Explore-Then-Commit strategies\",\"Sublinear Time Orthogonal Tensor Decomposition\",\"DECOrrelated feature space partitioning for distributed sparse regression\",\"Deep Alternative Neural Network: Exploring Contexts as Early as Possible for Action Recognition\",\"Dual Learning for Machine Translation\",\"Dialog-based Language Learning\",\"Joint Line Segmentation and Transcription for End-to-End Handwritten Paragraph Recognition\",\"Temporal Regularized Matrix Factorization for High-dimensional Time Series Prediction\",\"Active Nearest-Neighbor Learning in Metric Spaces\",\"Proximal Deep Structured Models\",\"Faster Projection-free Convex Optimization over the Spectrahedron\",\"Bayesian Optimization with a Finite Budget: An Approximate Dynamic Programming Approach\",\"SoundNet: Learning Sound Representations from Unlabeled Video\",\"Weight Normalization: A Simple Reparameterization to Accelerate Training of Deep Neural Networks\",\"Efficient Second Order Online Learning by Sketching\",\"Dynamic Mode Decomposition with Reproducing Kernels for Koopman Spectral Analysis\",\"Distributed Flexible Nonlinear Tensor Factorization\",\"The Robustness of Estimator Composition\",\"Efficient and Robust Spiking Neural Circuit for Navigation Inspired by Echolocating Bats\",\"PerforatedCNNs: Acceleration through Elimination of Redundant Convolutions\",\"Differential Privacy without Sensitivity\",\"Optimal Cluster Recovery in the Labeled Stochastic Block Model\",\"LazySVD: Even Faster SVD Decomposition Yet Without Agonizing Pain\",\"An algorithm for L1 nearest neighbor search via monotonic embedding\",\"Gaussian Process Bandit Optimisation with Multi-fidelity Evaluations\",\"Linear-Memory and Decomposition-Invariant Linearly Convergent Conditional Gradient Algorithm for Structured Polytopes\",\"Efficient Nonparametric Smoothness Estimation\",\"A Theoretically Grounded Application of Dropout in Recurrent Neural Networks\",\"Fast \\u03b5-free Inference of Simulation Models with Bayesian Conditional Density Estimation\",\"Direct Feedback Alignment Provides Learning in Deep Neural Networks\",\"Safe and Efficient Off-Policy Reinforcement Learning\",\"A Multi-Batch L-BFGS Method for Machine Learning\",\"Semiparametric Differential Graph Models\",\"R\\u00e9nyi Divergence Variational Inference\",\"Doubly Convolutional Neural Networks\",\"Density Estimation via Discrepancy Based Adaptive Sequential Partition\",\"How Deep is the Feature Analysis underlying Rapid Visual Categorization?\",\"VIME: Variational Information Maximizing Exploration\",\"Generalized Correspondence-LDA Models (GC-LDA) for Identifying Functional Regions in the Brain\",\"Solving Marginal MAP Problems with NP Oracles and Parity Constraints\",\"Multi-view Anomaly Detection via Robust Probabilistic Latent Variable Models\",\"Proximal Stochastic Methods for Nonsmooth Nonconvex Finite-Sum Optimization\",\"Variance Reduction in Stochastic Gradient Langevin Dynamics\",\"Regularization With Stochastic Transformations and Perturbations for Deep Semi-Supervised Learning\",\"Dense Associative Memory for Pattern Recognition\",\"Causal Bandits: Learning Good Interventions via Causal Inference\",\"Refined Lower Bounds for Adversarial Bandits\",\"Theoretical Comparisons of Positive-Unlabeled Learning against Positive-Negative Learning\",\"Homotopy Smoothing for Non-Smooth Problems with Lower Complexity than O(1/\\\\epsilon)\",\"Finite-Sample Analysis of Fixed-k Nearest Neighbor Density Functional Estimators\",\"A state-space model of cross-region dynamic connectivity in MEG/EEG\",\"What Makes Objects Similar: A Unified Multi-Metric Learning Approach\",\"Adaptive Maximization of Pointwise Submodular Functions With Budget Constraint\",\"Dueling Bandits: Beyond Condorcet Winners to General Tournament Solutions\",\"Local Similarity-Aware Deep Feature Embedding\",\"A Communication-Efficient Parallel Algorithm for Decision Tree\",\"Convex Two-Layer Modeling with Latent Structure\",\"Sampling for Bayesian Program Learning\",\"Learning Kernels with Random Features\",\"Optimal Tagging with Markov Chain Optimization\",\"Crowdsourced Clustering: Querying Edges vs Triangles\",\"Mixed vine copulas as joint models of spike counts and local field potentials\",\"Achieving the KS threshold in the general stochastic block model with linearized acyclic belief propagation\",\"Adaptive Concentration Inequalities for Sequential Decision Problems\",\"Nested Mini-Batch K-Means\",\"Deep Learning Models of the Retinal Response to Natural Scenes\",\"Preference Completion from Partial Rankings\",\"Dynamic Network Surgery for Efficient DNNs\",\"Learning a Metric Embedding for Face Recognition using the Multibatch Method\",\"A Pseudo-Bayesian Algorithm for Robust PCA\",\"End-to-End Kernel Learning with Supervised Convolutional Kernel Networks\",\"Stochastic Variance Reduction Methods for Saddle-Point Problems\",\"Flexible Models for Microclustering with Application to Entity Resolution\",\"Catching heuristics are optimal control policies\",\"Bayesian optimization under mixed constraints with a slack-variable augmented Lagrangian\",\"Adaptive Neural Compilation\",\"Synthesis of MCMC and Belief Propagation\",\"Learning Treewidth-Bounded Bayesian Networks with Thousands of Variables\",\"Unifying Count-Based Exploration and Intrinsic Motivation\",\"Large Margin Discriminant Dimensionality Reduction in Prediction Space\",\"Stochastic Structured Prediction under Bandit Feedback\",\"Simple and Efficient Weighted Minwise Hashing\",\"Truncated Variance Reduction: A Unified Approach to Bayesian Optimization and Level-Set Estimation\",\"Structured Sparse Regression via Greedy Hard Thresholding\",\"Understanding Probabilistic Sparse Gaussian Process Approximations\",\"SEBOOST - Boosting Stochastic Learning Using Subspace Optimization Techniques\",\"Generating Long-term Trajectories Using Deep Hierarchical Networks\",\"Learning Tree Structured Potential Games\",\"Observational-Interventional Priors for Dose-Response Learning\",\"Learning from Rational Behavior: Predicting Solutions to Unknown Linear Programs\",\"Identification and Overidentification of Linear Structural Equation Models\",\"Adaptive Skills Adaptive Partitions (ASAP)\",\"Multiple-Play Bandits in the Position-Based Model\",\"Optimal Black-Box Reductions Between Optimization Objectives\",\"On Valid Optimal Assignment Kernels and Applications to Graph Classification\",\"Robustness of classifiers: from adversarial to random noise\",\"A Non-convex One-Pass Framework for Generalized Factorization Machine and Rank-One Matrix Sensing\",\"Exploiting the Structure: Stochastic Gradient Methods Using Raw Clusters\",\"Combinatorial Multi-Armed Bandit with General Reward Functions\",\"Boosting with Abstention\",\"Regret of Queueing Bandits\",\"Deep Learning Games\",\"Globally Optimal Training of Generalized Polynomial Neural Networks with Nonlinear Spectral Methods\",\"Perspective Transformer Nets: Learning Single-View 3D Object Reconstruction without 3D Supervision\",\"A Credit Assignment Compiler for Joint Prediction\",\"Accelerating Stochastic Composition Optimization\",\"Reward Augmented Maximum Likelihood for Neural Structured Prediction\",\"Consistent Kernel Mean Estimation for Functions of Random Variables\",\"Towards Unifying Hamiltonian Monte Carlo and Slice Sampling\",\"Scalable Adaptive Stochastic Optimization Using Random Projections\",\"Variational Inference in Mixed Probabilistic Submodular Models\",\"Correlated-PCA: Principal Components' Analysis when Data and Noise are Correlated\",\"The Multi-fidelity Multi-armed Bandit\",\"Anchor-Free Correlated Topic Modeling: Identifiability and Algorithm\",\"Bootstrap Model Aggregation for Distributed Statistical Learning\",\"A scalable end-to-end Gaussian process adapter for irregularly sampled time series classification\",\"A Bandit Framework for Strategic Regression\",\"Architectural Complexity Measures of Recurrent Neural Networks\",\"Statistical Inference for Cluster Trees\",\"PAC Reinforcement Learning with Rich Observations\",\"Improved Deep Metric Learning with Multi-class N-pair Loss Objective\",\"Unsupervised Learning of Spoken Language with Visual Context\",\"Low-Rank Regression with Tensor Responses\",\"PAC-Bayesian Theory Meets Bayesian Inference\",\"Data Poisoning Attacks on Factorization-Based Collaborative Filtering\",\"Learned Region Sparsity and Diversity Also Predicts Visual Attention\",\"End-to-End Goal-Driven Web Navigation\",\"Automated scalable segmentation of neurons from multispectral images\",\"Privacy Odometers and Filters: Pay-as-you-Go Composition\",\"Minimax Estimation of Maximum Mean Discrepancy with Radial Kernels\",\"Adaptive optimal training of animal behavior\",\"Hierarchical Object Representation for Open-Ended Object Category Learning and Recognition\",\"Relevant sparse codes with variational information bottleneck\",\"Combinatorial Energy Learning for Image Segmentation\",\"Orthogonal Random Features\",\"Fast Active Set Methods for Online Spike Inference from Calcium Imaging\",\"Diffusion-Convolutional Neural Networks\",\"Bayesian latent structure discovery from multi-neuron recordings\",\"A Probabilistic Programming Approach To Probabilistic Data Analysis\",\"A Non-parametric Learning Method for Confidently Estimating Patient's Clinical State and Dynamics\",\"Inference by Reparameterization in Neural Population Codes\",\"Tensor Switching Networks\",\"Stochastic Gradient Richardson-Romberg Markov Chain Monte Carlo\",\"Coordinate-wise Power Method\",\"Learning Influence Functions from Incomplete Observations\",\"Learning Structured Sparsity in Deep Neural Networks\",\"Sample Complexity of Automated Mechanism Design\",\"Short-Dot: Computing Large Linear Transforms Distributedly Using Coded Short Dot Products\",\"Brains on Beats\",\"Learning Transferrable Representations for Unsupervised Domain Adaptation\",\"Stochastic Multiple Choice Learning for Training Diverse Deep Ensembles\",\"Active Learning from Imperfect Labelers\",\"Learning to Communicate with Deep Multi-Agent Reinforcement Learning\",\"Value Iteration Networks\",\"Blind Regression: Nonparametric Regression for Latent Variable Models via Collaborative Filtering\",\"On the Recursive Teaching Dimension of VC Classes\",\"InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets\",\"Hardness of Online Sleeping Combinatorial Optimization Problems\",\"Mixed Linear Regression with Multiple Components\",\"Sequential Neural Models with Stochastic Layers\",\"Stochastic Gradient Methods for Distributionally Robust Optimization with f-divergences\",\"Minimizing Quadratic Functions in Constant Time\",\"Improved Techniques for Training GANs\",\"DeepMath - Deep Sequence Models for Premise Selection\",\"Learning Multiagent Communication with Backpropagation\",\"Toward Deeper Understanding of Neural Networks: The Power of Initialization and a Dual View on Expressivity\",\"Learning the Number of Neurons in Deep Networks\",\"Finding significant combinations of features in the presence of categorical covariates\",\"Examples are not enough, learn to criticize! Criticism for Interpretability\",\"Optimistic Bandit Convex Optimization\",\"Safe Policy Improvement by Minimizing Robust Baseline Regret\",\"Graphons, mergeons, and so on!\",\"Hierarchical Clustering via Spreading Metrics\",\"Learning Bayesian networks with ancestral constraints\",\"Pruning Random Forests for Prediction on a Budget\",\"Clustering with Bregman Divergences: an Asymptotic Analysis\",\"Variational Autoencoder for Deep Learning of Images, Labels and Captions\",\"Review Networks for Caption Generation\",\"Stein Variational Gradient Descent: A General Purpose Bayesian Inference Algorithm\",\"A Bio-inspired Redundant Sensing Architecture\",\"Contextual semibandits via supervised learning oracles\",\"Blind Attacks on Machine Learners\",\"Universal Correspondence Network\",\"Satisfying Real-world Goals with Dataset Constraints\",\"Deep Learning for Predicting Human Strategic Behavior\",\"Phased Exploration with Greedy Exploitation in Stochastic Combinatorial Partial Monitoring Games\",\"Eliciting Categorical Data for Optimal Aggregation\",\"Measuring the reliability of MCMC inference with bidirectional Monte Carlo\",\"Breaking the Bandwidth Barrier: Geometrical Adaptive Entropy Estimation\",\"Selective inference for group-sparse linear models\",\"Graph Clustering: Block-models and model free results\",\"Maximizing Influence in an Ising Network: A Mean-Field Optimal Solution\",\"Hypothesis Testing in Unsupervised Domain Adaptation with Applications in Alzheimer's Disease\",\"Geometric Dirichlet Means Algorithm for topic inference\",\"Structured Prediction Theory Based on Factor Graph Complexity\",\"Improved Dropout for Shallow and Deep Learning\",\"Constraints Based Convex Belief Propagation\",\"Error Analysis of Generalized Nystr\\u00f6m Kernel Regression\",\"A Probabilistic Framework for Deep Learning\",\"General Tensor Spectral Co-clustering for Higher-Order Data\",\"Cyclades: Conflict-free Asynchronous Machine Learning\",\"Single Pass PCA of Matrix Products\",\"Stochastic Variational Deep Kernel Learning\",\"Interaction Screening: Efficient and Sample-Optimal Learning of Ising Models\",\"Long-term Causal Effects via Behavioral Game Theory\",\"Measuring Neural Net Robustness with Constraints\",\"Reshaped Wirtinger Flow for Solving Quadratic System of Equations\",\"Nearly Isometric Embedding by Relaxation\",\"Probabilistic Inference with Generating Functions for Poisson Latent Variable Models\",\"Causal meets Submodular: Subset Selection with Directed Information\",\"Depth from a Single Image by Harmonizing Overcomplete Local Network Predictions\",\"Deep Neural Networks with Inexact Matching for Person Re-Identification\",\"Global Analysis of Expectation Maximization for Mixtures of Two Gaussians\",\"Estimating the class prior and posterior from noisy positives and unlabeled data\",\"Kronecker Determinantal Point Processes\",\"Finite Sample Prediction and Recovery Bounds for Ordinal Embedding\",\"Feature-distributed sparse regression: a screen-and-clean approach\",\"Learning Bound for Parameter Transfer Learning\",\"Learning under uncertainty: a comparison between R-W and Bayesian approach\",\"Bi-Objective Online Matching and Submodular Allocations\",\"Quantized Random Projections and Non-Linear Estimation of Cosine Similarity\",\"The non-convex Burer-Monteiro approach works on smooth semidefinite programs\",\"Dimensionality Reduction of Massive Sparse Datasets Using Coresets\",\"Using Social Dynamics to Make Individual Predictions: Variational Inference with a Stochastic Kinetic Model\",\"Supervised learning through the lens of compression\",\"Generative Shape Models: Joint Text Recognition and Segmentation with Very Little Training Data\",\"Image Restoration Using Very Deep Convolutional Encoder-Decoder Networks with Symmetric Skip Connections\",\"Object based Scene Representations using Fisher Scores of Local Subspace Projections\",\"Active Learning with Oracle Epiphany\",\"Statistical Inference for Pairwise Graphical Models Using Score Matching\",\"Improved Error Bounds for Tree Representations of Metric Spaces\",\"Can Peripheral Representations Improve Clutter Metrics on Complex Scenes?\",\"On Multiplicative Integration with Recurrent Neural Networks\",\"Learning HMMs with Nonparametric Emissions via Spectral Decompositions of Continuous Matrices\",\"Regret Bounds for Non-decomposable Metrics with Missing Labels\",\"Robust k-means: a Theoretical Revisit\",\"Bayesian optimization for automated model selection\",\"A Probabilistic Model of Social Decision Making based on Reward Maximization\",\"Balancing Suspense and Surprise: Timely Decision Making with Endogenous Information Acquisition\",\"Fast and Flexible Monotonic Functions with Ensembles of Lattices\",\"Conditional Generative Moment-Matching Networks\",\"Stochastic Gradient MCMC with Stale Gradients\",\"Composing graphical models with neural networks for structured representations and fast inference\",\"Noise-Tolerant Life-Long Matrix Completion via Adaptive Sampling\",\"Combinatorial semi-bandit with known covariance\",\"Matrix Completion has No Spurious Local Minimum\",\"The Multiscale Laplacian Graph Kernel\",\"Adaptive Averaging in Accelerated Descent Dynamics\",\"Sub-sampled Newton Methods with Non-uniform Sampling\",\"Stochastic Gradient Geodesic MCMC Methods\",\"Variational Bayes on Monte Carlo Steroids\",\"Showing versus doing: Teaching by demonstration\",\"Combining Fully Convolutional and Recurrent Neural Networks for 3D Biomedical Image Segmentation\",\"Maximization of Approximately Submodular Functions\",\"A Comprehensive Linear Speedup Analysis for Asynchronous Stochastic Parallel Optimization from Zeroth-Order to First-Order\",\"Learning Infinite RBMs with Frank-Wolfe\",\"Estimating the Size of a Large Network and its Communities from a Random Sample\",\"Learning Sensor Multiplexing Design through Back-propagation\",\"On Robustness of Kernel Clustering\",\"High resolution neural connectivity from incomplete tracing data using nonnegative spline regression\",\"MoCap-guided Data Augmentation for 3D Pose Estimation in the Wild\",\"New Liftable Classes for First-Order Probabilistic Inference\",\"The Parallel Knowledge Gradient Method for Batch Bayesian Optimization\",\"Improved Regret Bounds for Oracle-Based Adversarial Contextual Bandits\",\"Consistent Estimation of Functions of Data Missing Non-Monotonically and Not at Random\",\"Optimistic Gittins Indices\",\"Finite-Dimensional BFRY Priors and Variational Bayesian Inference for Power Law Models\",\"Launch and Iterate: Reducing Prediction Churn\",\"\\u201cCongruent\\u201d and \\u201cOpposite\\u201d Neurons: Sisters for Multisensory Integration and Segregation\",\"Learning shape correspondence with anisotropic convolutional neural networks\",\"Pairwise Choice Markov Chains\",\"NESTT: A Nonconvex Primal-Dual Splitting Method for Distributed and Stochastic Optimization\",\"Clustering with Same-Cluster Queries\",\"Attend, Infer, Repeat: Fast Scene Understanding with Generative Models\",\"Parameter Learning for Log-supermodular Distributions\",\"Deconvolving Feedback Loops in Recommender Systems\",\"Structured Matrix Recovery via the Generalized Dantzig Selector\",\"Confusions over Time: An Interpretable Bayesian Model to Characterize Trends in Decision Making\",\"Automatic Neuron Detection in Calcium Imaging Data Using Convolutional Networks\",\"Designing smoothing functions for improved worst-case competitive ratio in online optimization\",\"Convergence guarantees for kernel-based quadrature rules in misspecified settings\",\"Unsupervised Learning from Noisy Networks with Applications to Hi-C Data\",\"A Non-generative Framework and Convex Relaxations for Unsupervised Learning\",\"Equality of Opportunity in Supervised Learning\",\"Scaled Least Squares Estimator for GLMs in Large-Scale Problems\",\"Interpretable Nonlinear Dynamic Modeling of Neural Trajectories\",\"Search Improves Label for Active Learning\",\"Higher-Order Factorization Machines\",\"Exponential expressivity in deep neural networks through transient chaos\",\"Split LBI: An Iterative Regularization Path with Structural Sparsity\",\"An equivalence between high dimensional Bayes optimal inference and M-estimation\",\"Synthesizing the preferred inputs for neurons in neural networks via deep generator networks\",\"Deep Submodular Functions: Definitions and Learning\",\"Discriminative Gaifman Models\",\"Leveraging Sparsity for Efficient Submodular Data Summarization\",\"Local Minimax Complexity of Stochastic Convex Optimization\",\"Stochastic Optimization for Large-scale Optimal Transport\",\"On Mixtures of Markov Chains\",\"Linear Contextual Bandits with Knapsacks\",\"Reconstructing Parameters of Spreading Models from Partial Observations\",\"Spatiotemporal Residual Networks for Video Action Recognition\",\"Path-Normalized Optimization of Recurrent Neural Networks with ReLU Activations\",\"Strategic Attentive Writer for Learning Macro-Actions\",\"The Limits of Learning with Missing Data\",\"RETAIN: An Interpretable Predictive Model for Healthcare using Reverse Time Attention Mechanism\",\"Total Variation Classes Beyond 1d: Minimax Rates, and the Limitations of Linear Smoothers\",\"Community Detection on Evolving Graphs\",\"Online and Differentially-Private Tensor Decomposition\",\"Dimension-Free Iteration Complexity of Finite Sum Optimization Problems\",\"Towards Conceptual Compression\",\"Exact Recovery of Hard Thresholding Pursuit\",\"Data Programming: Creating Large Training Sets, Quickly\",\"Generalization of ERM in Stochastic Convex Optimization: The Dimension Strikes Back\",\"Dynamic matrix recovery from incomplete observations under an exact low-rank constraint\",\"Fast Distributed Submodular Cover: Public-Private Data Summarization\",\"Estimating Nonlinear Neural Response Functions using GP Priors and Kronecker Methods\",\"Lifelong Learning with Weighted Majority Votes\",\"Scaling Memory-Augmented Neural Networks with Sparse Reads and Writes\",\"Matching Networks for One Shot Learning\",\"Tight Complexity Bounds for Optimizing Composite Objectives\",\"Graphical Time Warping for Joint Alignment of Multiple Curves\",\"Unsupervised Risk Estimation Using Only Conditional Independence Structure\",\"MetaGrad: Multiple Learning Rates in Online Learning\",\"Hierarchical Deep Reinforcement Learning: Integrating Temporal Abstraction and Intrinsic Motivation\",\"High Dimensional Structured Superposition Models\",\"Joint quantile regression in vector-valued RKHSs\",\"The Forget-me-not Process\",\"Wasserstein Training of Restricted Boltzmann Machines\",\"Communication-Optimal Distributed Clustering\",\"Probing the Compositionality of Intuitive Functions\",\"Ladder Variational Autoencoders\",\"The Multiple Quantile Graphical Model\",\"Threshold Learning for Optimal Decision Making\",\"Unsupervised Feature Extraction by Time-Contrastive Learning and Nonlinear ICA\",\"Can Active Memory Replace Attention?\",\"Minimax Optimal Alternating Minimization for Kernel Nonparametric Tensor Learning\",\"The Product Cut\",\"Learning Sparse Gaussian Graphical Models with Overlapping Blocks\",\"Yggdrasil: An Optimized System for Training Deep Decision Trees at Scale\",\"Average-case hardness of RIP certification\",\"A forward model at Purkinje cell synapses facilitates cerebellar anticipatory control\",\"Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering\",\"CliqueCNN: Deep Unsupervised Exemplar Learning\",\"Large-Scale Price Optimization via Network Flow\",\"Online Pricing with Strategic and Patient Buyers\",\"Global Optimality of Local Search for Low Rank Matrix Recovery\",\"Phased LSTM: Accelerating Recurrent Network Training for Long or Event-based Sequences\",\"Improving PAC Exploration Using the Median Of Means\",\"Infinite Hidden Semi-Markov Modulated Interaction Point Process\",\"Cooperative Inverse Reinforcement Learning\",\"Spatio-Temporal Hilbert Maps for Continuous Occupancy Representation in Dynamic Environments\",\"Select-and-Sample for Spike-and-Slab Sparse Coding\",\"Tractable Operations for Arithmetic Circuits of Probabilistic Models\",\"Greedy Feature Construction\",\"Mistake Bounds for Binary Matrix Completion\",\"Data driven estimation of Laplace-Beltrami operator\",\"Tracking the Best Expert in Non-stationary Stochastic Environments\",\"Learning to learn by gradient descent by gradient descent\",\"Kernel Observers: Systems-Theoretic Modeling and Inference of Spatiotemporally Evolving Processes\",\"Quantum Perceptron Models\",\"Guided Policy Search via Approximate Mirror Descent\",\"The Power of Optimization from Samples\",\"Deep Exploration via Bootstrapped DQN\",\"A Multi-step Inertial Forward-Backward Splitting Method for Non-convex Optimization\",\"Scaling Factorial Hidden Markov Models: Stochastic Variational Inference without Messages\",\"Convolutional Neural Fabrics\",\"Adaptive Newton Method for Empirical Risk Minimization to Statistical Accuracy\",\"A Sparse Interactive Model for Matrix Completion with Side Information\",\"Coresets for Scalable Bayesian Logistic Regression\",\"Agnostic Estimation for Misspecified Phase Retrieval Models\",\"Linear Relaxations for Finding Diverse Elements in Metric Spaces\",\"Binarized Neural Networks\",\"Local Maxima in the Likelihood of Gaussian Mixture Models: Structural Results and Algorithmic Consequences\",\"Memory-Efficient Backpropagation Through Time\",\"Bayesian Optimization with Robust Bayesian Neural Networks\",\"Learnable Visual Markers\",\"Fast Algorithms for Robust PCA via Gradient Descent\",\"One-vs-Each Approximation to Softmax for Scalable Estimation of Probabilities\",\"Learning Deep Embeddings with Histogram Loss\",\"Spectral Learning of Dynamic Systems from Nonequilibrium Data\",\"Fast Mixing Markov Chains for Strongly Rayleigh Measures, DPPs, and Constrained Sampling\",\"Mapping Estimation for Discrete Optimal Transport\",\"Batched Gaussian Process Bandit Optimization via Determinantal Point Processes\",\"Protein contact prediction from amino acid co-evolution using convolutional networks for graph-valued images\",\"Linear Feature Encoding for Reinforcement Learning\",\"A Minimax Approach to Supervised Learning\",\"Edge-exchangeable graphs and sparsity\",\"A Locally Adaptive Normal Distribution\",\"Completely random measures for modelling block-structured sparse networks\",\"Sparse Support Recovery with Non-smooth Loss Functions\",\"Neurons Equipped with Intrinsic Plasticity Learn Stimulus Intensity Statistics\",\"Learning values across many orders of magnitude\",\"Adaptive Smoothed Online Multi-Task Learning\",\"Safe Exploration in Finite Markov Decision Processes with Gaussian Processes\",\"Probabilistic Linear Multistep Methods\",\"Stochastic Three-Composite Convex Minimization\",\"Using Fast Weights to Attend to the Recent Past\",\"Maximal Sparsity with Deep Networks?\",\"Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings\",\"beta-risk: a New Surrogate Risk for Learning from Weakly Labeled Data\",\"Learning Additive Exponential Family Graphical Models via \\\\ell_{2,1}-norm Regularized M-Estimation\",\"Backprop KF: Learning Discriminative Deterministic State Estimators\",\"LightRNN: Memory and Computation-Efficient Recurrent Neural Networks\",\"Fast recovery from a union of subspaces\",\"Incremental Variational Sparse Gaussian Process Regression\",\"A Consistent Regularization Approach for Structured Prediction\",\"Clustering Signed Networks with the Geometric Mean of Laplacians\",\"An urn model for majority voting in classification ensembles\",\"Avoiding Imposters and Delinquents: Adversarial Crowdsourcing and Peer Prediction\",\"Fast and accurate spike sorting of high-channel count probes with KiloSort\",\"Combining Adversarial Guarantees and Stochastic Fast Rates in Online Learning\",\"Ancestral Causal Inference\",\"More Supervision, Less Computation: Statistical-Computational Tradeoffs in Weakly Supervised Learning\",\"Tagger: Deep Unsupervised Perceptual Grouping\",\"An Efficient Streaming Algorithm for the Submodular Cover Problem\",\"Interaction Networks for Learning about Objects, Relations and Physics\",\"Efficient state-space modularization for planning: theory, behavioral and neural signatures\",\"Provable Efficient Online Matrix Completion via Non-convex Stochastic Gradient Descent\",\"Online Bayesian Moment Matching for Topic Modeling with Unknown Number of Topics\",\"Computing and maximizing influence in linear threshold and triggering models\",\"Coevolutionary Latent Feature Processes for Continuous-Time User-Item Interactions\",\"Optimal Learning for Multi-pass Stochastic Gradient Methods\",\"Generative Adversarial Imitation Learning\",\"Latent Attention For If-Then Program Synthesis\",\"Dual Space Gradient Descent for Online Learning\",\"Riemannian SVRG: Fast Stochastic Optimization on Riemannian Manifolds\",\"Professor Forcing: A New Algorithm for Training Recurrent Networks\",\"Learning brain regions via large-scale online structured sparse dictionary learning\",\"Efficient Neural Codes under Metabolic Constraints\",\"Approximate maximum entropy principles via Goemans-Williamson with applications to provable variational methods\",\"Efficient High-Order Interaction-Aware Feature Selection Based on Conditional Mutual Information\",\"Bayesian Intermittent Demand Forecasting for Large Inventories\",\"Visual Question Answering with Question Representation Update (QRU)\",\"Learning Parametric Sparse Models for Image Super-Resolution\",\"Blazing the trails before beating the path: Sample-efficient Monte-Carlo planning\",\"Asynchronous Parallel Greedy Coordinate Descent\",\"Iterative Refinement of the Approximate Posterior for Directed Belief Networks\",\"Assortment Optimization Under the Mallows model\",\"Disease Trajectory Maps\",\"Multistage Campaigning in Social Networks\",\"Learning in Games: Robustness of Fast Convergence\",\"Improved Variational Inference with Inverse Autoregressive Flow\",\"Algorithms and matching lower bounds for approximately-convex optimization\",\"Unified Methods for Exploiting Piecewise Linear Structure in Convex Optimization\",\"Kernel Bayesian Inference with Posterior Regularization\",\"Neural Universal Discrete Denoiser\",\"Optimal Architectures in a Solvable Model of Deep Networks\",\"Conditional Image Generation with PixelCNN Decoders\",\"Supervised Learning with Tensor Networks\",\"Multi-step learning and underlying structure in statistical models\",\"Structure-Blind Signal Recovery\",\"An Architecture for Deep, Hierarchical Generative Models\",\"Feature selection in functional data classification with recursive maxima hunting\",\"Achieving budget-optimality with adaptive schemes in crowdsourcing\",\"Near-Optimal Smoothing of Structured Conditional Probability Matrices\",\"Supervised Word Mover's Distance\",\"Exploiting Tradeoffs for Exact Recovery in Heterogeneous Stochastic Block Models\",\"Full-Capacity Unitary Recurrent Neural Networks\",\"Threshold Bandits, With and Without Censored Feedback\",\"Understanding the Effective Receptive Field in Deep Convolutional Neural Networks\",\"Learning Supervised PageRank with Gradient-Based and Gradient-Free Optimization Methods\",\"k*-Nearest Neighbors: From Global to Local\",\"Normalized Spectral Map Synchronization\",\"Beyond Exchangeability: The Chinese Voting Process\",\"A posteriori error bounds for joint matrix decomposition problems\",\"A Bayesian method for reducing bias in neural representational similarity analysis\",\"Online ICA: Understanding Global Dynamics of Nonconvex Optimization via Diffusion Processes\",\"Following the Leader and Fast Rates in Linear Prediction: Curved Constraint Sets and Other Regularities\",\"SDP Relaxation with Randomized Rounding for Energy Disaggregation\",\"Recovery Guarantee of Non-negative Matrix Factorization via Alternating Updates\",\"Unsupervised Learning of 3D Structure from Images\",\"Poisson-Gamma dynamical systems\",\"Gaussian Processes for Survival Analysis\",\"Dual Decomposed Learning with Factorwise Oracle for Structural SVM of Large Output Domain\",\"Optimal Binary Classifier Aggregation for General Losses\",\"Disentangling factors of variation in deep representation using adversarial training\",\"A primal-dual method for conic constrained distributed optimization problems\",\"Fundamental Limits of Budget-Fidelity Trade-off in Label Crowdsourcing\",\"An Online Sequence-to-Sequence Model Using Partial Conditioning\",\"Learning to Poke by Poking: Experiential Learning of Intuitive Physics\",\"Learning Deep Parsimonious Representations\",\"Only H is left: Near-tight Episodic PAC RL\",\"Double or Nothing: Multiplicative Incentive Mechanisms for Crowdsourcing\",\"Learning with Symmetric Label Noise: The Importance of Being Unhinged\",\"Algorithmic Stability and Uniform Generalization\",\"Adaptive Low-Complexity Sequential Inference for Dirichlet Process Mixture Models\",\"Covariance-Controlled Adaptive Langevin Thermostat for Large-Scale Bayesian Sampling\",\"Robust Portfolio Optimization\",\"Logarithmic Time Online Multiclass prediction\",\"Planar Ultrametrics for Image Segmentation\",\"Expressing an Image Stream with a Sequence of Natural Sentences\",\"Parallel Correlation Clustering on Big Graphs\",\"Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks\",\"Space-Time Local Embeddings\",\"A Convergent Gradient Descent Algorithm for Rank Minimization and Semidefinite Programming from Random Linear Measurements\",\"Smooth Interactive Submodular Set Cover\",\"Galileo: Perceiving Physical Object Properties by Integrating a Physics Engine with Deep Learning\",\"On the Pseudo-Dimension of Nearly Optimal Auctions\",\"Unlocking neural population non-stationarities using hierarchical dynamics models\",\"Bayesian Manifold Learning: The Locally Linear Latent Variable Model (LL-LVM)\",\"Color Constancy by Learning to Predict Chromaticity from Luminance\",\"Fast and Accurate Inference of Plackett\\u2013Luce Models\",\"Probabilistic Line Searches for Stochastic Optimization\",\"Inferring Algorithmic Patterns with Stack-Augmented Recurrent Nets\",\"Where are they looking?\",\"The Pareto Regret Frontier for Bandits\",\"On the Limitation of Spectral Methods: From the Gaussian Hidden Clique Problem to Rank-One Perturbations of Gaussian Tensors\",\"Measuring Sample Quality with Stein's Method\",\"Bidirectional Recurrent Convolutional Networks for Multi-Frame Super-Resolution\",\"Bounding errors of Expectation-Propagation\",\"A fast, universal algorithm to learn parametric nonlinear embeddings\",\"Texture Synthesis Using Convolutional Neural Networks\",\"Extending Gossip Algorithms to Distributed Estimation of U-statistics\",\"Streaming, Distributed Variational Inference for Bayesian Nonparametrics\",\"Learning visual biases from human imagination\",\"Smooth and Strong: MAP Inference with Linear Convergence\",\"Copeland Dueling Bandits\",\"Optimal Ridge Detection using Coverage Risk\",\"Top-k Multiclass SVM\",\"Policy Evaluation Using the \\u03a9-Return\",\"Orthogonal NMF through Subspace Exploration\",\"Stochastic Online Greedy Learning with Semi-bandit Feedbacks\",\"Deeply Learning the Messages in Message Passing Inference\",\"Synaptic Sampling: A Bayesian Approach to Neural Network Plasticity and Rewiring\",\"Accelerated Proximal Gradient Methods for Nonconvex Programming\",\"Approximating Sparse PCA from Incomplete Data\",\"Nonparametric von Mises Estimators for Entropies, Divergences and Mutual Informations\",\"Column Selection via Adaptive Sampling\",\"HONOR: Hybrid Optimization for NOn-convex Regularized problems\",\"3D Object Proposals for Accurate Object Class Detection\",\"Algorithms with Logarithmic or Sublinear Regret for Constrained Contextual Bandits\",\"Tensorizing Neural Networks\",\"Parallelizing MCMC with Random Partition Trees\",\"A Reduced-Dimension fMRI Shared Response Model\",\"Spectral Learning of Large Structured HMMs for Comparative Epigenomics\",\"Individual Planning in Infinite-Horizon Multiagent Settings: Inference, Structure and Scalability\",\"Estimating Mixture Models via Mixtures of Polynomials\",\"On the Global Linear Convergence of Frank-Wolfe Optimization Variants\",\"Deep Knowledge Tracing\",\"Rethinking LDA: Moment Matching for Discrete ICA\",\"Efficient Compressive Phase Retrieval with Constrained Sensing Vectors\",\"Barrier Frank-Wolfe for Marginal Inference\",\"Learning Theory and Algorithms for Forecasting Non-stationary Time Series\",\"Compressive spectral embedding: sidestepping the SVD\",\"A Nonconvex Optimization Framework for Low Rank Matrix Estimation\",\"Automatic Variational Inference in Stan\",\"Attention-Based Models for Speech Recognition\",\"Closed-form Estimators for High-dimensional Generalized Linear Models\",\"Online F-Measure Optimization\",\"Online Rank Elicitation for Plackett-Luce: A Dueling Bandits Approach\",\"M-Best-Diverse Labelings for Submodular Energies and Beyond\",\"Tractable Bayesian Network Structure Learning with Bounded Vertex Cover Number\",\"Learning Large-Scale Poisson DAG Models based on OverDispersion Scoring\",\"Training Restricted Boltzmann Machine via the \\ufffcThouless-Anderson-Palmer free energy\",\"Character-level Convolutional Networks for Text Classification\",\"Robust Feature-Sample Linear Discriminant Analysis for Brain Disorders Diagnosis\",\"Black-box optimization of noisy functions with unknown smoothness\",\"Recovering Communities in the General Stochastic Block Model Without Knowing the Parameters\",\"Deep learning with Elastic Averaging SGD\",\"Monotone k-Submodular Function Maximization with Size Constraints\",\"Active Learning from Weak and Strong Labelers\",\"On the Optimality of Classifier Chain for Multi-label Classification\",\"Robust Regression via Hard Thresholding\",\"Sparse Local Embeddings for Extreme Multi-label Classification\",\"Solving Random Quadratic Systems of Equations Is Nearly as Easy as Solving Linear Systems\",\"A Framework for Individualizing Predictions of Disease Trajectories by Exploiting Multi-Resolution Structure\",\"Subspace Clustering with Irrelevant Features via Robust Dantzig Selector\",\"Sparse PCA via Bipartite Matchings\",\"Fast Randomized Kernel Ridge Regression with Statistical Guarantees\",\"Online Learning for Adversaries with Memory: Price of Past Mistakes\",\"Convolutional spike-triggered covariance analysis for neural subunit models\",\"Convolutional LSTM Network: A Machine Learning Approach for Precipitation Nowcasting\",\"GAP Safe screening rules for sparse multi-task and multi-class models\",\"Empirical Localization of Homogeneous Divergences on Discrete Sample Spaces\",\"Statistical Model Criticism using Kernel Two Sample Tests\",\"Precision-Recall-Gain Curves: PR Analysis Done Right\",\"A Generalization of Submodular Cover via the Diminishing Return Property on the Integer Lattice\",\"Bidirectional Recurrent Neural Networks as Generative Models\",\"Quartz: Randomized Dual Coordinate Ascent with Arbitrary Sampling\",\"Maximum Likelihood Learning With Arbitrary Treewidth via Fast-Mixing Parameter Sets\",\"Hessian-free Optimization for Learning Deep Multidimensional Recurrent Neural Networks\",\"Large-scale probabilistic predictors with and without guarantees of validity\",\"Shepard Convolutional Neural Networks\",\"Matrix Manifold Optimization for Gaussian Mixtures\",\"Semi-supervised Convolutional Neural Networks for Text Categorization via Region Embedding\",\"Parallel Recursive Best-First AND/OR Search for Exact MAP Inference in Graphical Models\",\"Convolutional Neural Networks with Intra-Layer Recurrent Connections for Scene Labeling\",\"Bounding the Cost of Search-Based Lifted Inference\",\"Gradient-free Hamiltonian Monte Carlo with Efficient Kernel Exponential Families\",\"Linear Multi-Resource Allocation with Semi-Bandit Feedback\",\"Unsupervised Learning by Program Synthesis\",\"Enforcing balance allows local supervised learning in spiking recurrent networks\",\"Fast and Guaranteed Tensor Decomposition via Sketching\",\"Differentially private subspace clustering\",\"Predtron: A Family of Online Algorithms for General Prediction Problems\",\"Weighted Theta Functions and Embeddings with Applications to Max-Cut, Clustering and Summarization\",\"SGD Algorithms based on Incomplete U-statistics: Large-Scale Minimization of Empirical Risk\",\"On Top-k Selection in Multi-Armed Bandits and Hidden Bipartite Graphs\",\"The Brain Uses Reliability of Stimulus Information when Making Perceptual Decisions\",\"Fast Classification Rates for High-dimensional Gaussian Generative Models\",\"Fast Distributed k-Center Clustering with Outliers on Massive Data\",\"Human Memory Search as Initial-Visit Emitting Random Walk\",\"Non-convex Statistical Optimization for Sparse Tensor Graphical Model\",\"Convergence Rates of Active Learning for Maximum Likelihood Estimation\",\"Weakly-supervised Disentangling with Recurrent Transformations for 3D View Synthesis\",\"Efficient Exact Gradient Update for training Deep Networks with Very Large Sparse Targets\",\"Backpropagation for Energy-Efficient Neuromorphic Computing\",\"Alternating Minimization for Regression Problems with Vector-valued Outputs\",\"Learning both Weights and Connections for Efficient Neural Network\",\"Optimal Rates for Random Fourier Features\",\"The Population Posterior and Bayesian Modeling on Streams\",\"Frank-Wolfe Bayesian Quadrature: Probabilistic Integration with Theoretical Guarantees\",\"Scheduled Sampling for Sequence Prediction with Recurrent Neural Networks\",\"Unified View of Matrix Completion under General Structural Constraints\",\"Efficient Output Kernel Learning for Multiple Tasks\",\"Scalable Adaptation of State Complexity for Nonparametric Hidden Markov Models\",\"Variational Consensus Monte Carlo\",\"Newton-Stein Method: A Second Order Method for GLMs via Stein's Lemma\",\"Practical and Optimal LSH for Angular Distance\",\"Learning to Linearize Under Uncertainty\",\"Finite-Time Analysis of Projected Langevin Monte Carlo\",\"Deep Visual Analogy-Making\",\"Matrix Completion from Fewer Entries: Spectral Detectability and Rank Estimation\",\"Online Learning with Adversarial Delays\",\"Multi-Layer Feature Reduction for Tree Structured Group Lasso via Hierarchical Projection\",\"Minimum Weight Perfect Matching via Blossom Belief Propagation\",\"Efficient Thompson Sampling for Online \\ufffcMatrix-Factorization Recommendation\",\"Improved Iteration Complexity Bounds of Cyclic Block Coordinate Descent for Convex Problems\",\"Lifted Symmetry Detection and Breaking for MAP Inference\",\"Evaluating the statistical significance of biclusters\",\"Discriminative Robust Transformation Learning\",\"Bandits with Unobserved Confounders: A Causal Approach\",\"Scalable Semi-Supervised Aggregation of Classifiers\",\"Online Learning with Gaussian Payoffs and Side Observations\",\"Private Graphon Estimation for Sparse Graphs\",\"SubmodBoxes: Near-Optimal Search for a Set of Diverse Object Proposals\",\"Fast Second Order Stochastic Backpropagation for Variational Inference\",\"Randomized Block Krylov Methods for Stronger and Faster Approximate Singular Value Decomposition\",\"Cross-Domain Matching for Bag-of-Words Data via Kernel Embeddings of Latent Distributions\",\"Scalable Inference for Gaussian Process Models with Black-Box Likelihoods\",\"Fast Bidirectional Probability Estimation in Markov Models\",\"Probabilistic Variational Bounds for Graphical Models\",\"Linear Response Methods for Accurate Covariance Estimates from Mean Field Variational Bayes\",\"Combinatorial Cascading Bandits\",\"Mixing Time Estimation in Reversible Markov Chains from a Single Sample Path\",\"Policy Gradient for Coherent Risk Measures\",\"Fast Rates for Exp-concave Empirical Risk Minimization\",\"Deep Generative Image Models using a \\ufffcLaplacian Pyramid of Adversarial Networks\",\"Decoupled Deep Neural Network for Semi-supervised Semantic Segmentation\",\"Equilibrated adaptive learning rates for non-convex optimization\",\"BACKSHIFT: Learning causal cyclic graphs from unknown shift interventions\",\"Risk-Sensitive and Robust Decision-Making: a CVaR Optimization Approach\",\"Asynchronous stochastic convex optimization: the noise is in the noise and SGD don't care\",\"Lifelong Learning with Non-i.i.d. Tasks\",\"Optimal Linear Estimation under Unknown Nonlinear Transform\",\"Learning with Group Invariant Features: A Kernel Perspective.\",\"Regularized EM Algorithms: A Unified Framework and Statistical Guarantees\",\"Distributionally Robust Logistic Regression\",\"Adaptive Stochastic Optimization: From Sets to Paths\",\"Beyond Convexity: Stochastic Quasi-Convex Optimization\",\"A Tractable Approximation to Optimal Point Process Filtering: Application to Neural Encoding\",\"Sum-of-Squares Lower Bounds for Sparse PCA\",\"Max-Margin Majority Voting for Learning from Crowds\",\"Learning with Incremental Iterative Regularization\",\"Halting in Random Walk Kernels\",\"MCMC for Variationally Sparse Gaussian Processes\",\"Less is More: Nystr\\u00f6m Computational Regularization\",\"Infinite Factorial Dynamical Model\",\"Regularization Path of Cross-Validation Error Lower Bounds\",\"Attractor Network Dynamics Enable Preplay and Rapid Path Planning in Maze\\u2013like Environments\",\"Teaching Machines to Read and Comprehend\",\"Principal Differences Analysis: Interpretable Characterization of Differences between Distributions\",\"When are Kalman-Filter Restless Bandits Indexable?\",\"Segregated Graphs and Marginals of Chain Graph Models\",\"Efficient Non-greedy Optimization of Decision Trees\",\"Probabilistic Curve Learning: Coulomb Repulsion and the Electrostatic Gaussian Process\",\"Inverse Reinforcement Learning with Locally Consistent Reward Functions\",\"Communication Complexity of Distributed Convex Learning and Optimization\",\"End-to-end Learning of LDA by Mirror-Descent Back Propagation over a Deep Architecture\",\"Subset Selection by Pareto Optimization\",\"On the Accuracy of Self-Normalized Log-Linear Models\",\"Regret Lower Bound and Optimal Algorithm in Finite Stochastic Partial Monitoring\",\"Is Approval Voting Optimal Given Approval Votes?\",\"Regressive Virtual Metric Learning\",\"Analysis of Robust PCA via Local Incoherence\",\"Learning to Transduce with Unbounded Memory\",\"Max-Margin Deep Generative Models\",\"Spherical Random Features for Polynomial Kernels\",\"Rectified Factor Networks\",\"Learning Bayesian Networks with Thousands of Variables\",\"Matrix Completion Under Monotonic Single Index Models\",\"Visalogy: Answering Visual Analogy Questions\",\"Tree-Guided MCMC Inference for Normalized Random Measure Mixture Models\",\"Streaming Min-max Hypergraph Partitioning\",\"Collaboratively Learning Preferences from Ordinal Data\",\"Biologically Inspired Dynamic Textures for Probing Motion Perception\",\"Generative Image Modeling Using Spatial LSTMs\",\"Robust PCA with compressed data\",\"Sampling from Probabilistic Submodular Models\",\"COEVOLVE: A Joint Point Process Model for Information Diffusion and Network Co-evolution\",\"Supervised Learning for Dynamical System Learning\",\"Regret-Based Pruning in Extensive-Form Games\",\"Fast Two-Sample Testing with Analytic Representations of Probability Measures\",\"Learning to Segment Object Candidates\",\"GP Kernels for Cross-Spectrum Analysis\",\"Secure Multi-party Differential Privacy\",\"Spatial Transformer Networks\",\"Anytime Influence Bounds and the Explosive Behavior of Continuous-Time Diffusion Networks\",\"Multi-class SVMs: From Tighter Data-Dependent Generalization Bounds to Novel Algorithms\",\"High-dimensional neural spike train analysis with generalized count linear dynamical systems\",\"Learning with a Wasserstein Loss\",\"b-bit Marginal Regression\",\"Natural Neural Networks\",\"Optimization Monte Carlo: Efficient and Embarrassingly Parallel Likelihood-Free Inference\",\"Adaptive Primal-Dual Splitting Methods for Statistical Learning and Image Processing\",\"On some provably correct cases of variational inference for topic models\",\"Collaborative Filtering with Graph Information: Consistency and Scalable Methods\",\"Combinatorial Bandits Revisited\",\"Variational Information Maximisation for Intrinsically Motivated Reinforcement Learning\",\"A Structural Smoothing Framework For Robust Graph Comparison\",\"Competitive Distribution Estimation: Why is Good-Turing Good\",\"Efficient Learning by Directed Acyclic Graph For Resource Constrained Prediction\",\"A hybrid sampler for Poisson-Kingman mixture models\",\"An Active Learning Framework using Sparse-Graph Codes for Sparse Polynomials and Graph Sketching\",\"Local Smoothness in Variance Reduced Optimization\",\"Saliency, Scale and Information: Towards a Unifying Theory\",\"Fighting Bandits with a New Kind of Smoothness\",\"Beyond Sub-Gaussian Measurements: High-Dimensional Structured Estimation with Sub-Exponential Designs\",\"Spectral Norm Regularization of Orthonormal Representations for Graph Transduction\",\"Convolutional Networks on Graphs for Learning Molecular Fingerprints\",\"Mixed Robust/Average Submodular Partitioning: Fast Algorithms, Guarantees, and Applications\",\"Tractable Learning for Complex Probability Queries\",\"StopWasting My Gradients: Practical SVRG\",\"Mind the Gap: A Generative Approach to Interpretable Feature Selection and Extraction\",\"A Normative Theory of Adaptive Dimensionality Reduction in Neural Networks\",\"On the Convergence of Stochastic Gradient MCMC Algorithms with High-Order Integrators\",\"Learning structured densities via infinite dimensional exponential families\",\"Are You Talking to a Machine? Dataset and Methods for Multilingual Image Question\",\"Variance Reduced Stochastic Gradient Descent with Neighbors\",\"Sample Efficient Path Integral Control under Uncertainty\",\"Stochastic Expectation Propagation\",\"Exactness of Approximate MAP Inference in Continuous MRFs\",\"Scale Up Nonlinear Component Analysis with Doubly Stochastic Gradients\",\"Generalization in Adaptive Data Analysis and Holdout Reuse\",\"Market Scoring Rules Act As Opinion Pools For Risk-Averse Agents\",\"Sparse Linear Programming via Primal and Dual Augmented Coordinate Descent\",\"Training Very Deep Networks\",\"Bayesian Active Model Selection with an Application to Automated Audiometry\",\"Particle Gibbs for Infinite Hidden Markov Models\",\"Learning spatiotemporal trajectories from manifold-valued longitudinal data\",\"A Bayesian Framework for Modeling Confidence in Perceptual Decision Making\",\"Path-SGD: Path-Normalized Optimization in Deep Neural Networks\",\"On the consistency theory of high dimensional variable screening\",\"End-To-End Memory Networks\",\"Spectral Representations for Convolutional Neural Networks\",\"Online Gradient Boosting\",\"Deep Temporal Sigmoid Belief Networks for Sequence Modeling\",\"Recognizing retinal ganglion cells in the dark\",\"A Theory of Decision Making Under Dynamic Context\",\"A Gaussian Process Model of Quasar Spectral Energy Distributions\",\"Hidden Technical Debt in Machine Learning Systems\",\"Local Causal Discovery of Direct Causes and Effects\",\"High Dimensional EM Algorithm: Statistical Optimization and Asymptotic Normality\",\"Revenue Optimization against Strategic Buyers\",\"Deep Convolutional Inverse Graphics Network\",\"Sparse and Low-Rank Tensor Decomposition\",\"Minimax Time Series Prediction\",\"Differentially Private Learning of Structured Discrete Distributions\",\"Variational Dropout and the Local Reparameterization Trick\",\"Sample Complexity of Learning Mahalanobis Distance Metrics\",\"Learning Wake-Sleep Recurrent Attention Models\",\"Robust Gaussian Graphical Modeling with the Trimmed Graphical Lasso\",\"Testing Closeness With Unequal Sized Samples\",\"Estimating Jaccard Index with Missing Observations: A Matrix Calibration Approach\",\"Neural Adaptive Sequential Monte Carlo\",\"Local Expectation Gradients for Black Box Variational Inference\",\"On Variance Reduction in Stochastic Gradient Descent and its Asynchronous Variants\",\"NEXT: A System for Real-World Development, Evaluation, and Application of Active Learning\",\"Super-Resolution Off the Grid\",\"Taming the Wild: A Unified Analysis of Hogwild-Style Algorithms\",\"The Return of the Gating Network: Combining Generative Models and Discriminative Training in Natural Image Priors\",\"Pointer Networks\",\"Associative Memory via a Sparse Recovery Model\",\"Robust Spectral Inference for Joint Stochastic Matrix Factorization\",\"Fast, Provable Algorithms for Isotonic Regression in all L_p-norms\",\"Adversarial Prediction Games for Multivariate Losses\",\"Asynchronous Parallel Stochastic Gradient for Nonconvex Optimization\",\"Embed to Control: A Locally Linear Latent Dynamics Model for Control from Raw Images\",\"Efficient and Parsimonious Agnostic Active Learning\",\"Softstar: Heuristic-Guided Probabilistic Inference\",\"Grammar as a Foreign Language\",\"Regularization-Free Estimation in Trace Regression with Symmetric Positive Semidefinite Matrices\",\"Winner-Take-All Autoencoders\",\"Deep Poisson Factor Modeling\",\"Bayesian Optimization with Exponential Convergence\",\"Sample Complexity of Episodic Fixed-Horizon Reinforcement Learning\",\"Learning with Relaxed Supervision\",\"Subsampled Power Iteration: a Unified Algorithm for Block Models and Planted CSP's\",\"Accelerated Mirror Descent in Continuous and Discrete Time\",\"The Human Kernel\",\"Action-Conditional Video Prediction using Deep Networks in Atari Games\",\"A Pseudo-Euclidean Iteration for Optimal Recovery in Noisy ICA\",\"Distributed Submodular Cover: Succinctly Summarizing Massive Data\",\"Community Detection via Measure Space Embedding\",\"Basis refinement strategies for linear value function approximation in MDPs\",\"Structured Estimation with Atomic Norms: General Bounds and Applications\",\"A Complete Recipe for Stochastic Gradient MCMC\",\"Bandit Smooth Convex Optimization: Improving the Bias-Variance Tradeoff\",\"Online Prediction at the Limit of Zero Temperature\",\"Learning Continuous Control Policies by Stochastic Value Gradients\",\"Exploring Models and Data for Image Question Answering\",\"Efficient and Robust Automated Machine Learning\",\"Preconditioned Spectral Descent for Deep Learning\",\"A Recurrent Latent Variable Model for Sequential Data\",\"Fast Convergence of Regularized Learning in Games\",\"Parallel Multi-Dimensional LSTM, With Application to Fast Biomedical Volumetric Image Segmentation\",\"Reflection, Refraction, and Hamiltonian Monte Carlo\",\"The Consistency of Common Neighbors for Link Prediction in Stochastic Blockmodels\",\"Nearly Optimal Private LASSO\",\"Convergence Analysis of Prediction Markets via Randomized Subspace Descent\",\"The Poisson Gamma Belief Network\",\"Convergence Rates of Active Learning for Maximum Likelihood Estimation\",\"No-Regret Learning in Bayesian Games\",\"Statistical Topological Data Analysis - A Kernel Perspective\",\"Semi-supervised Sequence Learning\",\"Structured Transforms for Small-Footprint Deep Learning\",\"Rapidly Mixing Gibbs Sampling for a Class of Factor Graphs Using Hierarchy Width\",\"Interpolating Convex and Non-Convex Tensor Decompositions via the Subspace Norm\",\"Sample Complexity Bounds for Iterative Stochastic Policy Optimization\",\"BinaryConnect: Training Deep Neural Networks with binary weights during propagations\",\"Interactive Control of Diverse Complex Characters with Neural Networks\",\"Submodular Hamming Metrics\",\"A Universal Primal-Dual Convex Optimization Framework\",\"Learning From Small Samples: An Analysis of Simple Decision Heuristics\",\"Explore no more: Improved high-probability regret bounds for non-stochastic bandits\",\"Fast and Memory Optimal Low-Rank Matrix Approximation\",\"Learnability of Influence in Networks\",\"Learning Causal Graphs with Small Interventions\",\"Information-theoretic lower bounds for convex optimization with erroneous oracles\",\"Fixed-Length Poisson MRF: Adding Dependencies to the Multinomial\",\"Large-Scale Bayesian Multi-Label Learning via Topic-Based Label Embeddings\",\"The Self-Normalized Estimator for Counterfactual Learning\",\"Fast Lifted MAP Inference via Partitioning\",\"Data Generation as Sequential Decision Making\",\"On Elicitation Complexity\",\"Decomposition Bounds for Marginal MAP\",\"Discrete R\\u00e9nyi Classifiers\",\"A class of network models recoverable by spectral clustering\",\"Skip-Thought Vectors\",\"Rate-Agnostic (Causal) Structure Learning\",\"Principal Geodesic Analysis for Probability Measures under the Optimal Transport Metric\",\"Consistent Multilabel Classification\",\"Parallel Predictive Entropy Search for Batch Global Optimization of Expensive Objective Functions\",\"Cornering Stationary and Restless Mixing Bandits with Remix-UCB\",\"Semi-Supervised Factored Logistic Regression for High-Dimensional Neuroimaging Data\",\"Gaussian Process Random Fields\",\"M-Statistic for Kernel Change-Point Detection\",\"Adaptive Online Learning\",\"A Universal Catalyst for First-Order Optimization\",\"Inference for determinantal point processes without spectral knowledge\",\"Kullback-Leibler Proximal Variational Inference\",\"Semi-Proximal Mirror-Prox for Nonsmooth Composite Minimization\",\"LASSO with Non-linear Measurements is Equivalent to One With Linear Measurements\",\"From random walks to distances on unweighted graphs\",\"Bayesian dark knowledge\",\"Matrix Completion with Noisy Side Information\",\"Dependent Multinomial Models Made Easy: Stick-Breaking with the Polya-gamma Augmentation\",\"On-the-Job Learning with Bayesian Decision Theory\",\"Calibrated Structured Prediction\",\"Learning Structured Output Representation using Deep Conditional Generative Models\",\"Time-Sensitive Recommendation From Recurrent User Activities\",\"Learning Stationary Time Series using Gaussian Processes with Nonparametric Kernels\",\"A Market Framework for Eliciting Private Data\",\"Lifted Inference Rules With Constraints\",\"Gradient Estimation Using Stochastic Computation Graphs\",\"Model-Based Relative Entropy Stochastic Search\",\"Semi-supervised Learning with Ladder Networks\",\"Embedding Inference for Structured Multilabel Prediction\",\"Copula variational inference\",\"Recursive Training of 2D-3D Convolutional Networks for Neuronal Boundary Prediction\",\"A Dual Augmented Block Minimization Framework for Learning with Limited Memory\",\"Optimal Testing for Properties of Distributions\",\"Efficient Learning of Continuous-Time Hidden Markov Models for Disease Progression\",\"Expectation Particle Belief Propagation\",\"Latent Bayesian melding for integrating individual and population models\"],\"x_coord\":{\"__ndarray__\":\"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\",\"dtype\":\"float32\",\"shape\":[4088]},\"y_coord\":{\"__ndarray__\":\"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\",\"dtype\":\"float32\",\"shape\":[4088]}},\"selected\":{\"id\":\"4919\",\"type\":\"Selection\"},\"selection_policy\":{\"id\":\"4920\",\"type\":\"UnionRenderers\"}},\"id\":\"4694\",\"type\":\"ColumnDataSource\"},{\"attributes\":{},\"id\":\"4919\",\"type\":\"Selection\"},{\"attributes\":{\"overlay\":{\"id\":\"4741\",\"type\":\"BoxAnnotation\"}},\"id\":\"4718\",\"type\":\"BoxZoomTool\"},{\"attributes\":{},\"id\":\"4702\",\"type\":\"LinearScale\"},{\"attributes\":{\"callback\":null,\"overlay\":{\"id\":\"4742\",\"type\":\"BoxAnnotation\"}},\"id\":\"4719\",\"type\":\"BoxSelectTool\"},{\"attributes\":{\"callback\":null,\"tooltips\":\"@title\"},\"id\":\"4727\",\"type\":\"HoverTool\"},{\"attributes\":{},\"id\":\"4720\",\"type\":\"ResetTool\"},{\"attributes\":{\"callback\":null},\"id\":\"4700\",\"type\":\"DataRange1d\"},{\"attributes\":{\"fill_alpha\":{\"value\":0.1},\"fill_color\":{\"field\":\"C\",\"transform\":{\"id\":\"4729\",\"type\":\"CategoricalColorMapper\"}},\"line_color\":{\"field\":\"C\",\"transform\":{\"id\":\"4729\",\"type\":\"CategoricalColorMapper\"}},\"size\":{\"units\":\"screen\",\"value\":10},\"x\":{\"field\":\"x_coord\"},\"y\":{\"field\":\"y_coord\"}},\"id\":\"4731\",\"type\":\"Circle\"},{\"attributes\":{\"callback\":null},\"id\":\"4698\",\"type\":\"DataRange1d\"},{\"attributes\":{\"fill_alpha\":{\"value\":0.1},\"fill_color\":{\"value\":\"#1f77b4\"},\"line_alpha\":{\"value\":0.1},\"line_color\":{\"value\":\"#1f77b4\"},\"size\":{\"units\":\"screen\",\"value\":10},\"x\":{\"field\":\"x_coord\"},\"y\":{\"field\":\"y_coord\"}},\"id\":\"4732\",\"type\":\"Circle\"},{\"attributes\":{\"text\":\"t-SNE Abstract embeddings \"},\"id\":\"4696\",\"type\":\"Title\"},{\"attributes\":{\"fill_color\":{\"value\":\"#1f77b4\"},\"size\":{\"units\":\"screen\",\"value\":10},\"x\":{\"field\":\"x_coord\"},\"y\":{\"field\":\"y_coord\"}},\"id\":\"4733\",\"type\":\"Circle\"},{\"attributes\":{\"data_source\":{\"id\":\"4694\",\"type\":\"ColumnDataSource\"},\"glyph\":{\"id\":\"4731\",\"type\":\"Circle\"},\"hover_glyph\":{\"id\":\"4733\",\"type\":\"Circle\"},\"muted_glyph\":null,\"nonselection_glyph\":{\"id\":\"4732\",\"type\":\"Circle\"},\"selection_glyph\":null,\"view\":{\"id\":\"4735\",\"type\":\"CDSView\"}},\"id\":\"4734\",\"type\":\"GlyphRenderer\"},{\"attributes\":{},\"id\":\"4704\",\"type\":\"LinearScale\"},{\"attributes\":{\"source\":{\"id\":\"4694\",\"type\":\"ColumnDataSource\"}},\"id\":\"4735\",\"type\":\"CDSView\"},{\"attributes\":{\"formatter\":{\"id\":\"4740\",\"type\":\"BasicTickFormatter\"},\"ticker\":{\"id\":\"4707\",\"type\":\"BasicTicker\"},\"visible\":false},\"id\":\"4706\",\"type\":\"LinearAxis\"},{\"attributes\":{},\"id\":\"4738\",\"type\":\"BasicTickFormatter\"},{\"attributes\":{},\"id\":\"4707\",\"type\":\"BasicTicker\"},{\"attributes\":{},\"id\":\"4740\",\"type\":\"BasicTickFormatter\"},{\"attributes\":{\"grid_line_color\":null,\"ticker\":{\"id\":\"4707\",\"type\":\"BasicTicker\"}},\"id\":\"4710\",\"type\":\"Grid\"},{\"attributes\":{\"bottom_units\":\"screen\",\"fill_alpha\":{\"value\":0.5},\"fill_color\":{\"value\":\"lightgrey\"},\"left_units\":\"screen\",\"level\":\"overlay\",\"line_alpha\":{\"value\":1.0},\"line_color\":{\"value\":\"black\"},\"line_dash\":[4,4],\"line_width\":{\"value\":2},\"render_mode\":\"css\",\"right_units\":\"screen\",\"top_units\":\"screen\"},\"id\":\"4741\",\"type\":\"BoxAnnotation\"},{\"attributes\":{\"formatter\":{\"id\":\"4738\",\"type\":\"BasicTickFormatter\"},\"ticker\":{\"id\":\"4712\",\"type\":\"BasicTicker\"},\"visible\":false},\"id\":\"4711\",\"type\":\"LinearAxis\"},{\"attributes\":{\"bottom_units\":\"screen\",\"fill_alpha\":{\"value\":0.5},\"fill_color\":{\"value\":\"lightgrey\"},\"left_units\":\"screen\",\"level\":\"overlay\",\"line_alpha\":{\"value\":1.0},\"line_color\":{\"value\":\"black\"},\"line_dash\":[4,4],\"line_width\":{\"value\":2},\"render_mode\":\"css\",\"right_units\":\"screen\",\"top_units\":\"screen\"},\"id\":\"4742\",\"type\":\"BoxAnnotation\"},{\"attributes\":{},\"id\":\"4712\",\"type\":\"BasicTicker\"},{\"attributes\":{\"items\":[{\"id\":\"4744\",\"type\":\"LegendItem\"}]},\"id\":\"4743\",\"type\":\"Legend\"},{\"attributes\":{\"dimension\":1,\"grid_line_color\":null,\"ticker\":{\"id\":\"4712\",\"type\":\"BasicTicker\"}},\"id\":\"4715\",\"type\":\"Grid\"}],\"root_ids\":[\"4695\"]},\"title\":\"Bokeh Application\",\"version\":\"1.4.0\"}};\n",
       "  var render_items = [{\"docid\":\"90aa50bb-6d4b-4ecf-b5d8-9ad0392a96b7\",\"roots\":{\"4695\":\"7364e8b3-44f5-42a7-a0e3-1aa7a9a6cbeb\"}}];\n",
       "  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\n",
       "\n",
       "  }\n",
       "  if (root.Bokeh !== undefined) {\n",
       "    embed_document(root);\n",
       "  } else {\n",
       "    var attempts = 0;\n",
       "    var timer = setInterval(function(root) {\n",
       "      if (root.Bokeh !== undefined) {\n",
       "        clearInterval(timer);\n",
       "        embed_document(root);\n",
       "      } else {\n",
       "        attempts++;\n",
       "        if (attempts > 100) {\n",
       "          clearInterval(timer);\n",
       "          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\n",
       "        }\n",
       "      }\n",
       "    }, 10, root)\n",
       "  }\n",
       "})(window);"
      ],
      "application/vnd.bokehjs_exec.v0+json": ""
     },
     "metadata": {
      "application/vnd.bokehjs_exec.v0+json": {
       "id": "4695"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "output_notebook()\n",
    "# add our DataFrame as a ColumnDataSource for Bokeh\n",
    "plot_data = ColumnDataSource(tsne_df)\n",
    "\n",
    "# create the plot and configure the\n",
    "# title, dimensions, and tools\n",
    "tsne_plot = figure(title=u't-SNE Abstract embeddings ',\n",
    "                   plot_width = 800,\n",
    "                   plot_height = 800,\n",
    "                   tools= (u'pan, wheel_zoom, box_zoom,'\n",
    "                           u'box_select, reset'),\n",
    "                   active_scroll=u'wheel_zoom')\n",
    "\n",
    "# add a hover tool to display words on roll-over\n",
    "tsne_plot.add_tools( HoverTool(tooltips = u'@title') )\n",
    "\n",
    "# draw the words as circles on the plot\n",
    "tsne_plot.circle(u'x_coord', u'y_coord', source=plot_data,\n",
    "                color=factor_cmap(u'C', 'Spectral6', color),\n",
    "                 fill_alpha=0.1,\n",
    "                 size=10, hover_line_color=u'black',legend=\"label\"\n",
    ")\n",
    "\n",
    "\n",
    "tsne_plot.xaxis.visible = False\n",
    "tsne_plot.yaxis.visible = False\n",
    "tsne_plot.grid.grid_line_color = None\n",
    "tsne_plot.outline_line_color = None\n",
    "\n",
    "# engage!\n",
    "show(tsne_plot);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4088, 2)"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "tsne_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f82adb28390>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = sns.load_dataset('iris')\n",
    " \n",
    "# Basic 2D density plot\n",
    "sns.set_style(\"white\")\n",
    "sns.kdeplot(df.sepal_width, df.sepal_length)\n",
    "#sns.plt.show()\n",
    " \n",
    "# Custom it with the same argument as 1D density plot\n",
    "sns.kdeplot(df.sepal_width, df.sepal_length, cmap=\"Reds\", shade=True, bw=.15)\n",
    " \n",
    "# Some features are characteristic of 2D: color palette and wether or not color the lowest range\n",
    "sns.kdeplot(df.sepal_width, df.sepal_length, cmap=\"Blues\", shade=True, shade_lowest=True, )\n",
    "#sns.plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f82ada29240>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEHCAYAAAC0pdErAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nOydd3hUVf6H38mkzWTSe++dACEh9BBCV1SUta1iwbXsWlcURSwoCoK9rBUrCIKCKNJL6C0hlJCE9F4nmZTJZPrc3x+BYExC0NVF/d33eebhebj3nnvuncz5nPNtRyIIgoCIiIiIiMjPsLrcHRARERER+WMiCoSIiIiISJ+IAiEiIiIi0ieiQIiIiIiI9IkoECIiIiIifWJ9uTvwWzFixAj8/f0vdzdERERE/lTU1NRw9OjRPo/9ZQTC39+f9evXX+5uiIiIiPypuO666/o9JpqYRERERET6RBQIEREREZE+EQVCRERERKRPRIEQEREREekTUSBERERERPpEFAgRERERkT4RBUJEREREpE9EgRARERER6RNRIERERERE+kQUCBERERGRPhEFQkRERESkT0SBEBERERHpE1EgRERERET6RBQIEREREZE+EQVCRERERKRPRIEQEREREekTUSBERERERPpEFAgRERERkT4RBUJEREREpE9EgRARERER6RNRIERERERE+sT6cndA5I9NS2srhUXFVFZV09ysoqlZRUtrKwaDAZPZjNlkQmptjVwmQyazx8HBAXc3N9zdXHF3d8PHywtvby9cnJ2RSCSX+3EuCY2mk4bGRuobGmlqakbZ1ESzSkW7Wk1HhwZ1R0fX85tMGE0mEARsbGywtrbG1tYWhYMDTk6OODk64ubqire3F95ennh7df1rZ2d3uR9RROSSEAVCpBu9wcCp0zkcPprJ0cwsCotKaGlt7XGOg1yOq6sLdnZ2SKVSrKVSTGYzWq2WTq2Wjg4Ner2+V9s21tZ4eHjg4+OFr7c3vj7e+Pr44Ofrg6+vD34+Pri5uWJl9fsuanU6HXX1DdTU1lFbV0dtXT119Q3U1tVRV99AY6MSTWdnn/13dHLEUaFAoVBgZ2uLjU2XMEokEoxGEzqdnna1mvKKStrVatrb1ZhMpl5tubm64OPtTUCAP0GBAQQHBhASHExkeBgeHu5/GiEV+etz2QVi/vz57NmzB3d3d3788UcAWltb+fe//01NTQ3+/v68+eabODs7X+ae/jWxWCwczTzON+s3sH3XbrRaHRKJhLjYGKZOnkhYSDAR4WGEhgTj6eGOTCa7aHuCINDZqaVZpaKpuZn6hkYalUqUyiYaGpXUNzSSX1DI7r370el0Pa61tbE5N9v2wsvTAw8Pdzzc3XFxdsLJyQknR0fkchm2NrbY2togtbbGYjafW8mY0XR20qHR0NHRQVtbO03Nzef6oaK+voH6hsZegieRSPDy9MDXx4eYqEjGjx2Dl5dn94zfy9MDTw8PHB0Vv3jgFgSB9nY1DUoljY1K6hu6+lDf0EBdfQOlpWXs2XcAg8HQfY2LszMR4WHExUYTHxvLoLgYwsNCsbGx+UX3FhH5LZAIgiBczg5kZmYil8t54oknugVi2bJluLi4cM899/DRRx/R1tbG448/ftF2rrvuOtavX/+/6PJfAp1Ox+q16/hy1Roqq6pwdFQwY/pUxo8bQ0pSEs7OTr/r/QVBoKW1tXs23zWAdw2cDY2NNDWraFQqUas7fvU9rK2tcXdzxcPdHW8vL3x8usTHz9cHfz9f/Hx98fbywtb28g2+FouFhkYlpeXlFBWXUFxSSkFhMWcLCunUagGws7MjOjKCmOgo4mKiGZIwiOjoSOxsbS9bv0X+Olxs7LzsK4jhw4dTXV3d4/927drFihUrAJg5cyazZ88eUCBELg2Dwcg367/jPx8up6FRSfKwRB6+/16mTZ6Ivb39L2rLYrHQqdUhCAJ2drbYWFtf8ixbIpHg5uqKm6sr8bEx/Z6n1+tpb1fTrlbT1t6OVqvFYDRiMBgxmUxYS6VIz33kchkKhQJHhQInR0ecnZ1+d3ON2WxG/5MVgEQiwc7W9pJNZVZWVufMbd6MGTmiR7vlFZXk5p0lJzeX/IIiduzKYO2674Cu1VZsTDTJw4YyYngyw5OG4eTk+Ns+nMj/ey67QPRFc3MzXl5eAHh5eaFSqS5zj/4aZOzdz/OLl1JVXUNS4lBeX7qYkSnJF71Gp9OTeTKHnPwCKqpqKK+qpqqmlnZ1B53aniYiiUSCXGaPh5srHm5uuLu74uvlib+vD/6+3gT5+xES5I+DXH7Jfbazs8PT0w5PT49f9cy/lrZ2NRXVNdTUNVBTV09tQyNNTSqaVC0om1W0qzvQdHb2egfQ9R4c5DIUDnJcXZzx8vDAw80FTw93gvz9CPT3JcjfDz8fL6RSaZ/3l0qlhIeFEh4WytUzpgNdq666unpOn8nlVE4uJ0+fZsXqtXzyxUokEgmD4mIZNSKFESlJJCUOxVGh+F3fkchfnz+kQIj8tjSrVCxa8gobN28lMjyMTz94l9Sxo/udXTepWlj/4zb2HjrK8dNnMBiMAHh5uBMU4MeYEcm4ODl1zdjlMiQSK/QGAwaDAU2nFuW5aKeyiioOHTtOh6an09fHy4PQoEDCQ4OJCA0mMiyEiJAgPNzd/qcOWoPRSGV1LSXllZSUV1JaXklZZRXlldW0tLX3ONdBLsPD3Q0PN1ciw0JwcXbCQS5H4SDH3s6uu98WiwWtTkeHppMOTSfNLS00NjVTWFKKskmFyWzubtPezo7I8BCiw8OIj45kaEIcsVHh2Pbjb5BIJPj5+eLn58u0KZOArhXWydM5HDmWxeGjx/jsy5V89OnnWFtbk5Q4lPS0caSPTyU0JFh0fov8Yv6QAuHu7k5jYyNeXl40Njbi5uZ2ubv0p2Xj5q08/9JSOjo6eOT++7j3H3P6tbmfPJPHp6u+ZdvufRhNJmIiw7nthmsZk5LEsMGDcFQ4/OL7C4JAu7qD6tp6KmtqKauspqyiktKKKjZs3kGHRtN9rpOjgvCQIMKCAwnw88Xf1xt/Xx+8PNzwcHPD6Rc4igVBoFOrRdXSRpNKRV2Dktr6BmobGimvrKGssorq2jrMZkv3Nd6eHoQFBzJt4nhCgwIICvAjwNcHf1+fX3Tv/jCbzdQrm6iqrqWiuobCknIKSkrJOHiEbzduAcDOzpZBMVEkD00geUgCwwbH4+rSf4CGnZ0dI4YnM2J4Mg/ffx9arZYTp3I4ePgIGfsOsOSVN1jyyhtEhIUybcokJqWnMSguVhQLkUvisjupAaqrq7nvvvu6ndRLly7F1dW120nd2trKvHnzLtqG6KTuSYdGw3MvLmHDD5sYOjiBlxc9R2REeJ/nVtXU8eLr77Jj70EcFQ7Mumoat8y6mvCQ4F7n6g1GCksrqFc209isQtncit5gwMrKCiuJBFtbG9xcnPBwdcHd1YVAPy+8+lkZCIJAY1MzRaXllJRVUlxeQWl5l3g0NjXz8z9NG2trnJwccZDJkMtlyOwvzNwFQUCn19Op1aHV6mhXd6DrI9xWLrMnONCfsOAgQoMCCAsOJCwkiLDgIBQOFzd9mUxdA3x1XSNKVQstbWqaW9tQd3RiNJowmU2YTGZsbWyws7NFZm+Hs6MDnm6ueLi74uPhRpC/L3Z9CHRdQyMncvI4kZNH9ukz5OQVdK82osJDGZ0yjDHDk0gZNuQXCXVNbS27MvaxdccuMo9nY7FY8PP14YppU5gxfaooFiIXHTsvu0A8+uijHDt2jJaWFtzd3XnwwQeZNGkSjzzyCHV1dfj6+vLWW2/h4uJy0XZEgbjA6ZxcHn58PtU1NTxw393cf+8/sLbuvVjU6/W89/kqPvxiFVIrKf+acyu333hdj4FSEARO5Baw62AmWafzOJVf1G1ygi6zh42NNYJFwCJYeszIz+Mgsyc4wJfw4EBiwoOJjQglJiIEb4/+TUp6g4G6hkZq6hpoUrXQ1NxCk0qFukNDZ2dXzsXP7f/2dnZdyXrn7P/urq64ubrg7uaCr5cnfj7el7QS0Or0FJZVUnT+U15FSXk1tY3KPp9PIZdha2uDjbU1UqkUg9GITm9Ap9P3MClBl1M60NeL0CB/4iJCSYiJYFB0BH7eHj36pdXpyMkrIPNkDkeyTpB1Kge93oC1VEry0AQmjB3J+NEjiAwLueQBvlmlImPvAbbt2MX+g4cwmkwEBwVyzYwruPrK6YT2MSEQ+evzhxaI3wpRILpMGB99+jlvvvsBnh4evLHsJYYnDevz3NKKKh6cv5D8whJmTEln/sP34evt1X1cq9Pzw469rFi/mbyiMqylUgZFh5OUEEvioGgCfb3xdHfF3dUZm5+Ij9FkQtXaRnNLG8rmVipq6iivrqOssoai8ipqG5Td57q7OBMfFUZcVBjxkWHERoYSEuD7uyfLncdsNlNRU09BaQVnS8opKKmgoLSCypr67tWLrY01YUEBRIQEEuTnTaCfN/4+Xnh7uOHq4oSLoyPW1n07mgE6OjtRNreibG6htlFJWWUNpZU1FFdUU1xe1S04Hm4uJA+OJWVIPClD44kJD+nxHvR6PSdy8th3JJOMA0coKC4FwN/Xm/Sxo0gfN4qRSUMvOUu7tbWN7bt288OmrRw5lokgCMTFRDNj+lSuumIafn6+v/a1ivzJEAXi/wG1tXXMnf80x7KyuWLaFF58dkG/uQw/bt/N/BdfwcbGhlefn0/62FHdx8xmM2t/3MlrH69E1dpOdHgws6+9gqunpKL4WfSRxWKhql5JS5sag9GE3ti1snB1csTNuetjb9czVr+tvYOC0gryi8vILSwlr6iUwtLK7pm2XGZPdFgwkaGBRIQEEREcQJC/N37entj/ihIVFosFVVs7VTUNVNTWUVlTT0lFDcXlVZRUVnevhqysrAj29yE6PJjosBCiw4OJCg0kyM+3lwDo9AZqlc3UNaqob1LR3NqOplOHRqdDqzMgtZJ0ld2wkaKQy/B0dcHDzRlvd1dCA3xwkNmfa0fP2eJycgpKOJlbwLFTedTUNwLg6e7K+BHDSBuVxLiURBx/Zv6qrW9k76GjZBw4woGjWej0ehQOctLHjWb6xPGMH5WCvf2lva/6hka2bN/Jpq3bOHHyNAApycO4ZsaVTJ8y6XfPiRG5vIgC8RdGEAR+2LSFhS+9jNlkZuHTT3Lt1TP6NDtYLBZef/9T3vtsJcMGx/P24ufw87mwaqiua+DRRW+SdTqPlKHxPPqPWxg+JK5HhM6xnAI27z1KbnEFBWVVdOp62/l/ire7K+FBfoQH+hIdGkhCVCjRoYHY2lxYdegNRorLq8gr6hKMs8UVFJdX0dza1qMtdxdnPNxdcHZU4OyoQCGXIZVKsbKSIJFI0BuMdHZq0Wh1tKk7aFK10qRq7WHmkUgk+Pt4EhESSGRIIBEhgcSEhxAZGthLgExmM6VVdeQVV5BfWklRRQ3FlTXUNPT2j0CXKc3ezhZBEDCazBhNJnR6Q6/zfD3dCAv0JT4ihMTYCBJjI/By7zKh1tQrOXIih71HjrPv6AnaOzTYWFszInEQk8amMHncCHy9eob86nR6DmedYPue/WzP2E9LWztymT0Txo5i+sTxpI0ZgXyADPjzVFZV88OmLXz/42ZKy8qxsbZm1MgUpk+ZxJSJ6bhcxGEu8udEFIi/KI1KJc88/xI7M/aSOCSB115+ieCgwD7P1ep0PPbcErbs2stN187g+Sce6WEa2rLnEE8seQeAhY/czbXTJnQLQ2NzK6s372b99gNU1StxkNkzKDKE2LAgokMD8XJ3wdbGBltbawQBWtvVqNrUNLW0UV7TQHFlLaVVtXR0dvkMbG1siA0PJDE2kqT4SIbFReLr2TtSrbmljZLKamrqGqltUFJTr6S5tY02dQdt6g46NFosFguCIGCxWLCztUUut8dBZo+jQoGnmwue7q54urkQ6OtNcIAvgb7e2Nn1zkC2WCyU1zRwuqCU0wWlnCooJb+0snuAt7WxISzQh4ggfyKC/Ajy9cLX0w0fTzc8XJ2R2fWdHKc3GGlqbaOppZ3axmZKq+ooraqlqKKWgrJKjKYu8Qr08WRsUgKpyQmMSozDyUGOyWQmO/csuw9msmP/UcqqagFIHhzHjIljmT5hDJ5uPX1zRpOJo8dP8v32vWTs24+qpRU7OzvGjxnF9CmTGJUyvMf33h+CIJCfn8+OnTvYvWs3NbU1WFtbM2rkKKZOnUpqairyX5DPcrlwlfdv/hPpQhSIvxiCILD++428uPRV9HoDcx+6nztm/73fpCtVayt3PTyf03lnmf/wfdx1yw09Vhjvr/iWVz5cwdC4KN5+/jECfL2777Pih50sXf41Or2RMYnxzJo6jimjk5Bdovnip32urldyurCMnMJSTuaXcLqwrHsA9vV0Y1hcl1gkxoYTExbUyzz1W3F+ZZBfWklecQU5hWWcKSrrFjCZvS3xESEkRIUyKCKE+MgQwgJ9sf7J+zWazFQ3NlNW00itsgVVewctag2tas1PVhcSHGS2uDk54u6iwMvVmfAAb0L8PLGzsUFvMJBXXEl2fhFHT53l8Mk8NFod1lIpI4bEMHVsMlNGJ3evLkoqqtm65xAbdx2gsLQCKysrRicN5urJqUxJHYnTT6KbWjrNmM1msk+dZkfGXnbu2UtbWzvOzk6kp45j6sQJDBsyuN+/mZ8iCAJnz55l27Zt7Nixg4bGBuzs7Bg9ejQT0iYwbtw4HB3/mFncokAMjCgQfyGKS0p5dtFijmYeZ9jQISx76fmLRp/U1NVz+wOPU1PfwFsvPcOUtHHdxwRB4JUPV/DBynVcNSmVZU891B2C2djcyhOvfczezNOkJifw/IO3E+zn3aNtdaeW/SfOUlxVT31zKw2qNto1Whzs7XCQ2aGQ2xPg5U54gHePgfE8RpOJ/JJKsvOKyM4rJjuviNrGZgCkVlaEB/kRHxFMWKAfoQE+hPp74+/tgaODfMDIHbPZQqOqlTplc/fMveTc7L24shb9Od/D+dVMQmQog6JCGRwdRkSQXw8xUHdqySmq5ExpFbklVZwpqaaiTonZ0jOiydFBhrODHGupFYIgIAAarY4WtQaL5cLPTGplRZCvB4MjgkmJDyc5LpyIAG/MFgsn8kvYc+wk2w5kUVZdj0QiYfigKGZOGsMVqSndIlBQWsGPO/ezced+KmvrsbW1YeLo4cy6YiKpKYmof2bZMhqNHM7MYuvO3ew7eAitVoenhzuTJ6QxffJEYqOjLikaymKxcOrUKbZv386evXtQKpVdK4tRo5g+bTqpqam/uGTL74koEAMjCsRfAK1Wy3sffcrHn36OXC5n3qMPc8OsmReN+Mk+ncu9jz2NwWDg4zeWkJI4uPuYIAgsems5n3/7IzdfPZUX5t7bPZvMKSxjzoJX6OjUMf+em5l99aQe+QY7jp5m1daDHDpd0G0icXdW4O3mgpNCRqfOgEaro12jRdlyISPZ1tqa+PAAUuIjuj8Kec/BpE6p4tTZEvJKKjhTVE5+SSUNzS09zrGxluLq5IiLk0N3aKnUygqd3kBHtw9C02MAl0gk+Hm5Ex7oS2RwAHHhQcRGBBMe6NvL5FKjVJGZW0JmXjHH80sprLwQ1eTn6Up8WCCRQT6E+XkR6u9FgJc7Lo4OPfwqP8VsttCm6aS+qZXiqnqKq+spqKgl+2wZTa3q7veXlhTPlJGDGZcYi72tDcWVtWzdn8n3uw9RWlWHrY01E0cmcsP0NMYOG4T0nBCdyivk+x172bhzP6rWdjzcXJiePp4Zk9OICO09edBqtew/dIRtuzM4eOQYRqOR4MAApkycwKS08YSHXlrorMViITc3l127drFt+zaUSiVyuZwJEyYwdcpUUlJS+gyv/l8iCsTAiALxJ0YQBHZm7GHRkleoqa1j5tVXMv+xR/Fwv3h2+Q9bdzHvhZfx9fZi+RuLeyS9CYLAorc/4fNvNjLnxqtZ8MCc7gEht7icW+e9jEIu49OXHiMy2L/7uhqliiffWcWBk2fx9XDhqnHJTBqRwODIoB4rg5/SqdNTVtNIcXU9eaU1HM8v5XRxBUaTGWupFUMiQxg9JIrRg6NIjA7tM4mso1NLRW0D5TUN1ClVqNraUbWqaVVrMJ8v9222ILO3xUEmQyG3x8VJ0eUj8HDD18udYF+vPs1iFouFoqp6svJKyMov5VhuMbXKLkFylNszLCaMYTGhDIkKJiEiCDcnRa/rG1rU1DW3UdvURmNLOzqDCYPRhNFkxtZGirNCjotChruzgjA/D/zcuzZPEgSByvomMvNKOHDyLBlZubRrtNjb2jB5xGBmpg1nXGIs1lIrcgrL2LDzID9kHEbVpsbX052/TR3HDdPG4+/d5bQ2GI3sPZLNt5t3sftQFmazmZiIMGZMnsC0CWP7zMhuV6vZtXcfW3fsJvvUaSwWCyFBQUxMS2XyhFQiwsIuSSzMZjPZ2dls2bqF3bt309HRgYuLC5MnTebaa68lKipqwDZ+D0SBGBhRIP6kVFZV8/zipezZd4CoyAief/pJUpKTLnqNxWLh7Y+/4O2PvyBl2BDeX/ZCj4FBEARefu9zPl69gTuuv4pnHrqrewDIL63klseWIJfZ8fVrCwjw8ey+bn3GMZ77YC0CAk/ePpObpo7uYYb5Jej0BrLPlnHwdAEHTxWQU1yJxSJga2PNsJhQRsRHkBwXTmJ0KA6y33b3tfaOTk4UlpN9toyTBeWcLCynXdNVVtvDxZHhcREMjw9nRHwE0cF+SKUXVmiCIFBa18SxvHJySms4W1FPQWUDnX1EKgFYS60w9ZFYJ7ezJSLAkyERAYxJCGdUfBhODjKMJjPHcovZeugkmw5k06LW4Oak4JrxydwyfSzhAT4YjCZ2Hs5m7ZY97D9+BoDxwwdzy1UTSRs+pLu/pbUqtmYc4McdGZwtLkUqlTI2JYkZk9MYNyKpz/0lmlUqdu87wM6MvT3EYtKEVNLGjiEmKvKSxMJgMHDo8CG2bdvGvn370Ov1JCQkcO3Ma5kyZcr/1AQlCsTAiALxJ8NoNPLJ5yt4+/2PsJZKeeSBfzL77zcOuGlMk6qFR595iQNHs5g1YyovPjW3154B73y+hjeWr2L2dVew8N/3dP/gG5pauO6hhQiCwNevP02Q74Xw1w/W7WDpF98zMiGSVx66lQBv94v332SmU2dAazDgqpD3uSr4Ke0dnWTml3Akp4gjOUXkllYjCAJSKytiQ/2JCw0gKtiXyCBfQnw98XJ1uqgDWxAE2jo6qaxvorK+idKaRvLLqsktraaqocvHYWUlISrIj6FRwSTHhZMcG0aQT89sZkEQqGxQcfBMCYfPlHI0rwxla9f+FM4OMmJDfIgJ8iHC3xM/Dxd8PZzxdnVCbmeDjbUUKysrTGYz7RodrR1aGlvaKalRUlyjpLCqgZNF1XTqDVhJJCTFBHPlqEFMHxGPl6sTBqOJfSfy+S7jGDuOnsZoMjN6cBSzr0hl0ogErKVSahqaWLtlL2u27KFR1UqAjwd/v3Ii109Lxcr2gsO6qKyCTTv2sHn3XppVrTg5Kpg0bhTT01MZOii2TzOlqqWF3fsOsGP3nm6x8PH2Im3sGCakjmVoQsJFEwTP09bWxqZNm1i3fh0VFRU4Ozsz85qZXH/99fj4+Ax4/X+LKBADIwrEn4jsk6dYsPBFCouKmTopnWfmz8PXx3vA644eP8nDCxbR2t7Owscf5saZV/Ya7P7zxTe8vvwrZk1PZ+n8B7sHBo1Wx02PvkhZdT3fvPkMseEXzFGrth5gwXtfc3VqEq8+chs2fQwKqnYNG/afZHtmPieKqjAYe26z6ePmRICXKxH+niRHBzM6IRxf9/7j6ds1Wk4UlJGVV0r22VIKKmppbuu5cZCjgwx3JwU21uf2g7CSoNUbaevoRK3RYvjZVp8hvp7EhQUQHxbA4MhghkaF9PJ/ALR36th3sogDp4s5mFNC9Tlzk7erIyPjw0iJC2FEXChhvh7/dQ0jg8nEicIq9p8uZmdmPgVVDUgkElJiQ7g+bRgzRidgZ2tDU6uaNdsPsWrrAWqbWvDzdGX2Fan8feoYnBRyjCYTOw4eZ+XGXRw5ld/lqxg9nL9NT2dITER3P01mM0ePn2Lzrr3sOXQMnV6Pt6cHU8aPYUraGGIjw/t8ppbWNvYfOkzGvgMcycrCYDDi4uzM2DFjSB03jsEJCT2iodwceou3IAhkZ2ezZu0a9uzZg0QiIT09nbvm3EVERMR/9R4vhigQAyMKxJ8Ag8HI2+9/yIfLP8PH24uFC55k4oTxA19nNPLu8i9577OvCA7w592XnyM2qucPThAEXv1oJe+v+JZrp6axdP5D3bM/i8XCfQvfIuPoST5e9ChpKUO6rzt8upDZz75L6rBYPnzqnl7iYDKbeWPtLj7ddAidwUh0oDdjEsJxcZTjYG+Lva0NTW0dVDaoqGxs4WxFPepzoaTh/p6kJUYxITGa4bHB2A7gzGxuU1NYUUd1YzONqnYaW9poaddgPOd/MJvNyOztcHKQ4ayQ4+akINjXgyCfro+8n7BcQRAoqVGScaKQjOwCMs+WYzJbcJTbMyo+lDEJEYxJCCfMr39BsFgEqpStFFQrKalT0azupK1DS6tGhyCAQmaLg70tLgoZEb7uRAd4EO7rjp1tz2cuqm5k8+EcNhw4RXldM66Ocm6eOJxbp47A190Zk9nMrmNn+PzHPRzJKUIhs+fGKaOZc/UE/Dxdu9qoqOGrjbtYt+MAmk4t4cEB/G1qGtPTRvfIxu7Uatlz6BhbM/Zz5PgpzGYzvt6epI8dyYTRIxgcF91nCGxNUxuZWVnsP3CAI0ePotPrcXR0ZNyYMaRPmMCg+Pju6/oSCoC6ujrWrl3LuvXr6OzsZPz48cy5cw7x8fEX/Rv4NYgCMTCiQPzBKSouYe6TT5Obf5YbZs1kwROPoXAYuGJnQXEpjz23hNyCImbNmMpzjz/cqyLpTx3SN18zlUVz7+thUli6fA0frvmRhfffxm0zJ3f/f62yhasfXYqrk4LvXnms12y7vVPHw2+tYc+JQq4ZO4R/XTueqMCLr3QsFgsFVY0czClm36kijme94MkAACAASURBVOaVYzCaUMjsGBUfxpjB4YwbHEmY3++7OVBbh5ZDZ0o4mFPCvlNFVDV2rRKiAr1IHxbDxKQYEqMCkfYTIdbeqSersJqswmoyi6rJLW9A+5MChnI7G1wc7HFysMdKIkGjM9ChM9Cm0XX7JKRWEhJCfRg3KJRxg0JJDPfD+pz/QBAEDueW8sWWw+zMOouVlYSrxgzmHzPGEhfSVSMpr7SaD9fvYNOBE0gkcFVqMnfPnEhsaFdQQa2qk637jvDt1t2cLanAztaWSaOTuWZyKsPio3uIXVu7mr2Hj7HrwBGOZp/CaDTh6uxE6qjhpI1KIWXY4O4sc5Xmgr9Fq9Ox5+BRjh4+RFbWMfR6Ha6ubkyZNJHp06bh7+fXr0gAtLe3s2bNGlZ/vZr29nYmpE3g/vvvJyQk5Jd+pf0iCsTAiALxB+aHTVuY/+wLyOUyFj//DJPTJwx4jdls5tNV3/Dae5/g6Khg8VNzmZw2ttd5BqORBcveY92W3b0c0gDfbtvHvFc/5parJvLCg7d3HzMYTdw4/02Kq+rZ8NrjhAf0HPg7tHpuXfQpuWW1PH/XVfx9UsqvevZOnYGDZ0rYk13A/tPF3QN1sLcbI+PDGDUojBFxIfi4/XflHQwmE6eKqzl8ppRDZ0o5XlCByWxBIbNjRFwoExKjGJ8YRcC5WXhfKNs07MguYtvxQg7ldV1vLbUiIcSHxHA/ogM9ifL3IMLPHUU/jnWjyUx5QwsF1Uryq5Qcya/gZEkdFkHA09mBG8cP5ua0ofi5X6h9VNWo4rPNh1mzK4tOvYHJybE8fH068aF+AFQ3NPPpDxms2XGITp2B1MRY7r1uEjERF8xFeUVlbNixl637j6Lp1BLg48X08aOYljqSkICeRfk0nVoOZmaz59AxDhw9jqazEztbW5KHDiIpMZGRycPw9HBH1dHTMa/T6cg+nsmB/Xs5ceI4FouFhEGDmD5tGmmpqXi79D/h0Wg0rF69mi9XfIler+famddy77334ura//dxqYgCMTCiQPwBsVgsvP7Oe7z/0SckD0vk3deXXdK2mkWl5TzxwlJOnslnyoRxvDj/UTzcev+QWtra+deCpRw9eYaH59zMQ3fe2EMcMo6e5N7n3iRlcAyfLX6sRy7Aks828NF3O/nPE3dxxZjEHu0aTCbmLPmSI7llvDf3ZqYMj/sv3kJPKhtU7DlRyL5TRRzLL+82R3k4K4gO8iYmyIcQX3e8XBzxdHXE3ckBqdQKqZUEqZUV7Z06VO0aVO0aqhtbKaisp6CqgcKqRnQGIxKJhLgQH8YNjmRiUgxDIgL69Kmcp16lZuvxQrZkFpBZWIUgQLCXC9OSo0kbHMaQMF9kdhd3wA9Eu0bH/txy1h3IYc/pUiRImJgYwV1Th5MSHdD9nbV1aPli62E+2XSQdo2ul1C0qjWs2nqAzzbuoalVTWxYIH+fPp4poxK7v1utXs+ug1lsyjhIZk4+giAQEx7MpNHDmTg6mSC/nk5jo9FI1qlc9h/N4sCx49TUNQAQERbC0CFDGTE8BalL75LtbS0qMg/u4/jBvdTX1+Hu7s7Mq69mxhVXEOTdf3i2SqVi+SfLWbduHXK5nH/e909mzZp1Sdne/SEKxMCIAvEHw2AwMvfJBWzetoMbZl3L80/P73eXt/OYzWY++eobXn//ExzkMhbOe5gZU9L7tIuXV9cx57EXqG1o5OX5DzJzSlqP41lnCrntyaVEBPnx1StP4ehwoZDb7swz3LXoA26dPo5F/7yxV9uLV2zh440HeOVfs/hbWt+lxH8LzBYL+eV1HMsvJ7+insKfDPSXSrewBPuQEhNCSlwILoqL1w8ymszsPFHMVxknOJhbAUCUvwfTkqOYPjyG6ID/3jndH1XKVlZnnGLNvlOo1FqSI/2Z+7dURsYEdZ/T3qnj882HuoViyvBY5t40udu8pzcYWbf7KB99t5uKukY8XJy4Yeo4Zk0cjetPcjiUzS1sP3CMbfuPkFtUBkBEcADjUxIZmzyE+MiwXiG+JwvKOJKVzZHM4+SeLUQQBDy9vAmOHUT00CQ8ff16PE+Qkz0FZ05zaMdmcnJOIZPJuOaqq7h79k24OPe/KiwrK2PZsmVkZmUSHR3N0wueJjY29le9U1EgBkYUiD8QBoORB+fOY+fuPTwx9xHuvvO2AQecyupaHlu4hKyTORddNQBkHM7i0RfewMpKwgeLn2L4kJ4z/Nzicm6euxhPN2fWvP40Hq4XfqjltUqumbuMQG93vl36aK9Q0gOni5n94mfcOiWFRf+45le+gV+P2WKhua2DxhY1ja1qVO2dmC0WzBYLFouAQmaHm5MD7k4OeLs54eGsGLjRc9Q0t7N27ym+3nuaxtYO/NyduGFcAleOiCHid/aJ/Byt3sg3+3N4b+NhGlo7SE0IZd7144kPvmDqOy8UyzceoENn4NpxQ3n4+vTuGXpzh5HDpwtYtWUPh0+dxVoqJT1lMNdNHE1yXEQPP1SdspmMw1lkHMnmVH4RZosFZ0cHRiUmMCoxgZGJ8VjZXhBWVYeB1tZWth88TObxbKpKihAsFhw9fPCLTcQvLhF7hRPRnl1+q2BnGQZlHVs2bWDvvn3I7O35+w2zuPWG63F07Ps7EgSBnTt38trrr6FSqbj11lu55+57fnEOhSgQAyMKxB8Ei8XCw4/PZ/PW7Tz31BPcdstNA16z7setLFz2FhKJFQsff4hrr5zSbynvtz77mnc+W0NMeAjvL36SYP+e9uWCsipunbcEO1tb1r7xDH5eF/IZNFo9s+a9RoOqlY2vP9Er16GprYMZ895FIbdn48v/QvY7FdL7X3OypJZ3fzjM7lPFAIxPCOOW9EQmDAnr10n9v0JnMPLlzmze//EIrRodM0fH8+QN4/F2vVAYr0Xdyfsb9vLl1iOYzBauTR3KQ3+bgEJxQfhLq+tZv+sQP+7LpF3Tib+XO9PGJDF9TBJhAT3NSu0dGo6czOXg8VMcOp6Dqq2rVEp4cCDDh8QTHRVDVFgojToDFW1aCpQ6yqoaaSvPQ1l4ClNrHUiscAqKxj06mehBccR4yQh2luGnkNHeXMeXK1ey/8ABnJ0cue+uO5l19Yx+zUhqtZq33nqLDd9vICgoiOcXPk9CQsIlv0NRIAZGFIg/CG+/9yFv/ecDnpj7CPfMuf2i5+r1ep5d+hbf/LCZEcOG8OoLT+HfTz5Ea7uauS++ScahLGZNT+eFuff1KiuRX1LBrfNextbGhq9enU/YT5yTFouF+5YsZ1dmDp899y9SE3su5w0mE7e88Ck5JTWsf+le4kJ6mhL+bJgtFnadLOHz7Vkczq/ExcGeW9ITuWn8EAI8/zuHuMUiUNbYSk6FkjMVTTSpOzGYLBhMZqytrIj0cyU2wJ3YAHdCvJwvyVzVrtHx4eajLN+WiY1UygNXj+LOKcnY/aT2U4OqnY9+2M/KHcewWCxcM24Yt10xlkCvCzZ/vcHIrmOn+HFfJsdyCrAIAtEh/qQlJ5CaNIiYkIAe/bFYLGTll5J5KpfMU2fIKyzBbLFgZ2dHUHgEcr8IDAofGlq6zH4N9e2gb8Oq6SzWLcUIJj32rl7EjJnE+NHJhLg64KeQ4aawpbikhE8+WU7WiZNER0bw5L8fYvCg/sNcjx07xguLXkCpVDJnzhzumnPXJdV5EgViYESB+AOwfVcG/3zoUa675iqWvfT8RQeG+kYl98xdwJn8Qh64azYP33NHvzOs4zn5PPTcqyibW3jm4X9w67XTe7V9pqic255Yiszejq9emU+If0+hefnzDXy4fifP3fM37piR1usez37yAyu2HeWth27g6rFDeh3/s9Cm0bHuQA5f7MimUtmKn5sjt01O4pYJQ/uNPLoUBEHg4Nkavt6fz768KtTarggfOxspPi4O2FpLsbG2QmcwU97YhuXcTy7Y04nrRkUze3w8roqBTSflDS28tHo3O08UE+zlwnO3TmLCkPAe59Sr2vjP+r18vSsLi8VCelIcd8wY10vUm1rb2XH4BNsOZZNTXNHlT3B1ZszQWEYkRBMZFoLLOfPP+YilEqWKoqJiMk/nUnT2LPpODUissMg9sTgG0dwpB8m5HAgvJyQtpdiqcrFoWnDz8mHGVTOYnjoWD6euZ3WV27Bj9x7eeO8DGpVN3HDtNTx0393I+tncqKOjg6XLlrJlyxYGDRrEi4teJCAg4KLvTBSIgREF4jJT39DIFTOvJygwgDUrPr3ovsGFJWXc+dA82tUdvP7Cgj7DV6FrUPpkzQ8se/8L/Lw9efuFxxgcE9nrvJzCMm57cikKuYxVr8wn8CclNABWbzvIU/9Zza3Tx/HCfTf0EpdvMo4z7/31/GPGWBbcNv1XPH2XTf1UaR01ze3dPgMJEtwcZXg6O+DprMDTxaHHjPi3wmIROFVWx+qMk2w8mo/OYCIpwp8505KZMiyqO/fg12C2WNh8vJT3t54gv7oZd0d7Jg4OITnch4QQTyJ8XHu1rzOYKKprIadCyebjJRw8W4PM1ppbx8fzwBXDcJIPLFT7csp44audlNSpmDIskgU3pxPk1XPjoMKaFr7eeZRvdmfSodWRFB3CrAnJpCfF9ao6q2pTc+BEHvuyz3A0pxCN9lwyY6AfMeGhxEeG4uLtg9LQNVQUKHVUNqqpLC6jtaIQWiuRCjosSNHbeNFp64/J2gk3Hze8vBUoOqtoyTtIR3MjkRHh3Dl7NsMHx3TnSHR2anlv+ad8ve47Avz8WPTMfBLi+ndKb9++ncVLFgOw8LmFpKWl9XuuKBADIwrEZUQQBO64518cP3GSjd9+fdG9G3LyC7jt/sews7Xl07deJi6694AP0K7uYN7it9m+/yhTUkey7KmHemwWc56D2bnct/BNXJwUrH71qR7F9wD2nchnzvPvM3ZoDMufubdX8b3swkpuXric4bEhfP7U7b+oOF9tczsrd53gUH4FuRUNfRat+zkezg74uTni6+aEl4sCb1cFPq4KvFwufJzl9lhZ9b36Mpkt1KvUVDW1cqa8gWMFVWQVVtOq0SG3s+GaUXHckp7Yw9n7a9l+sozXvj9GYW0LEb6u3D15CNekRGJn88sGpMJaFe9vPcH3x4pwdbBn7jUp3DQ2tt9nPI/BZGb5lmP8Z+NhTGYL/5g2nH9dNQoH+65Bt6Wzqwy7ulPHuj2ZrMvIokbZgoujnBmjhzJtZAKxIX49JgQqjQGz2UxhRQ0HTuRzprCEwvIqjCYTEokEJ3dPPPwCsMg9aDPJaWzUoKpXgSBgrcxHJu3A3lqLFWYMUmesvOPwjIjB39uRAHc55qocjmzfRIdazbSJE7hr9k2E+l4IAMg6cZLnFi9F2dTEP26bzV233dLvyrmmpoYn5z9Jfn4+N998Mw8/9HCfJidRIAZGFIjLyA+btvDveU/x/NPzufXmG/o9r6yymuvn3I9MZs+qD94k8GcO5vMUllZy71OLqalr5Il/3c6cG67u01y149BxHnzxXUL8ffhiyTy8PXpGPRVW1vG3ea/h7+XO2pcfwVHec1lf3djCzAXv42Bvx/dL/jlgeOh5TpXW8em2TDYdOwvAsAh/hkcFkBQZQJivG9bn8hYEAVTqTpRtGhpbO2ho6aBW1U6dSk29Sk1DawdtGl2v9iUSUNjbopDZYW9rgyAImC0WjCYzTe2dPYQoxNuVlOgAUqIDmZIUheNvUBnWbLGwaO0hvsg4Q7iPC4/MSOaKpPABB/SByK1s4oW1BzlWVMeoaD9euzMdX9eBo7AaWtQsXbuX7w7l4u2i4N/XjWXW2ATU+p4/a4vFwtG8Ur7NyGT/yUJMZjOB3m5MG5HA2CFR+Hh5djvlf5oEV9Gi5tjZUqoqqygoLqdFWQ+CgCCRopc4oWs3oDfYIND1/FLvIGSGOhSmGiSmTqRyFwKGpRKbEE+MlwwvWzi0Yxtbtm/HUaFg4WP3M25Ecvf91B0dLHvzHTZv38molOG89OxTODs50RcGg4G3336br9d8zbDEYSxdurRXcp0oEAMjCsRlQqfTMenKa3Fzc2XDmpX9bu6jbFZx/ZwH6NBo+ObT/xAa1Ldddevewzz24pvIZfb8Z9ETvUJYz7Nh50Eef+UjEqJC+Wzx4zg79lxdNLWqufaxV9EbjWx49fHuOj7n6dDquW7BBzS0tLP+xfsI9++58ugLlbqTZ7/cwaZjZ3GU2XLD+CHcMSnpv3L6avVG6lvUNLZqULZ10NjaQatGR4dWj7pTT6feeE5wrLCWWuHp7ECgpwuBns5E+nvg5XLpYa6Xgt5o4uHlu9h2soy7Jg7myVkj/ysT1c8RBIG1B8/ywtqD2FpLWXpbGlOGhl7StceLqnlpdQYnSmqJ9HPnX9ekkprQ914ObR2d7D6ez9Yjp8k6W44gCDg6yBgWHUpUcAAxIQH4e7lTfy5R8Xy0UpWyg+q6FpoqKjCoarA1NSO1EhAE0Bls0OrtMLlHgUSCm7cLLtJmjFUnMXS0ERgaxo3XXY2/vz9+ChmtTXUsfes/lJZXMuvKKTxyz+3If+J7WP/Djyx98x28vTx57aXniQwP7/Uc59myZQsvvvQibm5uvPLKK8REx3QfEwViYESBuEx88vkKFr/yOl999jEjU5L7PMdgNHLLvf8mr7CYVR+8wZBBfdteP1u7kUVvL2doXBTvvfQkPp59l9z++JtNLPnoa0YNjeXD5/+N4mcrA6PJzC1Pv01OcSVfL3mEIZE9TV6CIPDvd75h48HTfPn0nYxJ6P+HCV0z6jV7T/PKN3vR6Aw8cM1o7pyS/JvM1v9oPLNqHyv35vHsjWO4M/3SQy1/KaUNrTz08U5yq5q4a9JgnrhuBDaXuHf01qxCln2zl/KGFuKCvbnnytGMG9T/pj8t7Rp2nygkK7+ErPwymtu6drhzkNkTFuSHi6cHJpkr7WZb6pu0XZFKgKpehaW+BBtrM/a2BmS2BqysBEzYIXENxTM8Bn8fZywWM0JDIWXHD6DX6ZiQNp45f7sGb1cHDEYjX6xayzff/0igvx9vvjCf4IALzvTTZ3KZ9+zzdGg0LH52AaljRvf77Hl5eTz++OO0trXy0ksvkTY+DRAF4lIQBeIyYDQamTDtaoIC/Vn1+fJ+z1v27kd88Pkq3l78LDOmpPc6LggCb366mnc+W8OU1JG89dxc7PrIQbBYLCz56Gs+WbeFK8eP4NV59/a5D8Oi5ev49IcM3px7O9eMH97r+Pp9J5j77rf8+4aJPPS33v35KU3tGv759ndkFdUwIiaQF2ZPJipg4NXGn5GzNc1cuehbZqfFs/CmvgMHfkv0RjOLvz3Ml3vOkBTuzbt3T8bnEkxO0DUJWJmRwydbjlDT3EZskDe3TRrOhKERfW6Ler4AnyAI5Fd07Z1xvKicksoams9tGSuxskIqc8IgyGnrACP2WOrLf9KKgMzTBSdpCxJDG1a2MnyihzA4cSixPg7otFpO7tvNkSNHCQrw54E5t5EY07U6yj6dw0uvvo3FYuGpuQ8xPPFCpJxZq+bRp54hv6CQR+//Jzdff12/Ytfc3Myjcx8lLy+PuXPnctONN4kCcQlcbOyULly4cOH/tju/D2vWrOHGG3uXhrhcbNqynW83fM/CBU/265g+fuoMTy5axo0zr+Rfd97a6/j53d/eX7GO66+cxGtPP9JnSQ6T2cy8Vz9m1abd3DFzCksevavXPssAG/Zk8vLnG7jjqjTuvW5yr+O1Ta3MWfIlQyMDWfrP67C6SChulbKVmxavpqyhhZfnTOeZv6fj4TxwBdo/K8+s2o+yvZMP7puKve3vv8+ytdSKCQlBhPu48PWBfL45dJZo/67ciYGQWlkR4uvJ9eOH4ufuzNGzFXx38DTr9p9CrdUT6OXaY4WnNXY5tCUSCVJrW0L9vQkJDSIkOprBg2IQHFwxClK0mg4EbTMyiRo5rdg4OiCYDAgeoUgU7pit5Nh7h+Ps64fUqEZVUYiyohhPLy8SwwIYFB9HYEAA2SdOsHX3XmztZPj6+uPi6sHIlBRyzpxm/cbNuLm6EBUeBoCVjR1paWnU11Sz6tt1aHU6RiQn9SkScrmc6dOmU1JSwurVq9FoNIwfO/J3K43yV+FiY6coEL8T8599AQcHOc88+Xiff6Amk4l7H1uAtbWUj99YjG0fu8V9sHId73y+ltnXXcGLj/+zR22c82h1eh5Y9A6b9x3j0Ttm8dicG/r0dRRW1nH3ix8yLCaUNx69vc8s4Sc/+I6SGiUrn7nzok7popombl6yGo3ewIp5N5I+tO+NZv5KvPFDFolh3swaFf0/vW+0vxtTh4ay50wVn+zs2lkuJcp3wCxvnbFrR76YQG9uHJ/IkHB/mtWdbDiUw1e7jnM4vxy1VoenswIvZ4dukdAazkU/GUy06U3Y2tqgt3bAInPFaO+FvUcQGoMtOp0ROzTI5QIy2pA7WGPv7Iq3vzuBQT4MHjYUV09vmmsryc46TkNbBwpPb2JCAkhOTqKmppbtu/eibFeTNCgOhULB2NFjqKis4Lsft2BnZ8ug2K53bWNtzZgxYzFoNaz+dj2q1lZGpwzv8+/c2tqaSRMnoVarWf31asorKpmQlvqrt8f9/4AoEP9jTp7O4e33PuTh++9j6OC+bdUrv9nAtxu3svTZJ/oMZ9269zALlr3H1ZNTWfLE/X3+GNQaLXcueIVDJ/J4/sHbufv6K/ocqLV6A7OffReLxcLKRQ/i5NA7EenwmVJe/mobD1+fzuSLVGitaW5n1qIVIJGw6ombSQj5/beN/CPw2oZjJEf4kDYoaOCTf2PcHGXMGh1Fk1rL57vPsC+3mhFRvhdNrtMZL1iOJRIJgZ4uTE2OYcaIOFwd5RTXKPnxSB6rdmez+VgelY0qLBYBub09NtZS1AYTLvY2tOlNNHeaaO804OhgS4fWhIOLMzocERx8EWwckSBgpWvCStuAjUWLvcIJd1cnHJxcSBuZiMGg52T2CQoLCnH09kUiUxAWF49ep+fooUMUVlQREh2NHgmTx46mobGB7zdtQRAEhibEdz/DsGFJWAlmVn+zjuraWtLGjenz793KyorRo0ZjZ2fHyq9WceLUaaZPmTTglr3/X7nY2Pn7r5X/C9LT03FwcMDKygqpVPqH8jFcjO++/xF7e3uuvWZGn8c7tVre+WQFI5OGMi09tdfxipo65i1+myGxkSx98sE+xaG9Q8Pt85eRW1TB2wvu58rxI/rtz+JPv6O4qp6Vix7Eq5+9FT7auB9PFwX3XHVx+/rr6/aj0RnZtOgOwn0vvjf1XwkPZxnljW2X7f4yWxtenp3GuLhAFqzcx5UvfsvT14/m5nGxv2j15u/hwj+mj+Qf00dSpWxl7+liMgsq2Z51lh8O5SABAr3c8PfxIMTXA2uFI5HujkBP/4efl4Laxg7AGQjE01lKc1UZzdVlqI9mYFHHExGfQGkb+A0Zg51XMCf37mD1yq9Jmjgddx9/fJPTcXZ1Ze+2LXz+xQruuH02jTp44L77sLWxZeXadTg7OzLzimlAl0g8dN/dOMjlvLf8U5ycnHj8ofv7fH6JRMIdt99BoI8nTz6zkLvvf4QP33kDB4dLC9cW6eIPLRAAX3zxBW5u/deQ/6NhNBrZvG07Eyek9rsr3Kp1P9CsauG9Zb1LbhiMRh589hWsJBLeXTSvT4e0WqPltieXcba0kveefYhJo/svu52RlcvKLfu565p0xgzp2zxSXt/MnhOFPHJ9ep+O7fMUViv57tAZ7p6e8v9KHADSE4JZsecMaq0BR9nlK1R4ZVI4SWHePP7FHhZ8tY+tJ0p5eXYafm6/PKQ30NOFWycmc+vEZOrbOjlTVseZslpOldRyqqCcw6cLAbC2luLl7kqAryeOzh44u7icW5X0vGdYgCcG/WDys7MpyT1DVVkZAXFJOLi4gbULIaOmU35sJ8e2/UBgYiqDBscjD09kwhVSMjb/yIqVq7j9tluxsrLi7jl3ou5Q897yL3BzcSF19MjuzOs5s/9OW3s7X6399v/YO+uoKLM+AD8zdEtKg1IqiogYgGIrisna3a1rrWussbq2u/Yaa3djYSIqNjYqFioYCJLSPd8fLH7iDBOIruv6nDPnKO+973sHZu7v/hqDMmXo30vcf1dI+3atUVZW4qdJU+k9cCgb1qyQq1vjdwr4Z8tVfoPcvH2HhMQkfJs1lXg9JzeX9dv34FHDjRquLmLXV27Zx71HT5k/aSQWpiYS5w+bsZT7TyJYIUM4xCWl8NOSrTjZmPNTj1bFjjt+5T4AnRpJDsUtJCDkIQIEDPatLXXct0irGvZk5+az6sStf3opmOprs2mkL792rsONp9H4/LqbfZcf8SkBiarKyrg5WNGzaS0m9/Bl/c99+GNYJ7q3qIOniyOqSgKu33nIpXPnCT51iqgnYRgpZ70v6f3+PmrqGNi7YFvNE1G+iOc3LhDx5BlRb1OJSwVNR2+UNMvw8mYwYfcfAVC1Rk28mzYjNPQux4+fBEBJSYnp40dT0dGeBUtX8vR55PtnCAQCRg0dhG+zJqxct4FDR49LfW9tW/my7Pd53Ll7jyEjx5CVnS11/Hf+z1cvIPr164efnx+7du36p5ciF8EXLqGsrIynh2STz4mgYKLfxtG3a3uxay9eR7Ny615aNqpLU2/xTVgkEjHpj3VcuHmP2aP70ah2NbExRcau2EFKWgaLx/aSqhlcuBuOk3VZma09bz2NwsnKGH1tycXUvmWq2prQtpYDa0/d4VlM0j+9HIRCAT0bVObolA44WRowbuMZ+q84TnRi6iff20BbFaFQgKWxPq1rVsavYU1GdW3BzKEFAsPeoix3Hz5j894T7D58BmFqnJhw0jE0wb5WfZQ0tEmJvEtW4puCdSurouPohZqOHi9unCU5IQ6AarU9qVWzBqcCT3P15m0A1NRUmfbzWLS1tfh13kISk/5v4hMKhUz5eRy13N34bcEfFrliBwAAIABJREFUXL1+Q+p78mnamLkzp3HpSgjjJvxCXl7eJ/+e/gt81QJix44d+Pv789dff7Ft2zauXbv2Ty9JJpeuhODqUgUdbckq/84DR7CyMKOBl7gAWLRuO0pCIZOG95E4d93eY+w7dYEfe7ajg4+47+JD/M9e49TVUMb1aEUFWwupY++Ev6JGBVupYwBevE2ivOm/x9xX2kz6wQM1FWXGrA8iO/fr2GBsjPXYObY1k9t7cPnRa1rM3EvQ3UjZEz+i0Hzz/v/a//+/uXZBLwcH4zK0rlmZyd2as3pMD1p7VyczI4MjgZd4fj+UvNzcIvdQVlFFt5wbytplSH0ZRnZKPACW5gbY1myMipoat86eJDc3F9symvzwQzvsbW1YvXk7gryCLG4D/TIs+nUC8QlJzF3+V5H7qygrM2/GNGytrZn462/ExsVLfY9+bVox8afRHD1xit/mLfwkjeu/wlctIMqWLSiqZmhoSJMmTQgNDf2HVySdzMxMwh4+wt3NVeL1t3HxXL52i7Ytmoo5np9GvuJw4Hl6+LWQmCV9+0E489buwqduDUZ2byd1HbGJycxcuxe3CuXo27qB1LHpmdmkZmRhbiQ7vt5YT4v45HSZ475VjPU0mdujHnci3jJ+05mvRkgoCYX0b1KVw5PbY6qvRb/lx/jj4JVPXp+BtqrEF4CulgbdG9Zgcn8/PKtX5mF4JJeCL5CellbkHhZmZXB090RdW4e0l3cx/PvcVN7KBF9fH9LeJXL/WggANvq6DO3Tg+zsbJZv3IGBlioGWqpUcrRnQPcOBAZf4uylkCL319HWZt6vU8jMyGT6nPnk50svCtm/d0/69uzG5m07Wbdpyyf9fv4LfLUCIj09ndTU1Pf/vnjxIg4Okqubfi2EPXxEbm4urlUlh7YeP30OkUhEKwkZ02u2+6OmqkL/Lm3FrmXn5DJ+4V+UNdJn7tj+MqNW5mw8QHpGNvNGdJOYO/Eh79IyANCTw2xkaVyGJ6/jyPlKNsZ/ghbV7fipbU0OhoQzYMVx0rPk75H9ubEzLYP/hHZ0r+fM1rN3GbfhFDlyVNEt5GMtothxHwgKoUBAxwbVaetTl8yMDK6cv4iJjnIRB7atmT61GzRESSgk/uk9GlcywslYHW/Xini4V+PMhSvkp6ZgoK1KFQdruvu1JPjqDa7evv/+Hj06tMWxvC3zV6wl+6O+5OVsbRgzYihXrl1n2+69Mtc/8acxtGjWhDkLFnHu/EW53vN/la9WQMTHx9O1a1dat25Nhw4dqFevHt7e0s0q/zSPnzwFoIKjo8TrwVeuYWNpjn25opnVySmpHA4Mpm3T+hjplxGbt/nAScJfRPHriF7oygjTC7kfjv+ZEPq1aYC9lewcBb2/cyLepWbIHNuyVgXiU9I5fv2RzLHfMkObuzG3Rz0uPHhFryUBJKdn/dNLeo+aijIzu9ZlUoc6XAh7yfTtZ8nPl9+UUnhqlxfzvw8W9SuXp1OrBuTm5HD3doGmb2WsjZWxNk7G6lQrZ4RHTVfiY6LJT0l634K0X7vmqKqoEHAm+P3zB3b0xcRQnzW7Dr43A6koKzNqYC9iYuPYfeiY2Dp+aN0Sb08PVq7dQOTLl1LXLBQKWTB7Bk6ODoyd8AtvomPkfr//Nb5aAWFlZcWhQ4c4dOgQAQEBDBky5J9ekkyePnuGuro6Fubipbpzc3O5cv0WdWqL1z86eCqYzKxsurRpJnYt4V0KS7f6U79mValOaYC8vHymrtqNhYkBwzv5yLVmTXVVdDTUiIqT7Xht4GKHbVl9/jzy6eaLfzud6lRk2YDG3Il4S/dFR74qIQHQ3rMiI3xrcOzmU7YH31N4fqGgKO4F//dTFAoJ9/Jm+HhW5W10NLr5aTgZq9PUXh8bvYKe1L5e1dDR1OByyJ33rUdtTfVp09CDM1dvk5leYDFQVVGhzw++3A57zO2wx+/XVMutKp7u1fhr225S04qaOgUCAZPGjUJVVZVZCxfJ9C+oq6uz/I/5ZGdnM2LseHI/8p98p4CvVkD8G3n5OgorC3OJiW3hES9Iz8iketXKYtdOnLuMva0llZ3EK6du9D9BanomEwZ0lvn8Q8HXeRQZxYTebdFUl7+aqpuTNcF3wmV+qYRCARM7N+Dhy1jGrz2q0Mn0W6RFdTtWDWnGw9fxdFt0mCQJ/Sv+SXo3qoqztTEnbz8r9Xt/KCQMtFXfO7L9PApCt4XpqXhYGLz/ubm2BuUM9GjgXonQJxHoqCu9v083H2/yRSLOXPu/j7FV47poqKsRcPZSkecO7tWZ1LR0jpw6I7YmYyMjhvTrw41bd7hyTXpUE0D5crbM+nUKt26HsnzVXzLH/xf5LiBKkZcvX2FVTI/c+w8LTkJVKhY1PyWnpHL19j2a1BWPakpNz2DTgVP41K2Bo6303ru5eXks3nEU5/KWtPCU7CQvjqY1K/EiJoH7z6Nkj3VzYFx7bw5eDmPallPkyXAKfus0rGLDmqE+PI5KpPuiI3J1zvtSCAQC6lS04v6LtxKbL30qH5qiCgWFtYkuJmV0eP4mTqKDu3qF8mRm5/AqOub9fCtTI+wsTTl34/+ajoaaGvVqVuP0pevkfHC6d3ZywNnJgd2Hjks80Pi19sXc1JQ/166XK0qpVQsf2rVuyYrVa7l5+86n/Dq+Sb4LiFLkbWwsphKS2wCeR75CWUkJW6uiG/2dB0/Iy8vHy72q2JzASzdJSUunr59sc9G5m2G8iI5jWMdmxTYmKo7mtSqjo6HGot2n5Ro/tGVtBrWoxdagW/RftI+UjK/LvPKlqV/ZmgW96nP/ZRyBdyL+6eUU4XV8ChqqKqgqf55idZJ8Fnn5+WiqKYuNMdBSxdW+IOT6zUcmTbdK9jyKeFVkU/eu4Upyahrhka+KjG3r04jIV695GvFCbD0qKir07taZsIePCL0XJtd7mP7LBMqamPDLr7PIyfl6gg6+Br4LiFIiLy+PpHfJGHzU8rCQV2+iMTM1Eeuxe/dRgWO7smN5sTmnr9zC2EAPt0r2Mp+/48RFjPV1aVxTPDtbFvo6mgxtV5+gm48IvvNE5niBQMCETvWZ1bsZ5+89x3fKBoJuP/1Px5X7utthbqDNlrP3ZQ/+QiSkZHDy9jNaVLdHQ+3zFqorFAB5udnEJ6dRtbyZROFR2DM77aNDhY2pMSlpGSQm/z/Rz6VCwee+8DtSSJ1a1QEIvnJd4lqaN2mMro4Om3bslGvt2lpaTJn4E48eP2HTth1yzfmv8F1AlBIZmZnk5+ejXUyCXFxCAsaG4klmES+jKGtkgK6O+LxbD8LxcK0kUyPIzcvjwu1H+NZxQ6WEJ8U+LTwob27EmOV7eBMvX1G6rg1c2TGxK8pKSvRbtJce83dxP/K/GRGSk5uPUCAgPfvrOIFm5+YxbsMpADp7O3+x5247XWD7r+kkuQdK5t9hwR9/To3/zuJPSE55/zNTY0O0tTR4/rKo6dPEyBA7W2tu338g8Rmamhp0aNua4IuXeRPzVq51N23UgHp1vVi28i/i4hPkmvNf4LuAKCXy/o7qUZbQqAcgPT0DLU3xENXYhCRMjMQFR1pGJlFv43Gwlp4FDfD0VQxZ2Tli7UMVQU1VhdXjupGZlcOQ37eTKedGV8PRkhOz+jKte2PCXryl5dSNdJu3kxPXH/+n8iX+PH6LV/Ep/NS2+Kq6X4r8fBEzdgZz+3kMv3atR/mykrXa0ubM7SdsDbxOa4/KxXYWfPq6YMMu/1GfcwGSK7KaGOgTlygeYVfRwY6H4cU731u18EEkEuF/9JRcaxcIBEweP5bMzEwWLl4q15z/At8FRCkhy7ySnZ0jsSnQu+QU9CRoD9GxBacYSzPZLTyj/7bnWpp8WhkMe0sT/hjegdCnr+k3dzNpmfL5FlSUlejdpDpn5w9kfId6REQnMHiZP15jVjJ9ayBXHrz4qpy3pUlqZjbjN51hWcANWtewx7OCbIH+OUlKy2TgyuMcvRHO0ObVaVZNek/x0iAvP5/VRy4xbs1BKlqXZXzH4lvVXn8YgbKSEDuLskV+nvl3AT0VpaIHLH09HZKSxetLOZSzIT4hiaQPNI4PsbIwp2qVyhw5GST3+7ArX45e3Tqz1/8Qj8Ofyp7wH+C7gCglNDQLYsEzMyUnnKmqqpCTK34qV1ISSiwPUBgdJE+z+rKGBep5dMKn9ytoWrMSvw9rz9WwCLr+uo64d/IXf9PVUmdIy9qcWziYtaN/oJqdOTvP3qHL3B3UHLmcn/4K4Pj1R9+MUzsqIZX28w6w7/JjhjV3Y0Fv6WVNPjc3nkbjO3Mv58NeMr6dB/2aSM+bKQ1exSYxbOleVgdcwrdmJdaM7oimuuREu5zcPI5eukPdqk7oaBatAvv6bTwCgQAz46KHHIFAKPHwZVa24OAUExtX7Nrq1/Hk4ZOnREXLZ2YCGDygL5oaGixdsUruOd8yX30/iH8LqioqqCgrk5IieUPVUFcnLV1ceKiqqEgsP1zYDzpHjqqT1qZGKAmFXAl9TMs6xZf/lpd23q5oa6oxcvEuWk/4k2WjOlG9GJuyJJSVhDRytaeRqz3pWdmcC33OiRuPOXnzCXsv3ENZSUgNR0uauDnQrLoj5oa6n7zmL01Gdg69lgYQk5jGxpEtqFvJ6h9bS1ZOLusCQ1l06DrmBtrsHd+OKjayNc9P4fGrWFYGXOXwlTBUlJWY1685HepWkVoGZn9wKPHJqXRpXB19zaIHn9cxbzEzKkNZvaL5O0LyUVIWio23/9uElZ6SJHatkBYNPFmycg3nLl2li1/x5e4/xEBfn949urJi9Voehz/F0f7za2BfM981iFJCIBBgYmJMzNtYideNjQx5K6HapKmxIVEx4qcgi7JGCAQCIl/LdvpqqqvRxceLnScv8ShSdi6DPDRxr8ieGQNQUVai8/S1rD50vkQ5D5pqqjSv4cTiwa24sXwkOyd2ZYBPTeLepTFj22m8xqykzfTNrDt+jYSUf08hwFl7LhP+JpE/Bzf9x4RDXn4+ey4+pOGUnSw4EELjqjYcnvzDZxMOIpGIyw8iGbB4H80mr+fE9cf0burO2fkD6ejtIlU4pGZksWD7SVzsLGjkVrRxlUgk4srdJ9SQEK2XlJJKGV1xE6yuTkHTn5TUNLFrhdiXs8HIUJ+QW4rlN/Tu0RV1dXXWbtis0LxvEbk1iOfPn7Nu3TqioqKKpKVv3vz9l1iImakpUW/eSL5W1phjgbHk5uYWcWRbmpcl+uQ5MrOyUFf7/+lJQ10Ni7KGPHouva5MIWO6+nI4+AZjF21m84zhGEj4UilK5fIWHJ47lJ9X+TN363FOhoQxs39rKtmKlxKRB2UlIbUqWFGrghXjO9bjWXQCJ2885ti1R/y2I4j5e87RxM2BLvWr4lnJRqFWml+Sey9i2RYcxoAmValTUXoC4+fiaXQiYzec4U7EW1xsjVnYpwEeTp/H/yESiThx4zErDl3mXmQMBjoajGzjSe+m7nL1BsnNy2PMsj28TUxhxZguYlF5DyJeE5eUgqeLeA2zuMRkqjuLd0LU+tukK0krL0QgEFDLzZWQm4pVgTbQ16dDuzbs3LOPcaOGY2L8ebWxrxm5BcSPP/5I586d6dixo8KJWP8VbKytuHDpssRrjuXLkZOby7PIlzjalXv/c5cKDohEIm7df4SHW9EcBg/XShwLDiErOxs1VekF1PR1tVk0pheD5/xF54mL2fTrMMyMPj16RVdLgz/HdOHA+TvM2nyU1hP+pJdPbUa2byhXBVhplDc1YLBvbQb71ubhy1h2nbvDgUv3CQh5SBVbUwa3rEWz6o4ofWWft7jkgk2puZt47srnJjs3j1UnbrPi6A00VFVY1LchbWo6fDZhGvr8DTO3neb6k9fYltVnTh8f2nk6o6Yq39aRn5/PxNUHOHX9AdP6tMTN0VpszP6gqygrCWlYo2gZmuTUNBLepWAtIVBDRbkg4CNXhgnWzcWZgFNniImNo6yxkVxrhgItYuvO3WzbuYfRI4bKPe9bQ+5vnrKyMl27dsXFxYXKlSu/f33n/5SztSbmbSypaeJqb+W/S2zcfVC0EmpNV2eUlIRcuCauBrdp6ElqeiaHgq7I9fwG7s5smj6UqLhEWo2ex/bjF0ol1FQgENDO25XTi0fTuZE7G45dxnvEQpbtO0NKeumUcKhgZcy07o25sngYc/r4kJKRxbDlB2kycR3+F+9/VSU9Cjfj3C+8pty8fDosOMCiQ9doVq08p37tRNtajp9FOGTl5DJl80naTN/M85hE5vb1IXBufzrXryq3cBCJRMzYeJS9Z2/yY/uG9G7uITYmLSOLPYFX8PF0xaiMTpFrz14WaON2VuZi8woPDbIEhGvligDcuitfVnUhtjbWNKrvzbade8jM/LpqbH1JZAqIpKQkkpKSaNCgAdu2bePt27fvf5aU9M+3XvyacPq7X8XDh4/FrpW3scKgjB4XrhbN/tTR0sTDzYWDJ8+JtUH0cK2Ei1N5FqzfzbuU4m2tH1K7iiP+C8Zha2bC5D930nTYbxw8d428Uggz1dPW4LcBbTgybxg1K9ryx65A6g5byKLdgbxNTP7k+wOoqSrTuX5VAuf2Z8XwNqirKjNmzRFa/LKBkzcefxXZ2g5mBZrZlUel4++Rl0evEwiNiOWXDh4s7d8YY13ppd9LSmZ2DoOW7Gfr6Vv0berOmfkD6VSvqkKaXHpmNqOX7WHT8cv08/Xixw6SQ183HTlLcloGfVuLX78XHgGAU7nizXiS8ic+pJKjPcpKSoSGPZR77YX06t6VxKQkjp0MVHjut4LMv7ifnx8//PAD/v7+rFu3js6dO+Pn5/f+59/5Py6VCzJWb9+9K3ZNSUmJep61OHcpRKy0cOfWTYmKieXc1VtFfi4QCJg1qg8J75KZs0b+EgAO1mbsmTeatVMGoa6myqjfN+Ezcja7T10mI+vTG7ZXsjXjr/E9ODR3KDUq2rBs31m8hi7gxyW7uP4wslQ2cSWhkBY1KnDk194sG9qanLx8Bi31x2/mFkIeyeeX+VyYG2hTy9EM/6tfVmDdfl4QsNDUtZyMkSUnL7/g9xx87zlz+/owpVsjdDTkrwwM8DQqlraTVnLoYihjOjVmcs/mErWc5LQM1vifpqG7M9WcbMWu33n4DAM9HSxNxU1M+aKCA49QKF1AqKmpUcHBjjv3FRcQHrVqUM7Whu27ZDch+laRKSCCgoI4ffo0x44dIygoqMjr6NGjX2KN/xqMjAyxtrLiSojkGjFNG9Ql6V0y5z5qm9i4Tk3MTIxYumGnWE6Es70tAzr4svv4OXYeFS9xXBwCgYBGNaoQsPhnlo/vi5JQyM/LtlG792TmbTpITPyna39Vylvw1/geBC0ZTS8fD87cekyHqWtoMmYJqw4EE5Pw6VqFUCigZa2KnJzdj3n9mvMmPoVOs7czcMl+nkZJ70H8OengWYHnMe84d//LCSv1v007z2I+Pd+lOA5feUDw3ef82qMJneqJF5CUxbnbT2g3aRUJyWls+aUPI35oUKwJbPH2AJLTMhjTraXE69fvPca1op3E+YUmR3nMa24uzty5F1akKqw8CAQCOrX34+btOzx7HqHQ3G8FuXXGzp3F+xFI+tl/nXp1Pbl89RpZWeLJYA3remBsaMD2/YeK/FxVRYXR/bsS+uAJAUEXxOaN7dMeb/cqTF26ibMhioXsCYVCfOu4cWzpRHbOHoVX1Qqs8Q+k7oBpjF+ylYcRrxV7gxKwNTXkl14tuLxqPHMHt0NfW5N520/gOWQ+vWZtxD/49idrLspKQjp6u3Bm/kDGtffmclgkzSavY9KG47xNkj+Zr7RoVcMec31tVhy9+cW0iJbu9lgYavPHoZDP0osjJzePJQcuUtHahG4NFEuyE4lELN59mj5zNmFhXIYDc4bgVaX4HIKHEa/ZHBBM12ZeONuJhwm/iU0gMiqG2lUrSZxfaI5VliORtKZbVdIzMgl7KLsQ5ce0adkcoVDI/oOHFZ77LSBTQMTGxnLv3j0yMzMJCwvj/v373L9/n6tXr5KRIbtN5X+NhvW8yczM5NyFS2LXVJSV6ezXirMXr/LgSdFU/nbN6lPRvhy/LVtP4ruiJ29lJSWW/jIcBxsL+k/5nRXbDspszv4xAoGAWpXt+XNCP4JWTqVLMy+OXLhJ85Fz6DVtBRduP/zkjU5LXY1ODd3ZM3MgpxePZnBbb55FxTFm+R5qDpzLxNX+3Hr84pOeo6GmwrBWHpxZMJDujdzYe/4uDcavYbH/BdIyP918Ji+qykoMaubK9afRBIe9kj2hFFBTUWJ0qxqERsTy666LpS6Y7kVEExGTyOAWtWSabj5EJBIxZ+txluwNol1dV/bNHISlcfERdHl5+UxZuQtdLQ3G9ZCcwHb5doFT2dO1OAFR8PmX1XMdoIZrQY/4qzcV7/dgYmxMXS8PDh45+lX4v740StOnT58ubcDJkydZu3Ytz5494/Hjx1y4cIELFy5w//59+vbti53d15FpuGvXLjp16vRPLwMLczN27fUnMSmJls3FW4hWcrRn276DvHgVRWufRu9/LhQKqebsyKY9RwiPfIVvQ68i6rOaqgptG3vx8k0sGw+c5M6jZ9Sr4YKGmmL2YYAyOlo0cHemW/M66GppEBhyl63HzhMYchd9XS3sLMp+cmSMvo4mXlXs6NPCA4/K5cnOzePwpVC2nQrh2NV7ZOfkYWdujLpqycpQa6qpUt+lPK1rV+J1fDJbg26x61woSkIhztZlUZZj4/hUKlgacjDkCdfD39C1bqUvkrdR0dKQtKxcNgTdJTdPhIeTeak999bTKI6GPGJEGy+M9bTknven/zmW7z9Lj2a1mDOoLaoq0qOclu8+zv4zIcwe1oVqTpL9Kat2HSHhXQoTBnSW7L9ITWPD7sM09a6Ns4RS+R+ipanJ4ROnSU1Pp41PY7nfVyF5eXnsP3iYul4emJnK7vP+b0Pq3imSk+PHj8s79B+hXbt2//QS3jNnwSKRo4u76PXrKInXl6/bIipXvZ7o/JVrYtfW7TooKufVWjRjyV+i/Px8sev5+fmiLYdOiZya9xZV/2GIaO3eo6LMrKxPWm9mdrZo54mLovqDpotsWw0TNRk6U7T39BVRdk7uJ933Y5LTMkTbT4WI2k5aKbLtMElUqft00YwNR0SvYhM/+d63wl+Lus3bIbLtOVfkOfpP0d7zd0W5eXmlsGrp7L7wQGQ7cKXozN3Iz/6sQvLz80UTNp8V2Q5cKZqw+awoq5T+TtuCbolse84VvXybJPecG48iRbYdJolGLd0typPj933/6UuRXZsRoh8Xbih2TG5unsjNb7Bo7LxVxY558TpaVM6rtWhPQKBc65w0a6GoindzUU5OjlzjPyQ5OVlUoWoN0W9zFyg899+AtL1TpgZRyPnz57l9+3aRV3h4OEKhEOOvINPwa9EgAMrb2rBp2w5ycnOoX7eO2PWqlZwICDzL6eBLdGrni8oHmdWulRxJTkll454jiBDh4ValyFyBQICLU3ka1a7Go+ev2Hb4NPtPXURHSwNHW8sSJZUpKylR2c6K7s3rYmdZlhsPnrPjxEUOnL2GproaTjbmpZKspqaiTJXyFnRu5E7TGpV4l5bB7jM32HTsMq/eJuJoVZYy2iUL3TQ10MHPqzI1HC25GR7F1qBbBIQ8xEBHE3tzo892uncw18f/6mPuRLylc52KX0SLEAgENKhiTW5+PhuC7nI9PJpGLjZoyJmfUByZObnsOX8XL2cbypvKVxl4xsYAYpNS2T61n0zNIS8vn4GzVpObl8eGaUOL1R5vPXjC5oOBDOnSqthWu++SU9m49wjNvGtT0UF2VFd6egaHjp+mYR0PTE0U26/U1NS4dSeUazdu0bt71682w7+kSNs75RYQW7duJSgoCGNjY9LS0ggICCA3N5d9+/aRnJxM9erVS3PNCvM1CQgdHR1evY5i/8EjdPBrg5ZWUXVdWVkZJ7tyrN+xl5SUVBrU+X8/aoFAQN2a1XjzNp4Nuw8jEkGtapXFPpTGBmVo16QO7pWduP3wKdsOn2b3sXOkZWZS3tIMrY+qZcqDUCikgq0F3ZrXobK9NaFPItl2/AL+Z66hVYqCAsC4jA4+tZzxq1eNvLx89p67xabjl0lMScfVwbLEpidrkzJ0qleVClbGXH34gq1Btzl9OxwbkzJYm5QplbV/iJJQiKaaCtuDw3AtVxZbE71Sf4YkhAIBXhUssTHWZcu5exy4+gR7M/1Per6BtgarAq5grKeFdxXZm25CchoTVvnTu7kHDdzEy2F8zM6TF9lx4iJzh3fFRUrvkh0BQdy8/4TZo/sWW0HgXUqBgGgqp4Aw0C/D2q27sLG0wN21iszxH5Odnc3+g0do3LA+JgpkZP8bkLZ3yv1tT0pKYv/+/UyYMIEJEyawb98+EhIS2LZtG/7+/qW22G+FoQP7kZuby5p1myRer+1ejb5dO7BlzwEOHCva1EQoFDJ7/FB+aNGQpRt2MmzKPFLSJBey83Jzxn/ZdNb9NpaKdtYs2exPnW6jGDpjKeeuhZYoQU4gENC4ZhUO/P4T66cOwUBPmwnLt9Nk6Ez2nwkplaS7QiyN9ZnWpyXBy8bSsUF1Nh+/QoORf7D9VEiJs6eFQgHNazhxbFZf/hjYksTUDLrP30WvhbsJ+wwd7/xqO2JaRovVJ26X+r1l0a62I7vGtUFLXYXeSwOYs++yXBWAJaGhpkITNwd2B4eSKkdJ9rdJKeSLRFQuL57p/DF5efms2R+Ii4MNLetKP0yeu3aXapXs0dUu3g9SeGCS121sbGiAQ3lbrt4s2d+ogXfdgrWdv1ii+f9W5BYQUVFRqHzQ8EZFRYWoqCjU1dVRlVEn6L+IjbUV7Vq3ZPvuvbyJlrwp/TxyEDXdqjLpt4VimZ5KSkrMnziSiUN7c+r8Vdr2H0vYE8kdtAQCAQ1XFzaNAAAgAElEQVRqubJh9k8EbVxIr7ZNCQl9SJ9JC/DuMYYlW/bzJlbxNooCgYAG7s4cWDiOtVMGoaWhzthFm/EZOZsz10u393JZA11mDWxLwILhOFmbMvmvg/zwy2qevpZcHVcelIRC2nk5EzR3AJO7NCD02Rt8p27kp78CSrVyrKqyEn0bu3DlcRR3I0u+3pLiWq4shyb9QPd6lVhz8g7t5x8o8ToGtqhFcnoWW4NuyRxb2Fdaq5j+Dx9y6mookdFxDP6hsVQTTVziO+49eY63u/RTfmGUlSKHlVrVXbl2K1ThfAgoyHFyrlSR4AvfBYREWrZsSadOnVi+fDnLly+nc+fO+Pr6kp6e/tVEMn1tDB88AIDZC/6QeF1FWZlls6diaFCGviN/5mlEZJHrAoGAAV3bsXXxTNLSM/Ab+BN/btlLdk7x7UBtLcoyeXBXLu1YyvIpI7C3NmfplgPU7T6KgVMXcTbkTolCZBvVqMLhReNZ8XM/cvPy6DtjJb2n/0n4y2iF7iWLCtam7JjWj0UjOvAiJgHf8cvZdPzyJ4UYqqkq09+nJucWDGJQi1ocuBxG4wlr2XfhbqmFLnauUxFNNWW2nL1XKvdTFHVVZWZ29WbFwCa8jk+l7Zz9LDhwlWwFa3FVszOnoasdSw5cJCImUepYE/2C2kmvYmUnXZ6//QAdLQ2a1pKefHftXkGZGq9q0uu8Kf2d//BxeRpp1HCtQnpGJg+flKxbnGetmtwJvScxx+lbRW4fRM2aNXF2diYhIQFVVVW6du1Kq1atUFFRoVkz8XDOL83X5IMoRE9Xl/y8PLbs2IWba1VsrMUTgrQ0NWlY14O9R45z6HggTevXRU+3aNEySzMT/Jo35NnLKLbsC+DomYuUt7bA2qL4kDslJSEONha0a+yFX2Mv1FVVCbp6mx0BZzgYdBkVZSWcylnKlWhUiEAgwMHajK7N6qCnrcGBc9fZcKiglk6tyvYK3UvWcyrYmNLO25WHL2LYeOwyt8NfUs/VEQ21kvkmoEBQ1KlsS7PqjtwKj2JT4E2uPX5F3cq2cp2Cpd5bRYnX8SkcCHlCj3qV32c9f2kczA3oXKci8SkZbAy6x5m7L6hiY0zZMvKHrdZ0smT7mdvcfvYGPy/n982rPkZXU51tp0JQUVbCp5az1Hv+vvUI5S1M+KGh9J7du46d5X54JNOG9ZCa45CRlc3aHQeoV9uNqhUdZL8pQEdbm/Xb9+BoVw7XypLzK6SRmZnJwSNHqevlgYV5yUref42Uig8CoFKlSvj4+NC0aVMMDQ2Jivqyxcr+jQzs15tytjZMnDqD5GL659paWbJp2QLSMzLpPHAkzyLFyzcYlNFl5awJrF8wlby8PHqOnsaQyXMJj5Bd6sHKzISf+nXkwrbFLJ08DD0dLaYs3Yh3jzGs3XtMYkc7aaiqKNO/bSPOrJpKxyYerDsYRLufFvL0Vena9030dVk/oScz+rXi8r1ntJu0kiev5G8fWRxOlsbsntyN33o15VZ4FG2mbyL0ueQ+HorQ1bsSWTl5HL4e/sn3+hR0NdWY17M+q4c0IzopjTZz9jFz90Uys+UzrZgZ6DKte2NCHr1k5ZHiKwkLBALquNhz+sZDmf3Lo+OSsC4r27n76PlLnMpZyoyIUv078k8Rc5G5qQkmRobcDXske7AEXF0KzF737j8o0fx/I3ILiC1btuDp6Unfvn0ZNGjQ+9d3pKOmpsbvc37jbWwsv86eV+y4io72bF+1iKysbDr2G86NO5JNFfU9qnN88zJG9+/KhZBb+PQcybjfFvPqjezNWVVFmZb1a+O/bDpb50/A3tqC2au307D3ePadPK+w6clQT4fZw7qwdsogouOSaD1mHv5nQmRPVACBQECPZrXZMb0/qZlZ+E1exdlbJfuCf4hQKKBbw2rsndIdJSUhHWdt58jVT/viO1sZ4Wiuz8Gripd0+Bw0dS1H0MzOdPN2Zv3pu7SevY97L+TzTfh5VaZ17Yos9r/AjSfFZ4p3bVKDlPRMAi6JF6j8EBUVJbmCDl5Fx2JjbiJznOrfUW7Z2cWbWyVRycmB+49K9vcxMjLEyNCQh4+/jr/vl0BuAbF582aOHz9OQEAAhw8ffv/6jmyqulRm2KD+HDgcwKEjx4odV9HRnt3rlqOjo023waM5cPSkxHFqaqqM6N2Js7vX0LdjawKCLtK461Cm/bGa6FjZBewEAgGe1ZzZtmAiW+dPwNhAj58WrKHTmFnva/ArQqMaVTi6dCJV7K0Zs2gzu09Jbpr0Kbg5WnNozlBsTA0YMH8rJ68pVt+/OCpZm3BwWi+qlDPlx5WHOX275Kd/gUBA6xoOXH8aTUySfOXZPze6GmrM7FqXjSN9eZeeRZvZ+5mz77JM34RAIGBmr2ZYGOoxdPlBYhIla7/uTjY4WJqwPuCS1AOGtoYGicmyfydxicmYGMpudKX2t4DIUNAfUNHRjqcRkSVyVAM42NsR/lRysMi3iNwCwtTUFB0dHdkDS5Hg4GCaNWtGkyZNWLNmzRd9dmkzbFB/qldzZcqMWUS+KN4sVN7Giv0b/sTNxZkxU2ezYMVfxTriDPX1mDS8D0E7V+Ln05AdB09Qv9Mgfl28hhgJ/a8l4VnNmf1LpzFv7ACevojCd/Bk1u45qrA2YWpYhk2/DsO7WkUmLN/O4WDJFW0/BXOjMmyf1h/ncuYM/2MngddLR9U31NVk07gOVLI24ceVh3n0quSRSI2qFsT3n7n3olTWVlrUc7bi5LROdPSqwJqTd+iw4AAv46RX29XVVGP1j36kZmQxeJk/WRJMVAKBgGF+9Xn0MoZjV4uPbKtU3oJ7z2SbQ/NF+XLl2giFQjTU1ciQIxz3Q8pZW5KXl8/rNyULrrC0MON11KebI/8tyC0grKys6NGjB6tXr2bDhg3vX5+LvLw8ZsyYwdq1awkICODIkSOEh/+ztt1PQVlZmcXzZyNUEjJ89E9SCx3ql9Fj4/IFdGnXipUbttFz+E/ExhcfpmpmYsTsn4dxesdK2jatx1b/YzTsPIRF67ZL7dlbiFAopIOPNyfWzqFu9SrMXrODgdMWkZKmWDFGNRUVVk0aQE1nO8Yt3srzqE/3F3yMrqY6myf3pqKtKcMX7eTxy9Lxe2iqqbJmlB+aaiqMWnW4xDkYTuYGmOtrcz7sn+1ZIQk9LTXm9KjHykFNeR7zDt+Ze9l/RXpPiwpWxvw+0JfbT98wdcspiWNaelbBwdKE33cGFtvB0MXehtdvE4iKlR4ZJRQIZXaJK0RLU4PUYvKDisPasqBv94tXJfOfmpmaEhsXR46USMJvCbkFhLm5OV5eXuTk5JCWlvb+9bkIDQ3FxsYGKysrVFVV8fX15fTp05/teV8Cc3MzFs2fzYNHj5k4dYbUL6aqigqzJo9l3tSfuRl6j1bdBhAioxqllXlZ5k4YQeD2P2no6c6yDbto2GUIuw6flCsc0NigDKt/HcX04T0JvnaXTmNmEh2nWP6EhpoqS8f1QVVFmd/W7Vdorrzoammw9uceaGuoMWrpbrJySmYu+BgzA12mdmvEw5ex+F8sWZ6HQCCgur0pt5+VvnAsLXzcynP0l/Y4WRowdkMQU7afl5pc5+PuxNBWHuwODmXXOfHPoJJQyM/dmvH8TRzbTkn2QTWpVdBv/ejFm1LXVtZIn2g5c3YM9HRJLCbwozhMjApKiMTFSxdUxaGnpwsgsa3wt4jcAmL48OEMHz6cvn37vv/38OHDP9vCYmJiMP2gcmLZsmWJiSn9LNgvTf26dRj74zAOHz3OmnUbZY7v0Lo5+zeuREtTk25DRrNy4zaZ5h8bCzOWzRjPvlXzsTY3ZeK8FXQcOpHnL2WfmgQCAT3bNGH97HG8io6l4+iZvFWwuZCJgR7DO/kQdO0eV+99Hq3PuIwOcwe340FkNGsPi/fQKCm+NStQtbwZi/wvlDhHwtXWhKjEVGLflV4yXmljaaTLzrGtGdzMlW3BYfRecpTE1OJ7L4/xq4OXsw1Tt5ySGPHV0M0Jryp2LN59mgQJvgZbc2Mq21lx8Nx1qb9Xy7JGRMqpeRqU0SUuQbHPppFBgYCITyyZgNDW0gYgJeXL9yD5J5BbQNy6dYsWLVrQokULAB4+fIicKRQlQtKH6FspkjW4f19aNm/GgsXLOHPuvMzxFR3sOLhlNT4N67Fg+V/0HjFeqsmpkGqVndj95xz+mDKap5GvaNlnFJv3Bci18dVxq8yW+ROJT0qm/5TfSctQrHF7T19vtDXU2Xu69B3WhTR2r4h3VQe2nLhSrGlDUQQCAT0aVSMqPpl7ESU7kNiZFtR8ioz9fJ3fSgMloZCf/WqzsHcDrj+Nps2cfTx6LflzpSQUsnRIa4x0tRiy1J+4j4SAQCBgam9f0jKzmLf9hMR7dGziwb2nL7n16Hmxa6rsUI5Hz1+SLsfnzbysMVExivmLNDUKapSlymF6lURhboaiPrp/K3ILiNmzZ7Nu3TrKlCn48FeoUIHr10vfEVmIqakp0dH/dyTFxMRgYiI7/O3fgEAgYO7MaVSq4MSo8ZN4Ei47s1NbS5Ols6cye/I4rt0OxbdrP0Juhcr1rLbN6nN8yzJquFRi+qI1DP1lHplyRH9UdSrPsl9GEPY0ktFzVip0otZQU6WFVzWOXbpNZin0wS6Onj61iUlMIeim4j2Hi6O+ix0CAZy5U7KMW0ujvzOM4xUzf/xT/ODhxK5xrcnOzaPTwoPciZB8gjfQ0WT1yHbEp2Tw40pxP42jVVl6N/dkz5mb3AkXD431a1ALHS0N1h86W+xa3Cs7kpefz80HsjVPS7OyRMfGk6VAqKtQKERdTa3Ezc4KvwPfymFVFgolypmZFc0eFJZSZU9JVKlShYiICF6+fEl2djYBAQE0bNjwsz3vS6OhocGqpX+gpqbKwOGjSUySrSoLBAI6t2vJgU2r0NHSptvgUazbtkeujdvU2JANv09j0vC+nAy+Qs/R03mXLFtNbljLlYkDuxB4+Sb7TsrWdj6kSW0X0jKyuPPk80X01HGxB+Dxy9Kz+RvqamJuqMtzGaUmikNXs6CJU2pm6Toy36Vnc/9VIu/SS1/gupYry96f2qKrqUr3RYe5Fi45UqeyrSm/9mjMpbBIVgVcFbs+sn0DjPS0mLrukJgA0dJQo2szL45dusWz15K1sxpVnFBXU+XkBdmHTzsbC0QiEc9eKNY2V0VFWW5H+MdkZhZoNhoaildL/jci9w5vZmbGzZs3EQgEZGdns27dus9ag0lZWZmpU6fSv39/WrRoQfPmzXFwkC+l/t+CubkZq5b+QXR0NEN/HCt30o+TfXn8N6+kUV1PZi1awfAJ08nIlK2SCwQC+nduw5LpY7kT9phOwycRlyhbMPVp14waVZyYuXKbQv4ItwoFZZhvPCjZSVwe1FSU0dFUJ/5d6ToNDXW0SlzQT+nvQnK5pVT19vyjN3RZcRq3X/bju/A4VSftw2XiXlr9foKFAaE8eqOYHb44LI102TWuDcZ6mvRaEsDFB5IT5Dp6u9CyVgUW7T/PzfCim7OOpjq/9GxB6NPX7Ai8Jja3X5uGqCgrs3Kv5BwfLQ11GtZy5dj5azI38Yr2BZ+vh+HFm6wkIRAIEJWwp3dhNQQdbe0Szf+3IbeAmD59Otu2bSMmJoZ69erx4MEDpk6d+jnXRr169Thx4gSBgYEMGTLksz7rn8LNtSrzfvuVkOs3mT5rrtxmHF1tbVYumMnEHwdzPCiYroNGE5cg34m3ZaO6rFswhRev39B//G8yzU1CoZC5Y/qTmZXFsm0H5HoGgIGuNjamRtx9+vlCPtMys8jIykZTveQ1miQhFFLiUNf0rIKoqk+txyQSifh551V6rDzLy/g0BjWsyPJeXkxu40qb6rZoqirzZ2AYzeYdo+nco/wZGEZcimK+oo8x09dm17g2WBvp0m/FMW4+Fc8XEAgEzOrtg5mBLj+uPExyetHPTysvFzwrl2fBjpPEJhXVUo31denazAv/M9eKDYNu28iL+KRkTl+WXlG2nJU5Whrq3A57rNB7FAgEJQ5AiE9IQF1dHXX17xpEEQwMDPj999+5dOkSly9fZuHChejry854/I5sWrdszpCB/di1dz+bt++Ue55AIGBAj86sWjCTR+HP+KHPUJ5FyGfOqVPDlSXTx3H3YTjjZy+T+YUpZ2lKx+b12XX0LC/eyG/OqWBrwaPIz1ezKyQsgty8fDwrl642G/cuHWO9kp0SX//te7Aw+LRT5tkHb9h15Rm9vR05PcmX8S2r0rKaNQMaVGRme3d2jWjE1RltmNneHT1NVeYfuYPn9IP8uOUS15+VPNnPWFeTbWNaYaavRf8Vx3kWI66h6GqqsXRIK94kJDN54/Einx+BQMCMfq3JyMxhzlbxygFD2jdFVUWZJTuOSnx+/VpVMTM2ZOvhQKnrVFJSolrlClwPVTxhsqQ+hOiYt5iZfnrP9n8LMgXEzJkz+e2334p9fad0GDNiKI0b1GPWvN+5fFVcNZdGk/p12LFmCekZmXQZNIrw55GyJwFN6tZi7MDuHDl9nl2HJSdBfcjwrm0QCgX8tUfyF1sSFWzNiYiKJV1GMbeSsj0wBB0NNdwrFN+hTFEys3OISUzB3KBklQMeRxVEAn1qd7llJ+9jZajFpNauqClLrpRrrKNBjzoO7BnZmMAJLejqZU/Q/SjaLw2k47JALj6OLtFp2VBHgw0jfBEIoM/So8Qmi5vbqtlbMNqvLkeuPuTwlaKbtJ2FMQPb1MU/+DZXwoqagIz1dendsh6Hgm9wT4J2qaykRFffBly8eZ8nkdL9CzWqVuLh0wgSkqRnhX9MSTWIV6+jMDcrvoryt4ZMAVG5cmWcnZ2LfX2ndBAKhSyc+xvlbKwZMWY8L14WXyBNElWdK7Bj9WJEiOg6eBRPnkXINW9wNz88q7vw27J1RLySXkKgrJE+bRp5su/kebkTlCqVt0QkEvEoovS1iOsPIwm8/pBBbbxL3KJUEncjYsjJy8fVTnanNElcfPAaC0NtLA1LXprmVkQcNyPi6FevAqrFCIePsTfVY7pfda7+2pbpftV5EZdKtz/P8MOSQK6EK+7EtzXRY+3w5rx9l07/5cdIk+B0H+xbi2p25kzbckrMnDSsXT0sjfWZuvYQ2R/VPhr8QxPKaGsye72/xM26S8uGqKmqsGH/calrrFvDFZFIxIVr8neKU1aSr3Dgx4hEIp5HRlLOpvQOI187MgVEu3btpL4KmTlz5mdd6H8BHW1tVi9fTH5+PoNHjiFdwVht+3I27Fi9GAECeo/4iZjYOJlzhEIhCyb/iLKSEj/PWSozvruvnw+ZWdnsOnpOrjVVKl/QAyM0vHQjmfLy85m5KQDjMtr0aeFZqve+/KBAA3Ozt1B4blpmDhcfvqJORctPMkOsCnqAjroK7WvK7rf8MZpqyvT2duTclFb81sGd6KR0Oi8/zbCNF3gZr1iCV7VyZVk+sAn3XsQxaes5sc1cSShkQf8WpGfn8Mumk0Wua6ipMr1vS568esuGo0XzYXS1NfmxSwsu330ssTuhgZ4Ofk3qsP/UReISi88nqVLBnjK6Opy9ckPu96SkpFSiUhnxCYmkpKRia2Ot8Nx/K6UWp3rzpvQU+u/Ih62NNUsWzuVJ+FN+njJdYVXYztaGjcvmk5ySyoDRk0iXI97bzMSIicN6c+1OGIdOBUsd61TOCg/XSmw9HChXqKCFsT5mRmUIuV+6GdU7Aq8R+vQ1k3u2QPMTm/18zInrj3GzN8dQV1PhuQdDnpCamUMHzwolfv7dlwmcCH1Fv/pOaH+C811NWYnuXg4ETfJlTPMqnL4fRaM5ASw6dpccBSKsGrnYMLq1O4euhbPzgri9387ckNHt6nDy5hMCQormozSqXoHG7hVYuieI6ISiG32XZl7Ymhkzf/NBia1D+/r5kJ2Tw5ZDxfsilJSUaOBRnbOXb5ArZ7KkhoY6GSUweRZWcbW3K6/w3H8rny+R4Tslpq6XB+NGjeDo8ZOsXqt4QcSKjvYsnT2NsMfhjJk6W66szw6+jansZMf8VZtlfnl6tW1C1Nt4Tl2SfSgQCATUdHbg6r0nCvUPlkZaZha/7zyFh3N5Wnu5lMo9C4mISSTsxVt83J0UnpufL2LTmbtUsDTErXzZEj0/Ny+fmf43KaOpSt96iq9BEuqqyoxsVpmgSb40q2LJkhP38Ft8ivBo+TO9h/q4UaeiJdN3XiTspbhm2t+nJi7lTJm+JVAsPHhKL19y8/OZs6WouUhVRZlxPVrxKPIN+8+I51TYWZvT2MONLQcDpWbyN6pTk6TkFG7clc9ZraWhQVq64iHM3wXEd74aBvbthW/zpixcspxz5xVvlN6gTm0m/jiEk2fOs3LDNpnjhUIhU0b2Jzo2nvW7Dkod26i2G1amxqzfJ90+XEhDd2fi36VyU0qJBUXYe/YmSakZjOvSpNSjSfwv3kMggJa1FNcAjlwP53FUIkN9qpV4XZsvPCHkWSy/tK2Grkbpakbm+los6+XFqj51eJWQiu/vJ1h39iH5cuQECIUCFvVthL62GsPWnCQlo2iynrKSkHn9WvAuPZM5O88UuWZd1oBBrety6GIo1x5GFLnWwqsaLg42LN5xVGJG9JDOrUhKSWVHwBmxa4XUq+WGqqoKJ4KL7373IXp6uiS9U8ypDQUCQltbG9Oy30ZFB3koNQFRWs3fv1OAQCBg7ozpVHB0YNT4iUREKm7D79u1Pa19GvPHqvVcvi49phwKIkKa1q3Fqq37pCbQKSkJ6d2uGTfuP+a2HCURGtRwRlVFmSPn5bcTS2Pjscu4OVrj5li6tmCRSIT/pft4VbLFzEBXobk5eXksOnydCpaG+FYvWchtZFwKi47dpa6TKT/UUNz3IC8+Va048XML6jiWZeaBW4zaepksOcwzRroaLOnXmJdxKfyyTdwUWcHKmAE+Ndl74R7XHxcNshjS1hszQz1mbAgootEKBALG92xNVGwiW46K37NaJXs8XCuydo9kAQIFZb/ruLtyMviKXPuQYRk94uVIEP2YJ0+fYV++3H8mxBVKUUD07NmztG71nb/R1NRg5dI/EAqEDBo+ipRUxRyMAoGA2ZPHYmtlydips+Q6NY0f0ous7ByWrJOej9HBxxs9HS3+3CG7q6COpgY+HlXxPxNCSgmLpBWSkJxGxJt4mtcu/Qi6G09e8zL2He28FL/3juAHRLx9x7g2NREKFd9A4lMz6bXqLEpCATPbu3/2TchEV4O1/b0Z37Iqh25G0uPPMySlybbL13I0Z6RvdQ5dCyforng49fA2HpjqazNj2+kimomGmio/dWnKvedRHP6oPalXVSfquFZg5d5TpEloADSoY0veJiQRcE7cDFWIT30PomJiCX0o+8BibGRIbFy8wofa5xERlC9nq9CcfzsyBcTgwYOlvgrx8/P7rAv9r2JlacHyRQt4HvmCMeMny9XX4UM0NTRYPOsX4hOSmDRrocwvRXlrC7q29WHn4ROERxSfAa2tqUHvdk0JvHyTB09l5130b9uIlPRMth37tNLcT14VhGs6WJbMxi+Ng5fDUFdVpqmbYiVdUjKyWXLkOh5O5jSsorhWk5iWRd81wbx5l8G6Ad7YGn+Zzo0CgYChjSuxtKcntyPj8VtySq4op8E+1XAw02fK9vNioa+aaqpM6NSAuxHR7LtQVBC0qeOCczkzFu44JdbDY0y3liQkp7LpyFmx59V1r4KjrQXr9h4r9vPbuE5NlJWUOHZGtjnWrKwJ6RmZJCtQsjstLZ2Yt7GUs/3vhLiCHAKib9++Ul/f+fx41KrB1InjCToXzLw/lig8v0pFJ8YM6cfxoGB2HQiQOX5E705oamgwe4V0B3nvts3Q1dZkwfo9stdgb03dahVYvf8U71JL3ich8++2lxpqpVtaQyQSEXjrCfVdyqOtoabQ3PWBoSSkZjLBr7bCJ/+4lEw6LTvNg6hElvfypHo5Y4Xmlwat3WzYOrQB8alZdPvzDPFS+kIAqCorMbu7N1EJqaw4Jh6o0Lp2RarZmfPH/vNkZP1fgAiFQsZ3bcar2ET2nS06r5qTLQ3cnVl/6IxY9V+BQEDvds148OwFN8OeSFxTGV0dPKu7cEIOM5O5aYEPISpa/nLur14XJOzZWFvJPedbQKaAqFmzptTXd74M3bt0pEeXTqzbuIW9/ocUnj+gRye8alZnxsJlMjOtDfX1GN6zA2cv3+B8SPG+Cz0dLYZ2ac3ZkDtcuHlP5hom9G7Lu9QMlu2Sz7ktCQujguzkN/Gl22vh0as4ohNTqVdFsQiVpLRM1gWG0sy1HC62ijkv41Iy6f5nEC/iU9k4qD5NKlsqNL80qWlnwsaB9YhJzqD/X8HvBXFxuNub0d7DibWn7vA0umgNMIFAwM8d6xOdmMqmwKJ+p7ou9rg5WrNi/1kxLWKQXxPi36Wy97S4KalVAw+0NNSlOqsb161F5Ks3PI2UnmRqaV6QCf0qSv6+1IV9qC3MzWSM/LaQ2wcRERHByJEjadGiBY0aNXr/+s6XY/LPY/GsXZNfps/k0hXJrR2LQygU8vuMSWioqzFm6ixycqVvAD3bt8Ta3JQZS9aSLSWpqFfbplibmTB9+WaZdfkrlbOkQ+PabDx8lvslLOBnaaKPmooy1z+KhvlUrj0uWE+dyrYKzdt2LoyUzGxGtXZXaF549DvaLT7J87hU1vb3xtOh9E1milLN1ogl3T24FRnPpD3XZJ7Ef/arjbqKMvP2i2/otSpYUd+lPGuOhhTRIgQCASPbNyAq/h1HPvJF1HS2o6qjDRuPnBV7tpaGOq0benA0OKTYkNeGngV/g7OXpQdDFAqIl1HSKwd8SNSbAmHyXyqzAQoIiIkTJ9KlSxeUlJTYvHkzbdu2pU2bNp9zbd/5CBUVFVYsWoitrQ1DfhzLo8eS1e3iMDEy5LdJY7n34DHL122ROqukQ68AACAASURBVFZNVYWpowbwNPIV63cVr7GoqaowY0Qvnr18w8qdsh3WE3u3xUBPm/FLt5WoC5y6qgo+tZw5dDGUTAUaxcgiIjoRdVVlLAzlj17Kzctne3AYXhUtqGBhKPe8B1GJ+C05RUZ2HruGN6KO09ez6fhUtWK0TxX2X4tg/blHUsca6WowsFlVTt2JkNhkaEjL2iSmZrD/YlHt0ruqA45WJqwPuChW5K9787o8fRXD1Xvin+22jb3IzMrmRDG9IszLGmNvaymz7Ia+nh4a6upERctffiTmbSxCoRDDv1uW/leQW0BkZWXh4eEBgIWFBSNGjODKFfnijr9Teujq6rB+1XK0NDXoO3g4bxSwowI0b1QPP99mrFi3RWZHuoae7jStW4ulG3ZKrdPkXcOF1g09WLnjEPeeREi9ZxkdLX4b0omw56+YtX6/QmsvpFNDd5LTMtl0vPQ+f9GJKZjq6yjkQ7j86DVRial085Y/6ulFXCo9V51FS00F/1FNcLWRX7B8KUY0daZpFUvmHLrNizjpjtzeDaugo6HKukDxz1INR0tcypmx6dQNMUHQu7knYRFvuPWkqCbpW8cNHS0Ndp0Sb1Xr7uyIlakxh88U38bWy70qIf9j78zjYt7eOP6ZNkv2fW8mJSI3yhqyZE1lzZayJFFIRdpkb0FKSIhQrkIhSfasWSJLhZD2fd+Xmef3R7eYO1vu77qLO+/Xixdzzvl+z0zNeb7nPM/zeV7FCt0hMxgMdO/a+bt8EDm5OejQvh0kJRuni/Wz0GgDISMjAw6HAzk5Ofj7++PGjRvIy8v7kXMTI4BuXbvA19sLpaVlWL5qDUpKvq+0pdOGtejZvSvWO2xHQaHws/wtlishJSWFTS5eQjOyncwWo13rVljv7C2ynvCk4b9gud54nLwSiV8jvj8JcHh/Fiaq98O+wJtIzBCtN9UYWjZvgrLK76vUdvdtMmSkJDF2QOMcl7kllTA8fAc1tRycMh2Lnu3/mUVnJCQY2DFHHZKSEvC6Lty31KKpDGYMU0TEy0QU/S5MlsFgYJ7mQCSk5+HNF+7zfh2NgWgqI43gSG4fV32p2utRr1DBx1k9SUMdj2PiBNaUVlPph8qqarz7+EXovDt1bI/s3MavX3l5BejQ/p9nzH80jTYQdnZ2qKiogIODA2JjY3Hp0iW4urr+yLmJEUK/vko46LEHnz4nYuUay4ZSiI2hZQtZeDk7IS+/EFZOzkIX/i4d28N+zTI8jYmF3/krAvu1bdUSezauxOfUDNjuOy7y/HrTEj2MGdQPTj5BiIyOa/TcgbqFYruxLmSkpbDOMxClfGLnv5f2LZujoLQCVSKcs9/yMD4Nw/t0Q7NGKMlW1bJhfOweMosq4LtCE4pd/j8p8B9Np9bNsGikAoKff0F6gfBqffM0+qG6lo2waN7KgdOH9oWMtCQuPeL+Gbdo1gSThyrjyqM3PJpeMzTVUV5ZjdvPeI3TxJGDUV1Ti/vR/A3XoP51GfCiigh1av+dBiI//z93vAR8h4EYOHAgZGVl0aJFC9jb2+PAgQNQVVX9kXMTI4JRI4fDbedWPH0eDXPLjd+lUDmgbx/YrV+Nuw+jcPzMeaF99bW1MEFjCNy8TyIu4bPAfhqD+8Nq6RyE3nmMQ2eER1pJSUrCa+My9OnVFabOR/Hk7feJ+XVu1wr71sxF3JdMmLj5f1fhen4MVeqJWjYHD+K+NKo/m8NBYnYh+vVo3FOl57W3iEnKg4fBCKixOvwfM/3rMBylCDaHcPmF8Kg35Z7t0b19C0TG8mb7t5JtihF9e+HuG97fm4lD+qGorAIxCdxRR+rKvdFKthnuveDVVlLtp4BmTWXw7PU7njYA6Na5A1rINkdConDlgTatW6GokZL1AJBfUIB27f57BdIabSDevHkDHR0d6OrqNvx5+1Z0aKOYH4ve9GnYttkOdyLvw8ZhS6OE+epZPHcGJo0bjd0HjuDFa17J5XoYDAZcbdeibetWWLN5N0rKBOcxrJqvgxkTNLDX7zwu3X4k9P6tZJvh5FYz9OjcHsu3e+P+y++rDDZBrS/2mM3G49jPWLHbH8Xlf7zc5ghlOTRvIo1bLxtnqLIKy1Fdy0GvjqKd2il5pTh65x1mD2Fhyi//njh6ZseWUJVrj9CXwhdbBoOB0f164tG7NL51FsaosPA5Ix9pedyZ/KMHKkBSQgL3XnE7pKUkJaHxS1/cj+H9fZCRloJqXwU8e8vfgc5gMKAg10NkqGvLFrIoLStv9PeloKAQbdu0aVTfn4nvOmJycnLC7du3cfv2bWzevBm2trY/cm5iGslC/TnYYLEGl65cheO2XY2WEGAwGHB13IhuXTph1QZHZGYLLlPZrk0reDhZITk9E5bb3AV+sRgMBpwtl2HYwL6wdvXB9Yf8I07qad+6JQK2r0GPTu2xbJs3Ltz6PsfzjNGqcDGdicdvP2Omnfcf9knISEli6hAlXHwUhzw+1dN+T+VvMfzNm4iuO336QQI4RLDW/nOVZ/8Kxih1wbv0QlTWCI84+4XVCaWVNUjP53VqD/qtrkZsErdTuJVsMyj16ozXn3gX8yHK8sjILUR2Pq+PTKUPCwlJaQLl5nt07YSMbOHHRzLS0iCiRknWV1RUoLSsDB06iI+YBCIrKwt19a+x3urq6pCVlf0hkxLz/ZiuWIZVK5bh7LkL2Om6t9FGonWrlvDZuxPlFRVYae2ACiG+jGGDBsBxrTFuPXyG3T6Cw2SbyMjgyDZLqPRhYc2OAwi/L7yEaqd2rRHksh7DBijC2tMf7gFXvksafN54dZx2XIaCknLMsPPGtSeCd0PCMNUehsqaGhyPEF3yVVqy7qtTK2KelTVsBD35jMkqPdC1zffXl/i7UezSGmwO4VOWcB2v+vKqidm8C7pCt7pjuIQ0XuOtzOyK2ETeCLn+vet2Wm8+8e5eFOW6o7qmFsnp/MNUO7Rri5y8fKHzxW/Rao35nuT85qvo2OHfcTT4Z/JdPojNmzfjyZMnePr0KbZs2YJhw4YhNjYWsbF/7Asp5s/Fap05lhgsxInTAdjhIlp3qZ4+vVnYt90Bb+M/wFqE03rxrGlYoDcZPgHB8BUiC95Sthn8XDZCpQ8L5tu9cPLidaFzaCXbDMc3r8LcCcPhFXgNixz3IyO3QOiYbxmuzMIl51Xo1akdVu09gzUeZ1FQ8n2SHgrdOkB3uDKOXXuGT+nCn0DbyDYFAGQXCb9HXFoBCsuroav279Tw6dCy7n0WlQsPBOjQslldPz6Cfy2bNUGr5k2QXci7u2B2aY/colKenBaFnnW5Icl8doS9utVlrKdl8d8ttm7ZAmUVlUJ1y+r9dVKNCFvNyKyLwOrc6b8j811Pow1EfHw8EhMTceDAAXh5eeHTp0948eIFXFxcxNFM/xAYDAYcNllj6eJF8PM/g9379jfaSGhpasB2nSnCb0Vir7ev0Htss1yJKWNHYKfXcZy/ektg31ayzXHadRMmjhyMrQdPY+fhM0K39DLSUnBbZ4A9Fovx5mMypq1zRvgj0TLl9fTs1A7BO01hNV8LEU/iMMV6P248j/8u1U77BePRrIk0bE9cE1q3uGUzGXRr2wIf0oUbsfi0OlnpAT3+nccT1bV1n4Goutii0keayUjzle9o16ruFOL3xrxtS1nISEkhi88RU4c2dbsVQaVIpaXrjv2EVZgrKS2DbPNmjcprSEmtq6feq+ffJ4XydyH6APU3Tp8Wnnkr5p8Bg8GAvY0Vqqur4ePrh6qqajhssm5UAtjyRfpITEqF94kAtGvTBssXzeXbT1JSEvs2W6G0bAdsnL1QU1OLBXqT+fZt1rQJDjquxY7D/vC9EI6Y+I/wsF+N7p0Eb9dnjx+GQUpMWOz1w2oXX2gNVYHTijno0Vl0xJC0lCTMZ43D+MFKsPQ6DxM3f4z5RRFbl+uA2UX0+I6tZeG4cDysj16FR8gDWM0eI7Bv/14dEP0pE0Qk8PPN+m2H0e1feLwEADkldfkGbZoLL15Ur+raRJr/gkvf/P0t9aKL5XxyHtq0ao6CEt4Q29Yt64xKsYBgifqfBPG5Xz35hYVo07pxWfOJX75AWkrqPyezAXzHDiI3Nxd2dnYwNjYGAHz8+BHnzolW8RTz18NgMLDV0bZhJ2HntK1RMuEMBgNbbdZh6gRN7Nx3EGdDBOc9NJGRxhEXO2gOHwz73Ydw8GSQwCd1SUkJOJkZwsN2Nd4lpmC6qYNI57V898644GYN26Uz8Oj1e0w024GDQRGNDmdVZnZDqKsZHI2m4eWHZEyx2o/Dl+41St5jlsYAzB2tgkOhUXj5MU1gvwkD5ZCaV4L4VMHHUbK/1ZQW5eT9pxKTlIeWTaUh30n4YpqUU+ejkOvIm9/B5nCQW1SGjq15EwPrdZpkBdQV52d4RT3rVFRWQUJCAk1kBBu11PRM9Gjkgh///gN6y7MgLf3nKgj/G2i0gdi0aRNGjRqF7Ow6xxCTycSpU6d+2MTE/H/U7yTWrDJB0IWLsNrk0Kg8CSkpKezb4QDNkcNgv2uvUCPRtEkT+DjbYcbksdh7NADWOzxQWSX4rFp3/AiEeu9Azy4dYbrFE2t3HkB2nuDKXtJSkjCZqYUbBx0xVk0Ze/xDMX7VNgTdeNyo6BNpKUks09bA9X3roKmqCNeACEy22o/IGOEaVgwGA46LJqBL2xawOHwFRWX8HfcTBjIhKcHA+UeCNYvaytZJh4tKNvsnQkR4+CELqnLtRRZBepeaB0kJBuT4hP2m5RaDQ4RufHSu6o+WWjZvytNWXVMLaT5HQPVHR4J2bcWlZWjRvJnAdiLCl5Q09OrRXfAb+qZvbNw79Ov759QH/7fRaANRUFCAadOmQUKiboiUlFTDv8X8M2EwGLAwX4WN69ci9Oo1rFpn1aiMaxlpaXi7bYPmyGGw27kHfmcvCOwrLSWFPfbrYGm8CCERd6G/2hYp6YI1bpjdO+O8pxPWGc7E9YfRmLjcBgGht4Q6xrt1bAtv2xXw374GHdq0go1XACaZ70ToveeNimPv0q41fDYYwHfTYgDAkl1+sPEORnGZ4Op2LZs1gZeZHjLyi2F55Arfus0dWjWD7lBFnH0Qz9c5CwAjFOocm3ffNV459J/C88RcJOaUQHewaAf7vbgUDJbvgqYyvKfWT9/X6S0NVuBdkN8lZaJ7xzaQbcpdg6OsogqFJeXo2oE396De99ChLf9s9NSMbHTvItihnJmdg7z8Aij3URD8hn7jc+IX5OblYYjaIJF9f0YavcI3b94cBQUFDVY5JiYGLVv+NZWvxPx/rDReim2Otrh77wEMjVehoFB0Pd6mTZvAe/c2TBo3Gtv2eGHfYcHyGRISEjBfoo+jLvZISsuE9lILBF25KbC/jLQU1i2ehatHdmGAIhOO+/2gZ7ZZZE0JjV+UcHGPNXzsVkBGShJr9/hhotlOBF5/hKpG7I7GD+6L8N3mWD1DE+fvvoDWeg+EPhQsWDhYoTscFk7A7ZhP2HOet14yABhrDURFdS2O3+J/nZ7tW6BPl9YIfZH0r6vbfur+B8g2kcI0VeHJfen5pXibnAvN/vz7PYj9grYtmqFPd26/ExEh5mMKlOV4ayx8+S2EtVcXXl9VfXRb5/b8E9eS0zPQo6tgA/Eqti4Le0C/PgL71FMvqz90iJrIvj8j33XEtGrVKiQnJ2P+/PmwsbGBg4PDD5mUl5cXRo8eDT09Pejp6SEyMvKH3Oe/xKL5+vDc44I3sXGYt3gZ0huhhd9ERgZezk6YqzsNXsdOwcJhB6qEHCFNGDUUV47vQ39FeWxy8cLyjduRkS04cU2+R1f4u22Ch+1qFJaUwdDGFUab3BD3UbC0A4PBwKThvyDMwxb7rZegqYw0Nh04A80VW+ATfFNkzesmMtLYsHASLjqvQtf2rbHWMxDrvc4J1HNaPGEQFo5ThXdYFE8JTQBQ7tkB09V7w+d6DJJz+OcKLNXsg1fJ+bgVmy50bv8k4tMLcCUmGYajFCEronpf0MN4MBiA7lDeJ/Liskpce/4B2kP78hxTvfmchpTsAoxX4z2+efi67thOrR9vAafX7z9DQoIBJRavQSopK8enpDT07yO48NPDp9GQbd4MKsqij42u37oNeRYTzF7fX0r2Z6DRBiI5ORnHjh3D2bNnMWrUKMjJyX13feTvYcmSJbh06RIuXboETU3NH3af/xLaUybh5NFDyM7JwZxFRngbJ1raQlpKCi6OG7DBfAVCI25hwUoLoTr6Pbt1RsD+7di8zhhRL95g0iIzHPa/INC5zGAwoDt+BG4ed4PdyoV4/eEzpq9ygPl2L8R/FizxICkpAZ0x6rjiYYOTW83Qu0dnuPhdhMYyR+w8Hoy0HOGJUiry3RG80xTr9Sfg8oNXmL7xAKLf8xomBoOBrYsnYnjfXrD3u46YT7yLvN2cEZCUYMDxzH2+R1FzhsqD2aEFXEJjUFr559Ww+FGUV9Viw5knaN1MBivH9xPat6qGjbMP4jFGuSd6duD1MVx4+BZVNbWYp8mbRR5yLwbSkpKYOmwAT9ud57FQ7NkFXTvw6h9Fx36AolwPtGjejKctJu4DiAiqyvx3B0SEe4+fYrjaIMiIcDrn5uXjybNoTJmo9d2lZH8WGm0gDh06hBYtWqCoqAiPHz+Gvr4+tmzZ8gOnJuZHMFRdDWdP+kJCQhLzFi9DeMQNkWMYDAZWLVkE793b8TExCboGK3DngWBJDAkJCSyZq4Nrp7wwfLAK3A6fwmQDc1y981DgMUsTGWkYz5mKyFN7Yb5ID/eev4H2SnuYbvHA6/eCBQIZDAbGDOqHgB1rcWnvBmiqKePE5bvQXLEFFntPIi5RsCaPpIQE1s4Zj7NbjcHmEPQ3H8WhkEieOUpJSsDLTBed2sjCeN8FJGVx5z50bdsCNjOH415cCo7dfMVzH2lJCWybo47EnBKYHr+Pqj9QKOmvgs3hYO3pR4hLK8TeRcPRRlZ4fe6AyFhkFZZjmRavAaioqoH3lSgM6dMDA5jcEUPZBcU4e+s5pmuooHUL7oX+fVI6ot4kQFeTt0pfTkERomLiMXboL3znc/vhUzSRkYGaCn/D9jr2HVLSMqClqSH0fQHAhYuXwWazoTd9qsi+PyuNNhD1CSWRkZGYP38+tLS0vks99HsJCAiAjo4ObG1tUVT059Yf/q/TV6kPLgb6Q7mvEswtN+Kgz7FGnY9PHjcaF08eRscO7bHcYhO27fESKs3Rq3sXHHV1wMl9W9G0aROYO7phlskG3HvyQuD9WrWQheWSObjvvw/rDGci6lU8Zpg7YbGNCx68eCt0ngMV5eC1YRkij27BEp2xuPn0NbTXucDQ6YBQtdghfZkI37MG2iNUsPvX61jnGYTy39WG6NBKFies5oLN4WDJ3nPI+V1W8OKx/TFlEAu7Q57iyQfeXcaYvl3hOn8oHnzIgsXpx/9II0FE2Br8AjffpsFp1mBM6C88yie/tAL7w6IxWrkHRvfjTSI7cf05corKsGEu7wnAgeC7qKllY+2c8TxtR0NuoVkTGSyaMoqn7dKtR2BzOJg9kbeNzWYjIjIKmsMGQZbP7gIArty4A2kpKUydIDi/pf5aZ89dwLAhalDo/X11yn8qqJGYmJiQo6MjTZgwgYqKiqiqqop0dHQaO5wHIyMj0tbW5vlz48YNysnJodraWmKz2eTu7k6bNm0Seb2ZM2f+4bn8V6msrCSLDbYkr6xKK9esp+KSkkaP2+LmSSw1TRo7YyE9iY4ROaa2tpaCrtwgjVnLiaWhSzNXWNPNB0+Jw+EIHVdcWk6Hz4bSUH0zYmkZ0HRTewq58YCqa2pE3rOopIwOnYsg9cWbiKljRvNs99HDmHcC78nhcOhQyF1i6dvTZEtP+pyew9Pn+YcU6me8l6bY+1JRaQX3/coqabzjr6Syzpdik3nHEhH53n1HcuvO0Dyvm1RYViXyPfxVVNeyyen8c5Jbd4Z2XHzRqDHrjt0gxVU+9D4tj6ctMTOf+hrvoRUeF3jaot8nkby+PTkcvcTT9jzuE7F0zWmHL++4svIKGqpvRvPWb+c7n/C7j4iloUtX7zzk215ZVUXqWnq00spe1FujiJu3SV5ZlcKuXRfZ99+OsLWz0QaivLycIiIiKDExkYiIsrKy6P79+//35ESRkpJC2traIvuJDcQfg8Ph0PGT/qSookZa2jPofcLHRo99/OwFjdGdTyw1TbJ22kXZubwLxe+pqq6mXy9F0KjZxsTS0KWphmvpYsRdqqmpFTqusqqazl69QxOXbSSWlgGNmL+GDp8NpaKSUpH3rKisouOX79BQI1ti6pjRAjtPiv2cIrB/ZMwHUl26nVSXbqfo90m87a8/k+JSN5qzw5/KKrkX+dS8Yhphc4rUrPzoU2YB3+uHPEskBcuzNNzpIkXGp4uc/48mObeEZnlcJ7l1Z2hbcLRIo01EdD0mkZgm3uQZ+oynjc3m0LxdAaRiuo8y8oq52iqqqmn8OncaucqNSsorudoqq6tp4urtNHKZA5X+ro2IyPNUMLG0DOj52/c8bRwOh2ausKax+iZUW8v/dykkLIJYapp07zHvnH+PwTIT0hg/hWoa8SDyb+dPMRB/JVlZWQ3/PnHiBFlYWIgcIzYQ/x9RT5/RkNHjSXnwcAq6ENKoRYKIqLyigtwOHKE+wyaQypip5H0igCoqeL/cv6e6poYuXL1FWgtXE0tDlzRmLadjZy9SSVmZ0HFsNptuR72khdY7iaVlQP2nLycnr5OUmJop8p6VVdV0/PIdGrRwI7F0zcnGK4Cy84v49v2SkUtj1+wlpYWbKTzqLU97aFQcyRu50gLnMzxG4mNGPqlZnSB1az+KT83le/1XSbk0bmcoya07Q+Z+DyirsFzk/P9s2GwO+T9IoH4bgqj/xiC6FP2lUePS80to0PoTNHVbEFXxMewHLj8ipqELBUa+4nqdw+GQxf4gYunb071XCTxtGzxOE1PHjG4/4/2833xIJKWpS8hs236+c7py8z6xNHQpICScb3ttbS1NmmtEE+cYEpvNFvr+Xr1+S/LKquR9xFdov5+Ff52BsLa2punTp9P06dNp5cqVXAZDEGID8f+TlZ1Ni5aakLyyKllssG30kRMR0acvybTcYhOx1DRp+JTZdPrcRaqqrhY5js1m0437UTTPzJZYGro0cPICcj7oRxnZ/BfWb4lN+EJWroepzxQjkp+4mEw2u9OTV/EijVthSRltP3aeFGeuJZV5VnTm2gO+Y3KLSmmmnTex9O3pWOgDnvaQh29J3siV5u/iNRIJ6fk0bONJUlnnSw/jU/nOo6K6lvaFvyZFy7OkZB1ITuefU0qe6B3R/0stm01hL5NoonMYya07Q4sO3qLU/Mbdt7K6hmY6B5PymqP0MYN3h/TkXTLJG7nSmoOXeD7T42EPiTnXjjzP3eIZd/zyHWLqmNFe/1CetpKychpraEUjF6yl/KJinvbi0jIapmtE05daCNyJng8NJ5aaJoXduCPyPRqvWkuDR2h+1+//v5l/nYH4I4gNxJ9DbW0teXkfIYUBg2nMxGn0+Ino7fi3RD1/SXOWmRFLTZM0tPXp5NkLVFbeuKfjmNj3ZObgSr1HzyDFMTPJcps7xX34LHJcVm4B7TkeRINnmRJLy4BmmG+mq/eeinxS/JyWRQvsPImpY0aGmw9Qek4+T5+Kqmoy3RNAzLl25BpwjWfRqzcSc3f4U/HvjkVScotp0pazpGDqQydvvxFouBKzi8k64DH1Xv8rya//lcz9HtCduDSqrhU+/+8lp7iCjt99R+N21O1cxu0MpUvRXxq9W6ypZdOKg+HENPGmq9GfeNpTsgtJfY0Xjd3gw/NZ3HgWR/L69mTseorn5xJ67znJ65mT8fbDPG2VVVVksNGZFCYZ0pPX73juyeFwaLW9C/UePYNi4j7wnXdhUTENmzyLdAxWiPydiHr6jOSVVemgzzGh/X4mxAZCzHfz/MVLGjdFh+SVVWm7826qqKgQPeg3OBwORT56QnOW1hmKQeN1aO+hY43yURARJadl0tZ9R6i/lj6xNHRpscVmuvf0pciFrLyikk5fvkFjDS2JpWVAWks30oXr96lGwJk0Ud0O5lRYJPWbs55U5ltT2ANeBy2bzSb7IxeJOdeOHI9d4l3gouJIYakb6W05SQUl3MawuLyKlh+4SkwTb9p48g6VVwneVaXll9KOiy9ogM05klt3hlTtztOms08o4nUK5ZY0/vP/lrySSroU/YWWH4mk3ut/Jbl1Z0hnzzUKfZFEtSIWy29hszm0/vgtYpp4k9/tNzztRaUVNHHTURpouo8S0rgd9C8+JFPfRU6ku+kglf7u+PHGk9ekMGMN6W/aR+W/24VV19TQSqd9xNIyoPMR9/jO68DJIGJp6JJPAK9Tux7rLc6kMHQcvYnn9V18S01NDU3Rm0OjtaZSeSMfan4GhK2dDKJ/Wf6/AGbNmoXg4OC/exo/FeXlFXBz98TpXwPBlOuFrQ62GDVy+HddI/rVWxz1D8SNuw8gLSUF7YnjYDR/FgYq9xU5tqi4FGcuXYPf+SvIyStA/z7yMF00G1PGjhCq489mcxB+/ykO/XoZ7z6nQK5bJ1gv08e0MUMFJjx9Sc+BhbsfXn1Iwuq5k2C1aDqX1hgRwdn/Go6GPoD+ODU4r5zB1X7z5UeYHbiI3t3a4/SGeWjf6qu8N4dD2Bf6DAeuvoBC17bwXD4Byj0Fy51X1rBx710GrrxMxs23aSj/rY6CfKeW6NetDRQ6t0b3drLo3rY5mkpLQoLBgIQEAyUVNUgvLEdGQTm+5JYgJikPiTklAIDOrZthhhoTs4YwodT1+2or17I5sA+4h6CH72ClNwTm07hlJyqra7B073lEJ6Ti5AZ9jOj3VbvpXXImFm71RcvmTXFhx0p0+EbR9XrUa6zZfRx95brBf8catPwmNLWisgoWPXwEqwAAIABJREFUzodw49ELbDEzhOGMiTzzCg6/jQ279mP6hNHwcLLk+7MNu3EHa2y3YvVSA1ibGQt9n0dPnILLnn045LkXk7V4w29/VoStnWIDIUYkD6OewHHrTiQlp0B76iTYbbBCl87fV10rMTkVpwKDcT40HGXlFRg8sD8M5s7A1AmaQmWZAaCqugYXr9/FkYBgJKakQ75Xd5gazIbeJE1ISwkuaUJEuPX4Jfb6ncf7xBQM6qcAW5MFUB/AP8u2qqYGmw8HIejGY0wcNhD7LI0g26wJ1/U8zt3C/vN3MHfsYLiYzuQyEpFvPmOlZwi6d2iF0xvm8aiXPohPhdWJ2ygsq4T1jGFYPmGgSJXUyho23qTkIzoxB88+5+BjVjGS80oh6lvbuXUzDOzZDoOZHTC0d0eoyrWH5B8Q1ywur4L50Ru4H5eKNdpqWK+jzrUQ19SyYbo/BHdef4LHSh3ojlBuaPuYmo35W45BSlICgVtXQO6behwhd55ig6c/VBR64YTTKrRp+bV8cUFxCVY4uuNl/Cc4mS2GoR6vcQi8cgN2rgcxYrAKjrk5oGkT3oS+uPcJmLPMHP37KiLg8D6hmdMpqWmYojcHo0YMw2Gvff+pzGmha+dfsYX5KxAfMf1YKisraf8hH+qrOpRU1EfSgcNH/9A2vLiklE78ep7GzVxELDVNUtfSI7cDRygtQ3QgQm1tLYXdfkDaS9Y1RD6dvRwhMieitpZNgeF3aZi+ObG0DMjSxVtgeCyHw6Hjl++QvJ45TbdwocIS3qgq98AbxJxrR5t9L/Mce0XFJ5PKSncauf4QpWQX8ozNKyknk0N15/hzXEMEhsIKo6qmlpJySujRh0y6G5dOt2PT6ObbVHr0IZOSckqoUkTIcGN5n5ZH4x1/JcVVPnT2fhxPe0VVNa3wuEBMQxcKuP2Sqy0hJYuGmjiTuvEu+piWzdV2KiySWLrmtNDekyec9VNyOo1fYk1KU5dS+L2nPPfkcDjkffo8sTR0aYnlFqqo5B8xl5qeQSOnzaWR0+aIPNqsra2lRUtNSEV9JKWlZwjt+zMi9kGI+dNISk6hlWvWk7yyKo0YO5ECzwf/oVhxNptN9x4/IxNLO+o9ZBz1HjKOTK0d6fGzFyJ9DRwOh24/ekYzjK2IpaFLY/VNKDj8tsD493rKyitoz/EgUphkSCMXrKX70bxn6fXcevqGFGeupRlWu6mkjPv8n8Ph0I6TV4k5145vdNPbL5k00HQfjbE6zJMHUD/+3MN4GmjhS0pmR+jo9Zjv8gf8aDgcDp25F0v9zI+SurUfRb1P4+lTVFZJ+jsDiGXkQidvRHO1xSamk9rynaRuvIveJ38NP+ZwOLT79GVi6pjRsm3eVPE7n0Pk01c0UM+E1GavoqdveB3SFZWVtH6bO7E0dMnc0ZUqBfhzUtLSabTOPPplrDa9jefvuP6WvZ4HSF5ZlQLPB4vs+zMiNhBi/nSeRb+k2QsNSV5ZlSZM06NzwZeouhFhrfxITc8gl/2HadB4HWKpadLU+cso+IronQGHw6Gb95807CgmG5hTZFS00DFERDHvPpHW0rqEu+3e/lQrIFoo4nEM9dZbQ/Ns91FVNfdc2Gw2me4JIJa+Pd2KjucZ+/JjGg0wcaexG3wou5D/biWzoLTBga236wLFpYgO7f3RfMosoEXul4lp4k0L9l6iLD5zz8grpmkOx0lhqRtdehTL1Rb9PokGLtlGI0xd6dM3zuqq6hqy9jhFTB0zsvEK4Aoc4HA4dCToCvWetJimmthSSgb3joOIKCk1g3SXWxJLQ5e8TgQKfIiIT/hEI6fNIdVx0+l1HK+R+T3Xb9VlTNs4OIns+7MiNhBifggcDoeu3bhF2jP1SV5ZlUZrTaXTZwKprOyPRYBUVFRS4MUwmjzXiFhqmjRy2lzyDQgSGSbLZrMp7NYDGqtvQiwNXVq+YTt9SuKfe9Bwr8oqcvI6SSwtAzLd4kGVVfxlL0LuPCWmjhk5egfytJVXVtFU6/2kbryL8ot5j6KiE1Kpr/EemrP9NN+EMqK6z/Dikw+kZuVHiqt8yPPKc6oWsRP6ERSVVZJbSBT1We1DKmt9yf/uW2KzeRfhx/FJpL7Gi/qbuNPd19yhrlcfvyGlhZtp7Jq9lJL1NWS4qKSMFtrXhRPvC7jCtbiXlJXTqq2exNIyoFVbPam0nDda69L1SFKZOI8GTl5A1+9FCXwP1+/cpwGjp9CwybMo7n2CwH71RL+MIeXBw0lv7sLvitL72RAbCDE/FA6HQ7fuRNKs+YtJXlmVfhk2ina47KakZMFyFiKvd/8RzVuxllhqmqQ2QZe8TwSINBSVVdXkE3CBVCbOI6Wxs2jfsTMik/V8L4QTS8uAFlrvpDI+ixMR0Q7fC8TUMaMr93l3J7GJ6aQw34HWefIaEKK6EFimoQvZn7gmdB55JeW09ugNYpp407TtQfTys+jM8D+D4vIqOng1mgZa+BLTxJssfG9SdiGvsWOzOeR9JYp6L3Gl8TZH6H3K16d8DodDRy7fJ5a+Pc2yP0y5RV93HSmZuTTJbAcpzlxL529xL+6fUtJp0nIb6j1pMR0NCuPZFZSWldPGXfuJpaFLc0w3UqoAP1VNTQ15+Jwglpom6S1eSZnZ/HWwvuV9wkcaPEKTxk3RoZxGhl//rIgNhJi/BA6HQ8+iX9Iaq43UZ6A69e4/iJauNKPwiBtUJST+XxjPY96Qkbl1g0PbNyBI4NN+Pdm5+bRuyx5iaejS9KUW9O6TcAmJkBsPqPekxbTE1o3vsVZ1TS3prHehEUvt+Z57u52JIOZcO77ifkRE2wNuEtPQhT6miT5Cinj5mYZvPEXyKw+Ta3AUVVb/mN1EYlYh7Tj3qMEwLPO6KlBgMCWnkBa5/kpMQxda7RXCpaFUUVVN672CiDnXjlbtPUMV33w+j1+/p0ELN5LKfGt6GMN93BN65zEN0DGmwbNM6UE0r7TG05hY0pxrQvKj9GjvEX+BGdLJqekNiZlWm3c1Subl5avXNHiEJg0bo/WHH2J+JsQGQsxfTkZmFu3zOkQjx08meWVVUh81jna67f0uMcBviX71lhatWk8sNU0aozufQiNuiXRmR0Q+JnXtxaQ0dhadDr4qtP+ZK7eIpWVAWw+c4tv+IOYdMXXM6EToHZ62rPwi6j3PgZxP89cByikqpT7Ld5ODX4TQ+dZTXF5FG0/eIaaJN03acpaeJfw5gn7F5VV06ckHWuwRSkwTb1Iw9aHVPhH0KpH/k3ktm01+15+T8oq91N/Enc7c4U5WTMzIpanW+xvkM+oTCDkcDp0Ou0cKM9bQhFXb6NM3OlnVNTW0wzuAWFoGNGftVkrL4jaaVdXV5Op9kuRH6dGYOSvoaQy3j6MeDodD50PDSWXMVFIZM41CrjZOdTXy/kMaoDaCxk6eTl+Skhs15mdHbCDE/G3U1tbSnXv3yXStJfUZqE7yyqo0a/5i+jXowh/Surn3+ClNnb+MWGqaNGeZGcUn8Eo+fEtOfgEtsdpKLA1d2rLPR2ik09aDp4mlZUBRMbwhnRwOh+bYuNNYky18Dc2SXX40fp27wGuv9gqhERYHhc7199x5k0TDNp4kpok3mR+5Tql8IqKEweFw6GNGAQVExtKS/Veoz2ofYpp40/CNp8jzynO+Duh6XnxMI12nk8Q0dCHD3YGUklPIdd1zd6Kp/+ItpLp0O9158XV3UFZRSZbuJ4mpY0ZLtx6iotKvx4JpWbk0Z+1WYmkZ0JYDJ3kc/+8+faFpRnUBBzbO+wUKN6ZlZNHStTbEUtMkfeM1lNqI0FQ2m03eR3xJYcBg0p6pT1nZvI7w/ypiAyHmH0FObh4dO3GKJuvMInllVVIePJwsbezp4eMnIjVyvqW2tpbOhoSS2gRdUhw6npw9D1O5ECdjbW0t7djvSywNXTLeuF2gr6G8opJGG1jQZGMbvkdNp8IiialjRh9TeBekbSeukLIBf+NBRGR15AqNXH+oke/wmzlVVdO+y09JyewI9TU7SjvOPaJrLz5TSk5Rw704HA6VV1XTu9Q8Cnv+kbzCnpOZz3UaYl1nXJgm3jTazp+2Bz2kpwnpQkNqk7MLyPzgRWIautCQNV506VEs13vKKSyhVXvPEHOuHc1zOkppOV/zOBKSM2iS2Q5i6ZqTx5kwruiwO09iaNBMU1LRNabQO4+57slms+lIQDApjZ1F6tqLBTqi2Ww2nT53kVTGTCVljcl0/Mw5kaHNRERFRcVkYraO5JVVaa2VDZWWClcM/q8hltoQ84+CiPDqzVucD7mE0KsRKC0tBVOuFwwXzccsPR20bNFC9EUAFBQWwWW/D85dvope3bvBbcsmDB3EW/qynlMXwrDN8xjUB/bDyb1b0KQJbwb3jUfRWOnkge1rl2CRzgSutrScfIxavhmbV8zBUp2xXG2HQiKx+9friDvthGZ8rrvYLRCZBSW44Sxc7kEQqXklcLkQhasvPnFlUbdqLoPSihpwfvc17tauBYYodMFQxW4Y1qcb5Du3FpodnF1YisNhUfC/9RKSkhIwmToUJtOGQbZp3XshIpy/+wK7ToejvLIaFvoTYKI7GpISEiAiBIQ/wM7jwZBt1gTulkYYM6iu5Gd1TS3c/c7hSNBV9JXviYOOa8Hq8bX8aFpmNqx3eOJJzFtMGj0MOzauRoe2vFIgHz4lwn7XXkS/eguNoWrYZW+Nnt27ivzcHj95BhsHJ2Rl58B2w3oYLVrwn8qSbgziTGox/1gqKioo5PIVmr2gLgJKRX0kbdnpQimpvMlZgoh6/pLG6i2g3kPGkbu3r9DEvUvXI4mloUsrbXfxffrkcDikZ7aZxhpa8eRHcDgcUplvTQ7eZ3nGCdtBfEzLJaahC+0L/v8LbJVVVtOLz5nkf/ct7Q55Qk6/3ie3kCjyDn9BIVHv6U1SNpVWND4gILughLYH3CSl5Xuo9xJXsvG9ypPc9yElixZsOUbMuXY02+EwV/JbVl4hLd16qEERNzP3647ic0oG6a52JJaWAdl7HKfybxzIHA6HLly9RSqT5pPKxHkUdOUG38+usrKS9h46Rn2GTaBB43Xo3GXhvqR6KioqaNsuN5JXVqXxU3XpRcwrkWP+q4iPmMT8K3j1+i1Z2tiT0kB1UlRRI8tN9pTwUbiPoZ6S0jKy3uJc55tYaiZUuuNE0GViaeiSk7sP3/awu1HE0jKgiAe8UufTLVzIyInXl2DseoomWXrwvZ75wYvU13gP5Rb9c442sgpKaJv/TeprXGcYrI9coS+Z3HLnBSVltOV4KCnMd6BflmyngOtPuBzRgdcf0S8LNpDSbAvyC73LdeTlf/km9Z++nAbNNKVr97k/x9z8QjK1cyaWhi7pr95EyWn8Q3ofP3tB42caEEtNkywdd1JufuNkSaKePiMt7Rkkr6xKTjuc/3Bezn8FsYEQ868iPSOTdrjspv5qw6l3/0FkaWNPySnCE9/quRR+kwaMnkJqE3Tp6UvBT431PomzobzRL7W1bBoxfw0tsXXjaVuy5SDpWnK/zuFwaMiKXWSxP4h3Po9iiWnoQh7fuXuoZbOpuLyKMgtK/9TEucz8Ytpy+kbDjsHqyBVK/J1hqKqpoRNXH5Hq0u0kr29Pdj4hlPdNbsOn1Eyab+dBTB0zmmvjTh9Tvi7waVm5ZGjjSiwtAzLY6Ezp2dw5Bt9Glh32v8B3F1dQWEQbt7o2RKzde8yrycSPvPx82mC3meSVVUlzkjY9ePRY9CAxQtdOwVKYYsT8TXTt0hn2NtYwXbEMvn6n4ef/K8LCI7Bovj7MV61A2zaC5ap1p0zAgL6KWGFpj8WrrODssAEztSfx9LNZZYQPiUlw2nsYqsp9oCT/VaJaUlICuuNH4viFaygqKUPrb5RGWzRviqTMXK5rJWflI6ewFEP6ynG9nppTBIdT1zFYoRvMdEcKnHNucQWiPqQhPjUPscm5iP6UidLKmob29i2bYupgeUxT640hCl0hJfn9qqyV1TXYf/ERfK8/A4dDmKXRH6unj4Bc57YNfYgI15/FwcU/Al8y86Ch0hv2htPQT67OZ1BeWYWD5yJwLOQ2mjaRhrP5QuhrDYeEhAQ4HA4Cw+/C+chZcDgcbFtrhEXTJzSc9xcWl2C75zGERNyFsiILpz23oW9vJtcciQhXrt/Gtr0HUFhUhJVGC7B2hRGaNW0q9L1xOBxcuHgZrns9UVJailUrlsFspTGaNWsmdJwY0YgNhJh/LO3btcNGy3UwXLQAngcP49SZs7gYGgYbq3WYM1OPS2r7W+SZvRDsdwirbZxg5bQLSalpWGeyhMs5KSUlCXdHS0xZvAYbdnrigo8bl3T41NFDcCQoDLeiXmLWxFENr7do1hSl5ZVc93vxIQUAMKhPr4bXiAgWh0MBAPtW6vAs6kSEpwkZ8I+MRcTLRNSwOZCSkEDvrm2gM0QBnVvLQrapNJpISyLqQzrOP/oA/8g4MDu1wq+WuujStnGOfACIik+G7Ylr+JJVgJkj+8Ni5ij06sRtZOO+pGOb31U8iUuEYo9OOGFrCE3VPmAwGCAihD18iV3Hg5GRW4gZY4fAbulMdGxbJ2f+PjEFDh4nEB2XgBGq/bBr/XLIdevccO3wu4/g5O6DgqJimBvpw3yJPo/0dlpGJhyc9yHy0ROoKCvhpJcblJUURb632Ph3cNrhjJcxr6E2SBXbneyhpKjQ6M9GjAj+mk3Mj0d8xPTz8+79B5q3eCnJK6vS3EVL6N0H4Xo7VdXVZO20i1hqmrTFzZOvczPs9gNiaejSAT9uqQw2m00jF6ylFY7cuQ27joeQ0mwLrtc2+16m/ou3cIWP1ktsBEbyHnPFpeTStO1BxDTxpoEWvrQt8AG9SswSmjVdVllNl58mUP81x2jqtqBGO6I9Lz4glpELaVofpoexvBnl+cVltOlwMLH07WnQsh10OiKKS0gv9nMKzbPdR0wdM5q21pmexX5NdCwrryDXY2dJcbIRDZ5lSucj7nF9xpk5uQ2+Bp2l6yn2A68/qaamhnwDgkhZYzL1HzWZfAOCGh266rTDmRQGDCb1UePofAhvpT8xjUPsgxDz08DhcOhc8CVSGzmWlAaqk/cRX6ELCofDoZ37DhJLTZMcXfbxXUTMHV2pj+Ysiv+YyPX61oOnSWnqUq7aEd7nrxNTx4xLAnye01Ga7XCY655T7H1pku0xnpyD84/eUV+zozTE+iSdvR8ntAQpP+6+TSb5lYfJ6sQtkX0Dbr8kpqELWRwOpfJK3vtEPI0lNeOd1HueA233C+NKaissKSNH70CS1zOnQQs3UkD4/YaoLg6HQ1fuRtHIBWuJpWVA1m4+lFf4NfKppqaWjgdertPEGjebvE+f5yuVEfM2nqYvNCaWmiYtXWvT6IS3c8EXSX3UOFIYMJictjtTUdH3JRCK4UZsIMT8dOTl55P5+g0kr6xK8xYvFRoWy+FwyNnzMLHUNMlu5x4eI5FXUETq0xeTztL1XAlyMfEfiaVlQIHhdxteC733nJg6ZvTuy9f7jTHfQ2s9voa+Po5PIqahC529G9PwWmV1LW06dZeYJt40f88lvoJ4jWXf5afENPGmC48F11iOik8mhaVutGRPEI+RKq+sok2Hg4k5146mbfCi2MSvUh4cDofO33xMagY2JK9nTpsPB3IVTfrwJZUWbXAmlpYBaa+0p+dvuecQ/Sa+IRvaaL0TJabwyoQUlZSQk6sHyauPpWGTZ1HYjTuNCl19GxffIDE/Z6ERxTZCzluMaMQGQsxPCYfDoeBLoTRwiAYNHKJBl8P4ayHV93U7cIRYappku2M3z4J07e6jhloD344ZZ2RNc9Zta3jt1YcvPMqu/RdvoW0nrjT83/Z4OKmsdOcSrnM+/5iYJt7kGhxFNQLqTzSWmlo2zXW7SCrrfCmbT+gsh8MhrU1HaewGHyoq4xavyy0qpRl23sTStycX/2tU9Y1BTM3Oo8WbvYipY0azNuyh2E9fhexKysrJ+cgZUpxsRL/MMKGTF69z5Ylk5+aT9Q4PYmno0ogZS+nqnYc8nzGHw6ELoddoyKQZJK8+lpxcPaioEXIrBQWF5Lh1Z8Nx0rlg8XHSn4k4iknMTwmDwcBM3ekYojYYljb2sNhgixcxr2BrbQkZGWmevtarjSHBYODgcX90aNcWlquWN7RP1hyBaeM0cPDUOehMHA257l3BYDCwSGc8dh4+g7iPSVBWkENfZnc0lZHGi3eJ0B41GAAgJSmJWg6n4VovP6VjkEJdPwD4lFmA47deQ1+jLzbOHMY1LyLCvXeZeJ6Yg6LyahSWV6O6lg11+Y4Yr9wN8p2461rX3U8Czos1MXHLWZy68xZWekO52h/FJeFjeh72rNBGq+ZfazUXlpZj0VZffMnMg7fVAkwe2r+hLfTec9gfOgs2h4NtpvpYNGUUJH7Lkr724Dm2HzqNzNwC6E/RxIbl+mjfpm5eNbW1OHUhDJ7Hz6KqqhorF82CmdFctGjenGtOce8T4OTmiehXb6E6oB98PVyg0k9J6M+XzWYj6MJF7PU8gKLiYixeOA8WZqvQqlVLoePE/HmIDYSYfz09undDwIkjcHP3xPFTAXjzNhb797iiWzduKQYGgwHLVcuRV1CIA76n0aplCxgbzGtod1i7HPeevICd60Gc9tgGCQkJzJk0Bu5+53H0/FXs27QKMtJSGNC7J57Hf24Y16yJNMorqwEAbA4HH1JzMf6X3g3tB8Nfoqm0FDbM4DYO2cUVsAt8hpuxaWAwgNbNZNCmeZ20xbXXqdhx8SWUu7fB2skDMGVgT66xvbu0gWb/ngh+/AGWukO4IrSuPnuPFk1lMH1o34bXOBwO1ngEIjEjF8dtjaChUjc/NpsDZ7+L8L10G4P7suBhaYSeXToAAHLyC+G4/ySuP3yOfvK9cNBxLQYpf40QinrxBk7uPkj4kgLN4WpwXLsc8r26c80zr6AQ7od8cfbiFbRt3QoujhsxR2eKwAi0ep5Fv8B2592IjX+HoeqD4WRng75KfYSOEfPnIzYQYn4KpKWlYW9jjUGqv8DWcSv09BfC29Md6mqDuPoxGAxss7FAaWkZdnl4Q1pKGkbzZwEAunRsDzvzZbBzO4hTF8KwZK4OWreUhaHeJBwJCsPqBbpQlOuOsWr9scc/FOk5BejWsS3ku3VAQmp2wz04RGgiU/fVIiI8jE/FOJVe6NDqa1z+tdcpsA18ivJqNhxmDILhKEXISEk2tKfml+H6m1QERn2C6fEHsJqmAvOJ/bkMwbTBvXH37V3EpeShf68ODa9HJ6RhsGL3hjkAwKmIJ3jw+iN2rtBrMA4VVdVYu/sEbj59A6PpmnBYPgtSknVzuBr5BA6efiivrIKN8TwsnzO1oS07Nx+7Dp7A5Rv30KNrJxxxscMEjaFcc6uuqcGpwBB4HTuJiopKGM2fhXUrlqC1iKf/tPR0uLnvx5XwCHTp0hkeu50xfepksX7S38Vfdc71oxH7IMTU8znxC42fqkt9fxlCQRdC+PaprqmhlVb2xFLTpNCIrxFBHA6Hlm3YRn3HzaG4D5+JiCivsJj6aS8jK9e6SKVPqZnE1DEjn+AbRES03S+MlBZupsrqGuJwOMQ0dKE95yOJiCg5p4iYJt7kf/drUZz4tAJiWpwh7d3hlJDxVUabHxXVtWRx+hHJrTtDOy++4GrLKSon1kpv8gp73vBaWWUVyRu5kvuFew2vpWTnk9LCzbR0l1+DX6CmtpYMNx8glq45+YV+dcJXVdeQvcdxYmkZ0ExzJ/qY9NUZz2azKSAknFQmzSelcbPJ/VgAVVRy+zjqqwHWS2QYmVtTwmfhBZuIiEpKS2m3hxf1GzSM+g0aRu77D4olMv4ihK2d35+SKUbMPxwWUw4XzpzCEPXB2OS4FZu37UJNTQ1XH2kpKXjudIS6qgqsnZzx6NkLAHU7DJdNa9CmVQuYObqhorIK7Vq3hKGeFkJuPsTbhC+Q794Z6sq9cSrsHmrZbIwa2BtVNbW49yoBDAYDzM5t8S4lBwDA5tSprDZr8tUnEvkuA0TAcRNNKHRpLfS9NJWWhPui4Vg0UgFH7rzD3fj0hrYOrZqhZ/tW+JBe0PBaQloeOERQ7vU1UW3/udsgADtW6DU8iW8/dgH3XsZjl9kCGE3XBFB3pLRogzPOXLkNE/1pCNzngN69ugEAktIyYLDOEQ57vKHSVwHXTnlh/fKFaNrkq4/j05ckLF27EcYWtgCAYx7O8PPaDQUWd4b5t9TU1CDgbBDGT9WF9xFfTJk4ATevhGD9mtVo3lycCf13IzYQYn5K2rRpjeOHD8Bk2RIEBJ7DitXrUFxcwtWnSZMmOOq+C8ye3WFq7YBXse8AAB3btYH75vX4kpoOl0N+AIDVC3TRtlULbPf2B4fDwYoZE5CWnY+wBy+goaKAti2b49L9VwCAIX16IDohDWwOB21k6xbQgtKKhvs++5wDVseW6NSqcQsgg8GA44xBUOraGht+fYKqWnZDG7NTa3zJKmr4f0JanQyIYo+6I6f03EIE34vBoolD0a1DXfb0rxEPcSrsHoxnjMf8SXUSICkZ2Zi5ZgtiP36Bp91qbFqxoCGz/FzYTUwzWoe3Hz7D2cYM/h7bwOzx1b9TWlYOZ09vTJ23DC/fxMF+vRnCA49j/KgRAt8TESHixi1MnTEXm7c7Q54ph5Cz/nB33cnjOxLz9/G3GYjw8HBoa2ujb9++ePPmDVebj48PJk6ciMmTJ+P+/ft/0wzF/NuRkpKCjdU6uGx3wuOnzzB7oSESk5K4+rRu1RInvNzQtk1rLF5thddxdUZixOCBWDZPF6eDr+LS9Ui0aiGLDcv18ezNe5y8eANaQwegH6s73E5eRk1tLWZrDkZ41Ft8Ss/BeNXeKCitwK2YT2jdvAm6tpXF4/dfn/wZAL73SL2pjBTMJvZHTnElPmUVf52/bBOUVlU3/D8uOQvaaH9DAAAgAElEQVTNZKTRq2OdMQi8/RwcIiydVrdYJyRnYOvR8xg9qC82Gc0AAGTk5MNgowvKKioR6O4AnXF1fSsqq7BhlydsnL0wqL8SrvsfwDydSQ27ECJC+K1ITJxjiKOnAzFTezJuBftj+aK5PFIa3/L0eTTmGizBagtrSEpI4MgBD/x60hcDVfoLHCPm7+FvMxB9+vSBl5cXhgwZwvX6x48fERYWhrCwMBw7dgxbt24Fm80WcBUxYkQzd9YMnDrmjYKCQsyeb4hHUU+52rt27oRffTzRpnVLLLewRVJqGoA6Qb8hvyjD1vUA4hI+Q3+KJsYPV4XrsUAkJKVh60p9pOcWwCvwGlbqjUZTGWl4BN2C1iBFdGvXEicinoPBYEBbXQH3YlNQUFqn4TSwVzt8zi5BUXk1z1yFodilLrT04zcGoqaWDRnJr87t158z0V+uE6QkJVDLZiPodjTG/KKAnp3agYiw6cAZyDZtgr0WhpCUlEBeYTEW27igoLgEfs4bodKHBaDuSGnWyg0IDr+DNUvn4aT7FnTp2L7hPpnZOTC1doCZjRPat22DCycOwnXzRnRo91X87/e8e/8By0zNscDIGBkZmdi11RFhIUGYME5T7IT+h/K3GYjevXtDXl6e5/Vbt25BW1sbMjIy6NmzJ+Tk5PD69eu/YYZifiaGDVFHSKA/OnXqiGUrzXAxNIyrvVuXTjju6Qo2m42FK9cjKTUN0lJSOLBtI9q2boklVluRnpULZ0tjtGrRHGt2HEB/VnfMnTAcR0JuIjkjG8una+DKozeIjPmA5VOGIOpdMsKfvcecEUqo5XDgeeU5AGBUnzp1VPfw7/u9bvJblFNlzdcHpoyCMrRpUXeMVVpRhTdfMjFIoS7U9Hb0e2TmF2OhVl2exOV7z/HiXSJsjPTQsW0rVFZVY4WjO9KycnFshzV+Uar7PsZ//IK5qzYhOzcffnudsH75Qkj+ZoSICMFXIjBZfwnuRz2H7TpTXDx1GIOEPP2np2fA2tYR02fPR8yrN7CxssCtq5cwb84sSEmJAyn/yfzjfBBZWVno0uVrScLOnTsjKyvrb5yRmJ+Fnj26I+j0CagNHgSrTQ44dMQX9E2pTgWWHPwP7UVFZSUWrrTAl5RUdGzfFn57t6CyqhqmdrvQollT7Nu0Cp9SMrDZ6yQcjGehW4d2WO9+CkZThkGpV2fY+oRAZ1g/DJDrDMeT19GhZVMYaPbH6buxePYxA4OYHbBcUwkn7yfg7ONPjZ7/m5R8AED/7nVP6VU1tYhLycUgVp1D+kHsF1TXshtyME5FRKFr+9YYr6aEyqpquJ28jP7yPTBnQl0+xo7DAYh59wketqsxVKUuae3l2/dYYG4HKUlJBB1yweihX8OES0rLYOGwA9ZbnKGkII+rZ32xYvF8gYt8SUkJXPd6YoL2DIRduw7jJYa4cy0UJsuM0FSEhLeYfwY/1EAsWbIE06dP5/lz8+ZNgWOIT4ls8fZTzJ9Fq1YtcdznAHS1p2Kv5wFY2tijqqqqoV1ZSREB3u6orKzCApM6I6HI6gUPJyvEJSTC1G4X1Af0wRqDGQi+8QCnL93EPktDpGXnY4OnP9xWzUJhSQUs9gdi17LJKKmowiqvEKzTVkOP9i2x8lAE3qXlwVZXFaOVumBT4FPsj3gLDkd4afjk3FLsufoarZpJo0/XusinW6+TUMPmYKhinVM35GEs2rZoBjXF7oj7ko6Hbz5h4cQhkJKUhE/wTaTnFsDBeDYkJCRw7f6zhmilyaPUAQBPY2KxeP1mtG3dEkGHnNFbrkfD/d+++wBdgxW4evMOLE2X4VcfDzB79uCdKOqS8s4FX8QE7Rk4euIktKdMwq2wi9hkbYHWrf/X3p0H1JT+Dxx/typtUpItZGQtRCJ7Y5lJIcY2w1iyzBQp26ixk7JkN9YZzDAMYx17pOxNEZGlLI1SooVKe/f5/eH7bfTrWuY7dM14Xn/VPad7P31O3c99nnPO5yl9Z7j0HiuTC21fYdCgQSIqKqr4+zVr1og1a/7sjDl8+HBx6dIlZT9agrwPQvorFAqF+G7dBmHZsKkY8OVw8eTJ0xLbb8beEbaOLsLBqa+If/C84dwvvwWJ2m16iBGT54ic3Fzh7b9a1O48SPx84ITYevi0qOXiIaas2Cp+PXlR1OrrKyZ/t6t4RTn3FXvE7aR0YT95s2g+YaOIeZAqcvIKiu9xGPRdsDh7K0kUFZVuWnf9QZpoMW23aOLzq4iMSymOv/ucnaLTtJ9FYVGRuJOUKmoP+fP+i9ELtwjrIbPF06xsEf8wRdTr4yU85n8vhBAi8VGqaOo6WvRwnyby8p/3Yroec1c07txPdP7cXSQ/LrkK3L7Dx0W91p2Fg9NnIjwySrzKlavXhGv/QcUN9aKuRf/1gyOVqX/UfRCOjo4cPHiQ/Px84uPjiYuLw8bGRtVhSf8yampqfD3SjaULny820+fzL4m9/ed0T72PLPnpu0Cyc3LoP9KTm7fv0s+5M7PGj+bE2XC8ZgYyZ9wQOrZswrTlmyivpYZ7365sP3aOm3fjGNunEztOXuTSjbv49O/IofBbrNh7mo1jnFBXU6ffon0EX/2DwM/tmdm7OdEJ6Xz+3Ulaz9rHsHWh+PzyO14/nWPYulBcFh1FXU2NnZ6daVrTBCEE/rsuEB2fwtfdmqGupsbCnaFoaWrwZefmhN+M4+jv1xnu5IBBeR2mr92BupoavsNcKSgsZNy8VRQUFLLUxx1tLU0Skx/jNnkOBvp6/LR0NmamFYHno/nl6zfjNXUOTRs35Let62nR1FppPrOzc/Cbv4jeAwaTmPSQRf5z2LFlI9aNGpbJ8ZTeDZUViKCgINq3b09kZCSjR4/Gze1547S6devy6aef4uTkxIgRI5g+fXrxCTJJettcnD7hpx/WkpGZSZ+BXxIccqp4W8N6dfl5zRIUCkE/t7FEXL7K4N5OzPQexbHTYUyYvYTFU77C3qY+Exes5aMqFfmye3s27A0m42k6w7s7sPnIea7F3GNCn7bsO3+dKd8fZOXIj6lW0QCPdUEMX3mYjvUrc25GD5YNbk2zmiYkpGURdO0Bl+JSSUp/xhdtPmL/+K7UNTeiSKHAd8sp1gddYVCHRvRpXY8twZEciYjB27UthrraTFmzh2qVKjCyR1v2n4rgZEQ0kwa7ULWSMYs37SLiWgx+3sOpXd2cjMwshk+aQ1Z2Dt8vnF58pVJefj4TZ/izdO1GenfvxuaVC6n4kqVez14Iw8m1Lz/8uJUv+vcl6OAeXHs4y6nhf4MyG8e8Y3KKSfo7EpMeih59B4qPGtuKDRt/LNGqOiHpoXB0HSQate0mjoeeFUIIsfnXA6J2mx5i6IRZIjk1TXw+0U/U6TpY/LjvmJizYZeo5eIhvAM3iRW7gkWtvr5iwMz1YuepK6LxqMWimfsysfdctPjh+BXR2HODsHJfJzzWHhMnouJETl6B0vhy8gpEaPR98XngflFr1GqxYM8FoVAoRNClWGE1fOHzdR8Ki8S0DftErb6+IvRyjEh8nCZsv/hG9JqwUBQWFonzl6+LOl0HiymBG4QQQuTm5omBY3yFVYfe4kz4C2tX5OaKoWMni9rNO4gV6ze/dK2GnJwcMWOuv7Bs2FQ4ftpDhIVHKN1Per+96r1TTQglZ4X/gXr37s3u3btVHYb0D/bsWTaTfKdx9Hgw3T/tiv+sGejpPW9b/TglFTdvH6JvxuIz7ivcvujH9v3HmL54DVa1a7Jq7mTmrt1G8IXLDHXtipGxCUu3HaKpVS16dGrNgm1BGJQvx7h+Xdhx5jpR95Jo07AmQ7vZce7WQ3aeu0lWbgG62prYW1XB1KA85ctpoaOtwY2EVH6PTSKvoAgDXW18+rRiYLuGbAu5zNRNx2hcqzKbJvZjR3AEAVuO4ObchkkDutDPZyl3E5LZGzgRPR1tXL6eRgUDffasnImerg6T5y1n1+FgFk/3plfXjgDk5uYxeuJUzoRFMO/bCfTv5aw0V7dib+M1yYeY2NsMG/wFE73GyCuT/qFe9d4pC4QkvUAIwbrvN7Fo2Uo+qmPJ6mWB1KppAUBObi4TZ/hz+EQo/Xo6MesbL8Iir+ExdT6GBvp85zeF/SFhbNx9lA52NnT/uC0z1/2KjrYWYwZ0Z/vJSG7dT8aljQ0f1azGjyciSc3Ipr11bT5rZ42ujg6nrz8gLDaRzJx8svMKyM4roIapIe0b1qBdw+q0rFuFyDuJbDj8O6FX79HRxpIV7j34NeQSszYewNnBmiVjPmPS8q3sDQlnre9I2jWtz6DJAcTEJbBnxUw+qlmNdT/vJuC7zXgOG4CX20AAnmVnM3riVM6HRxIwbTJ9e3yqND/bdvzKnIBFGBjos9BvNh3atSnTYyS9XbJASNJfdObcBcZNnEKRoojFAX44dmwPPL+Ec+najaz8/iea2zRm1YJZPEp9wmgfP1KfZDDLexSFaDBn9RZMKhgyacRA1u49yc24B/TpZE9FUxM2H76AlqYGX3S1R0tHlz1no0lMy0RHW5NW9S1o26gW1UwNMTHUo6K+LulZOdxJSuVOUhqhUXeJeZCCiWF5Rn7aEqcWdZm6fj+nrsTSqZkVy8f159vvtrH/1EUmDHJmlGtnRs9YwumLV1k5dSyftLPjl9+O4TN/FU6d2rB81kTU1dXJepbN0LGTuBJ9g/nTvqG3c7dSOcnNzWX6HH927d1P+7YOLPSbjampSan9pH8WWSAk6X8Qn/AAD+9JRF+/gfuoEYzzGF18U9ih4yFMmhmAoYE+KwNmUsuiBt6zF3Mm/DK9P+lEvx6f8E3gBu4nPWKoa1c0dfT4ft9J9HTL8aVzJ2IT0zkWcQMtDXV6tGmCVa2qPEjLIvTqPeKS05XGo62lQSOLynzh2JT2jWpxOCyaRduDKCwqwmfQJ3xq3wivwE2cvXKLb4b0ZJhLR7wDVnPkdDj+3m70d+rIb8dP4TVrMe1bNmNtgC/aWlpk5+Qw0tuX3yOvsMJ/Jp84ti/12gkPEvHwnsi16Bt4uo9m7NejXrvoj/TPIAuEJP2P8vLymOkXwI5de7G3a8Gyhf5UqvS8U+qN2Dt8PXEaDx4+xGv0cEYO6seqn35l1eadVDEzZfaE0RwPu8q2g8FUNzdlVP8eHP09mjOXb2Jhbkpvx1YkPcnm4PlrZOXkYWxQnjbWdbCuUwMDPR00NDQQqKGvo42RXjm01CEm/hFHf79O2PV7FBYpsG9Ym4DRvbgV94Bpa37haWY2fh4D6NS8IV/NXMbF6Bimfv0Fw3t/wt6jIUz0W0YLmwZsXDQDXZ1yPM3IZISXD5HXrhM425een3QulYPzYeF4eE2kSKEg0H8OnR07lu1BkN4pWSAk6W/atXc/0+f4Y6Cvz5IF82ht/7zJZEZWFlP9AjkQdBKHlrYEzvLlQXIKE+cuIy4hkcG9nejo0JJ567ZzJz6RVk0a0LWdPbtDI7h6Ox59XR26t7PFvJIp9x8/5UzUbR4/yXplLJZVTenWsiGdmzcgJT2dVTuOEnX7PvVqVmHJ+KEU5OcxZu4KHqc9ZdHk0XTvYM8vB4Lwnb+KVs0as37+VMrr6vA4NY0hYyZxN+4+S/2mKR057Ny9l6mz/Khd04K1K5dS06KGkoikfzJZICTpLbgVe5sx3pO4F/cHI4cNwXusO9raWggh2LHvELMWLkdbW4tvvT1w6tyRwHVb2LzrIBWNDBnnNpB8hRqrt+8n9Ukm7VtY06l1C67cSeDo+ctk5+ZT0VAfBxsr6tWujq6ODqipkVtQRDktTSrol0dfVxttDXUepqRx6eY9zkfFkJiSTnUzEzwHfELbJvVY9uNudh49hVnFCqyZ5UXDOhYsXPMTG7bvpV3LpqyZ54uuTjni4hMYNnYyj1LSWBs4l7b2LUr8rvn5BfjNX8SW7Tto69CKlYsXYGDw6uVCpX8mWSAk6S3Jzs7Bb0Eg23fuomH9eiye70fdj543x7v7RzxT5iwg4vJVWrdoxlzfCTzLyWP2sg1ERF2nYd3ajB7Uh4RH6Wz49TDpGVlUq2yKi2NrjI0rcjMukdOXb/I4/c923urqaqirqVFYpCgRh4mRPs0bWNKjfQusLauz98RZNvx6mNy8fIa6dmXMoF48eZKB16xALl+PYZDrp0z1dENbS4sr127g5jUFeL7qW9PGJe92fvo0g6/HTSAsPIKRw4Yw0WuM7Lr6LyYLhCS9ZceDQ/CZPousZ9mMcx+N29DBaGlpoVAo+GXvQQKWryEvPx+3L/oxanB/Tv1+mcB1W7mf+JB6dWoycmAv1DXLsTvoDGcjoxFCYFHFjDa2jahtUQ09XV0KFYLHTzIpUijQLaeNbjltjA31qVOtErk5uVy/c5/Dp38n4loMAI6tmuI76nOqVTZl087fWL5xO5qamvhP9sDJsc1/RjoHmblwOZVNTdm4YgG1LUo23LsVe5uvPceTmJjEfL9Z9HR2UkV6pTIkC4QkvQMpKalMnzOPo8eDaVDPinmzp2PT+Pm6CI9SUvFftpp9h49jZGjAV0M+5/PPenDiTDjf/bST23EJmBgb4fJxOxzsmpGYks6Zi9c4f/k6z3Jyi1/DrGIFyuvq/KfLsSDzWQ5pT/9cOtWqVnWcO9rj0qk1FfT12LrvCJt/PcDj1HS6tLNn+riRVDOvREZmJr5+gRw6HkKbls1ZMndqqcV9fjt0BJ/ps9DX02fl4gW0aN4M6d9PFghJeoeOBp1gpl8AKalpDBrYDy+Pr4vbWl+/Fcui774n5OwFKplUZOjAPgx0debi1VvsPnKSE+fCyc8voLJpRRzb2OHQ3IZKJiYkpz3hfuIj7j98RE5uPmpqaqipgZ6uDpY1qlDXohof1ayGproaZyOucDr8MsdOXSA7J5d2LZsy6vPetGnRBCEEQaFnmLVwOY9T0hj/tRujvhxQ4hLVvPx8Fi5ezsafttLCthkrFs/HrFIlVaVTKmOyQEjSO5aZmcmipSvZ+stOjAwNGfP1KL7o3xdt7edrM/8eGcXKDT9yJiwCvfK69O/lzABXZ8xMTThxNpzDIec4fzGqePRQxcyEWtWrUqt6VSoYGaClqYm2thaFhYUkP04j6XEK8YnJ3PkjAQBjIwM6t7Vn6GfONKhbGyEEpy+Es3TtRi5fu4FVndrMnzaZJo0blIj73h9/MG6iD9HXb/DlFwPwmTi+OGbpwyALhCSVkRs3b+G/aAlnz4dhUaMG4z3dcerWpbgjcfTNWNZv+YWDQcEUFSmoX7cOrk5dcOrcEbNKpkRG3yL8cjT34hO5F59IXEIimVnZFL6wLrtJBSPMzUyoYmaKbeP6tLVrSsO6tVFXVycjK4ugkDNs2/0bl6KiqWpemTFug+nj8glaL5xoVigUbN+5i4BFS9HS0iJg7gy6OHYq83xJqicLhCSVISEEp86cIyBwKTGxt6ljWRv3UW44f9qt+Gqg5McpHD4Ryv4jx7kSfRMhBJY1a9DazpZWzZtS17IWNatXpVy55+tNKxQKCgqLUFMDba0/P+Hn5eURczeOGzF3CD5znpCzF8jPL8CiWlVGDO5P3x6fUk5bu0R8sbfv8O3MuVyMvIxDq5bMnzuLqlXMkT5MskBIkgooFAqOBJ1g1Zr13IyJxaJGdT7v35f+fVwxNPzznoL7CYkcCznNufBIwiOv8Cw7B3i+qFG1KpUxrWiMXvny6JXXRUNDg6xn2WRmZfEkI5P4B4kU/ecS2EomFenetRMuXR1p2rhhqfUY0p88Yd33m9n44xb09PTwnTye3j1d5LoNHzhZICRJhRQKBSdCQtmw8SciLkVSXleX3j1d6N3TBevGDUucMC4oLORm7B3u/hHPvT/iuXc/gSdPM8jOzuFZTg6FhYXo65XHQF8fA309LGvWoH7dOjSw+oia1asq7Y/05MlTNmz+iR+3bCM7J4eeLk74TByPqUnFskyD9J561XunvPtFkt4xdXV1ujh2ootjJ6Jv3GTzlm3s2LWHLdt3YFbJlI87daBT+3Y4tGqJrq4u1g3qYd2g3t96zYKCAs6cv8C+3w5x/GQIOTm5OHXrwlj30Vj958Y+SXodOYKQJBV4+jSDk6dOczw4hNPnLpCVlYW2tjYtW9jSpnUrGtavR92PLDGrVOmNpoCKioq4fecuEZciCb8YydkLYaSlpVPByAinT7owaEA/6lnVLYPfTPqnkSMISXrPGBkZ0sulO71cupOXn0/Y7xGcPnuO02fPMz9wafF+Bgb6VDE3x7iCEcYVKqCnp0eRoghFkYKCggIep6SS9PAhyY8eU1hYCIBZJVMc7Fvi7NSNDm3bystWpf+ZLBCSpGLltLVp39aB9m0dAEhJTSP29h1ib9/h9p27PEpJ4cmTJ9y+c5dn2TloaKijrq6BlpYmpiYm2LWwpUrlyljWroVdc1uqV6sqTzxLb4UsEJL0njE1qYipScXiluKSpCpySShJkiRJKVkgJEmSJKVkgZAkSZKUkgVCkiRJUkoWCEmSJEkpWSAkSZIkpWSBkCRJkpRSWYE4fPgw3bt3p379+ly9erX48YSEBGxsbOjZsyc9e/Zk+vTpqgpRkiTpg6ayG+WsrKxYsWIFM2bMKLXNwsKCffv2qSAqSZIk6b9UViDq1JEdJSVJkt5n7+U5iISEBHr16sWgQYOIiIhQdTiSJEkfpHc6ghg6dCgpKSmlHvfy8qJz585Kf8bMzIyTJ09ibGzMtWvX8PDw4ODBg+jr67/LUCVJkqT/550WiE2bNv3ln9HW1kb7P2voNm7cGAsLC+7du4e1tfVbjk6SJEl6lfduiiktLY2ioiIA4uPjiYuLo0aNGiqOSpIk6cOjshXlgoKCmDNnDmlpaRgaGtKgQQO+//57jh49yvLly9HQ0EBDQ4OxY8fi6Oj42uezt7enWrVqZRC5JEnSv8eDBw8ICwtTuu1fs+SoJEmS9Ha9d1NMkiRJ0vtBFghJkiRJKVkgJEmSJKVkgZAkSZKUkgVCkiRJUkoWCEmSJEkplTXr+zfw8vLi3r17AGRmZmJgYKC0C62joyN6enqoq6ujoaHB7t27yzrUElasWMGOHTuoWLEiAOPHj6dDhw6l9jt16hR+fn4oFAr69u3LqFGjyjrUEubPn8/JkyfR0tLCwsICf39/DA0NS+33PuT7dbnLz89n8uTJREdHU6FCBZYsWUL16tXLPM4XJSUlMXnyZFJSUlBXV6dfv34MGTKkxD5hYWG4u7sXx9qlSxfGjBmjinBLeN0xF0Lg5+dHaGgoOjo6BAQE0KhRIxVF+9zdu3fx9vYu/j4+Ph5PT0+GDh1a/JjK8y2kt8Lf31+sWLFC6bZOnTqJ1NTUMo7o5ZYvXy42bNjwyn0KCwvFxx9/LO7fvy/y8vKEi4uLiI2NLaMIlTt9+rQoKCgQQgixYMECsWDBAqX7qTrfb5K7LVu2iGnTpgkhhDhw4IAYN26cKkItITk5WVy7dk0IIURmZqbo2rVrqbgvXLggRo0apYrwXul1xzwkJES4ubkJhUIhIiMjxWeffVaG0b1eYWGhcHBwEAkJCSUeV3W+5RTTWyCE4PDhwzg7O6s6lLcmKiqKmjVrUqNGDbS1tenevTsnTpxQaUxt27ZFU/P5oLdp06Y8fPhQpfG8zJvkLjg4GFdXVwC6devG+fPnESq+Z9XMzKz4U7W+vj6WlpYkJyerNKa35cSJE/Tq1Qs1NTWaNm1KRkYGjx49UnVYxc6fP0+NGjXeu24QskC8BREREZiYmFCrVq2X7uPm5kbv3r355Zdfyi6wV9i6dSsuLi74+Pjw9OnTUtuTk5MxNzcv/r5y5crv1ZvFrl27aN++/Uu3qzLfb5K75ORkqlSpAoCmpiYGBgakp6eXaZyvkpCQwI0bN2jSpEmpbZcvX6ZHjx6MGDGC2NhYFUSn3KuO+f8/Jubm5u/V3/PBgwdf+gFTlfmW5yBe401alh84cOCVo4dt27ZRuXJlUlNTGTZsGJaWltjZ2b2zmOHVcQ8cOBB3d3fU1NRYtmwZAQEB+Pv7l9hP2adZNTW1dxbvf71JvlevXo2GhgY9evRQ+hyqyPeL3iR3qsrvm3j27Bmenp74+vqWarPfqFEjgoOD0dPTIzQ0FA8PD44dO6aiSP/0umP+Puc7Pz+f4OBgJkyYUGqbqvMtC8RrvK5leWFhIUFBQa88EVq5cmUATExM6NKlC1FRUe/8DetNW6337duXr776qtTj5ubmJaZwkpOTMTMze1vhvdTr4t6zZw8hISFs2rTppf/gqsj3i94kd+bm5iQlJWFubk5hYSGZmZlUqFChzGJ8mYKCAjw9PXFxcaFr166ltr9YMDp06MCsWbNIS0srvuBBVV53zP//MXn48GGZ/D2/iVOnTtGoUSNMTU1LbVN1vuUU09907tw5LC0tSwxfX5SdnU1WVlbx12fPnqVu3bplGWIpL869Hj9+XGk81tbWxMXFER8fT35+PgcPHnyjrrrv0qlTp1i/fj2rV69GV1dX6T7vQ77fJHeOjo7s2bMHgKNHj9KqVSuVf6IVQvDtt99iaWnJsGHDlO7z+PHj4k/jUVFRKBQKjI2NyzLMUt7kmDs6OrJ3716EEFy+fBkDA4P3pkAcPHiQ7t27K92m6nzLEcTfdOjQoVIHNzk5malTp7J+/XpSU1Px8PAAoKioCGdn51fOnZeFhQsXcvPmTQCqVavG7NmzgZJxa2pqMn36dEaMGEFRURF9+vRReWGbM2cO+fn5xW9eTZo0Yfbs2e9dvl+Wu2XLltG4cWM+/vhjPvvsMyZNmkSXLl0wMjJiyZIlZRqjMhcvXmTfvn1YWVnRs2dP4Pkl0ImJiQAMHDiQo0ePsm3bNjQ0NNDR0WHx4sUqL2wvO+bbtm0DnsfdoUMHQkND6dKlC7q6usybN0+VIRfLycnh3Llzxf+DQIm4VZ1v2e5bkiRJUkpOMUmSJElKyQIhSZIkKSULhCRJkoSswlIAAAPpSURBVKSULBCSJEmSUrJASJIkSUrJAiFJkiQpJQuEJL1FYWFhjB49+n/++atXrzJ37lyl2xwdHUlLSyMjI4OtW7e+tdeUpJeRBUKS3iPW1tZMnTr1lftkZGQU30wlSe+SvJNa+uBkZ2fj5eXFw4cPUSgUuLu7Y2FhQUBAANnZ2RgbG+Pv74+ZmRmDBw+mfv36XL16laysLObNm4eNjQ1RUVHMmzeP3NxcdHR0mDdvHpaWlq99bRcXF7Zu3YqBgQGtWrXCx8eHXr16MWnSJFxdXdHQ0OCHH35g7dq1pKenM2HCBNLS0rCxsSluuRAYGMj9+/fp2bMnDg4OdOzYkezsbDw9PYmJiaFRo0YsWrRI5Xc4S/98cgQhfXBOnz6NmZkZ+/fv58CBA7Rr1465c+eyfPlydu/eTZ8+fUq0vsjJyWH79u3MmDEDX19fACwtLdmyZQt79+7F09PzjVtlNGvWjEuXLhEbG0v16tWJiIgA4MqVK6Vaa69atQpbW1v27t2Lo6NjccuLCRMmYGFhwb59+/jmm28AuH79Or6+vhw6dIiEhAQuXrz4t/MkSXIEIX1wrKysmD9/PgsXLqRTp04YGhoSExNT3ONJoVBQqVKl4v3/22vLzs6OrKwsMjIyePbsGd988w1//PEHampqFBQUvNFrt2jRgvDwcKpWrcrAgQPZsWMHycnJGBkZoaenV2Lf8PBwVq5cCUDHjh0xMjJ66fPa2NgUN4ysX78+Dx48oEWLFm+eFElSQhYI6YNTu3Ztdu/eTWhoKIGBgbRp04a6deu+dHGh/z9V8991NOzt7Vm1ahUJCQl8+eWXb/TadnZ2/PzzzyQlJeHt7c3x48c5cuTI334z19bWLv5aQ0ODoqKiv/V8kgRyikn6ACUnJ6Orq0vPnj1xc3PjypUrpKWlERkZCTxfE+HFlbsOHToEPF850MDAAAMDAzIzM4vXIPhv2+43UaVKFdLT04mLi6NGjRrY2tryww8/0Lx581L72tnZ8dtvvwEQGhpavPKfnp4ez549+99+eUn6C+QIQvrgxMTEsGDBAtTV1dHU1GTmzJloamoyd+5cMjMzKSoqYsiQIcXtzY2MjBgwYEDxSWqAESNGMGXKFDZu3EirVq3+0uvb2NigUCiA51NOixcvVlogPDw8mDBhAq6urtjZ2VG1alUAjI2NsbW1xdnZmXbt2tGxY8e/kQ1JejnZ7luSXmHw4MFMnjwZa2trVYciSWVOTjFJkiRJSskRhCS9A7t27eLHH38s8ZitrS0zZsxQUUSS9NfJAiFJkiQpJaeYJEmSJKVkgZAkSZKUkgVCkiRJUkoWCEmSJEmp/wMFA57/pfgMoAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = sns.load_dataset('iris')\n",
    " \n",
    "# Basic 2D density plot\n",
    "sns.set_style(\"white\")\n",
    "sns.kdeplot(tsne_vectors[:,0], tsne_vectors[:,1])\n",
    "#sns.plt.show()\n",
    " \n",
    "# Custom it with the same argument as 1D density plot\n",
    "sns.kdeplot(df.sepal_width, df.sepal_length, cmap=\"Reds\", shade=True, bw=.15)\n",
    " \n",
    "# Some features are characteristic of 2D: color palette and wether or not color the lowest range\n",
    "sns.kdeplot(df.sepal_width, df.sepal_length, cmap=\"Blues\", shade=True, shade_lowest=True, )\n",
    "#sns.plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (terran3)",
   "language": "python",
   "name": "terran3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
